---
title: "Stan Bookdown"
author: "Kamran Afzali"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
---

# Choosing the Right Bayesian Model

**Introduction**

Bayesian modeling has become a central approach in modern data analysis, providing a coherent framework for incorporating prior knowledge and quantifying uncertainty. With the advent of powerful tools such as `Stan` and user-friendly interfaces like the R package `brms`, practitioners can now implement a wide array of Bayesian models with relative ease. However, the flexibility of the Bayesian framework also introduces a new challenge: selecting the most appropriate model for a given dataset and research question. The landscape of Bayesian models is vast, encompassing linear and generalized linear models, robust and regularized regressions, hierarchical models, time series models, multivariate approaches, and more sophisticated methods such as Gaussian processes and mixture models. This guide aims to offer a structured approach to model selection within the Bayesian paradigm, focusing on practical considerations, data characteristics, and modeling objectives.

**Modeling Objectives and Data Characteristics**

The choice of a Bayesian model should begin with a clear understanding of the research objective. In broad terms, the aim of modeling can be categorized into two primary goals: inference and prediction. Inference focuses on understanding the relationships between variables, quantifying uncertainty in parameter estimates, and testing theoretical hypotheses. Prediction, on the other hand, emphasizes the accuracy of forecasting outcomes for new observations. While the two goals are not mutually exclusive, they can lead to different modeling choices, particularly in terms of model complexity and regularization. Another factor influencing model selection is the nature of the data. Key aspects include the type of response variable (continuous, binary, count, categorical), the temporal structure of observations (time series, longitudinal data), the presence of outliers or heavy-tailed distributions, the structure of the data (e.g., hierarchical or longitudinal), and the dimensionality of the predictor space. A careful examination of these characteristics provides essential guidance for selecting an appropriate Bayesian model.

**Prior Specification and Computational Considerations**

An essential feature of Bayesian modeling is the specification of prior distributions. Priors can be informative, weakly informative, or non-informative, depending on the amount of domain knowledge available. Informative priors are grounded in expert knowledge or historical data, while weakly informative priors help stabilize estimates without unduly influencing the posterior. Prior predictive checks can assess the implications of the priors before seeing the data, ensuring they encode plausible assumptions. Modelers should also perform sensitivity analyses to understand how different priors affect inferences.

Computational feasibility is another practical concern. Some Bayesian models—especially nonparametric or high-dimensional ones—can be computationally intensive, requiring advanced MCMC algorithms or variational inference. Diagnostics such as the Gelman-Rubin R-hat statistic, effective sample size (ESS), and checks for divergent transitions should be used to ensure reliable inference (Gelman et al., 2013). Stan and `brms` provide tools to assess convergence and evaluate sampling efficiency.

**Bayesian Linear Regression**

Bayesian linear regression serves as the foundational model in the Bayesian framework. It assumes a linear relationship between predictors and a continuous response variable, with normally distributed residuals. This model is particularly useful for its simplicity and interpretability. When the assumptions of linearity and normality hold reasonably well, Bayesian linear regression provides reliable parameter estimates and predictive intervals. It also serves as a baseline model against which more complex models can be compared. In practice, Bayesian linear regression can be implemented in Stan with straightforward model code, specifying priors for the regression coefficients and residual variance. The flexibility of Bayesian inference allows for the incorporation of prior knowledge, which can be particularly valuable in small-sample contexts or when strong domain expertise is available.

**Robust Regression for Non-Normal Residuals**

Real-world data often deviate from the assumption of normally distributed residuals. Outliers or heavy-tailed distributions can exert undue influence on parameter estimates, leading to biased or unstable results. Bayesian robust regression addresses this issue by modeling the residuals using a t-distribution, which has heavier tails than the normal distribution. This approach reduces the influence of outliers, leading to more robust and reliable inferences. The implementation of robust regression in Stan involves specifying a likelihood based on the t-distribution and including an additional parameter for the degrees of freedom. This parameter controls the heaviness of the tails and can itself be estimated from the data. The robust regression model is particularly recommended when residual diagnostics from a standard linear model indicate non-normality or the presence of extreme observations.

**Regularized Regression for High-Dimensional Data**

When dealing with a large number of predictors or multicollinearity, regularization becomes essential to prevent overfitting and to enhance predictive performance. Bayesian regularized regression models incorporate shrinkage priors, such as the Laplace prior for Bayesian LASSO or the Gaussian prior for Bayesian ridge regression. These priors shrink the regression coefficients toward zero, effectively performing variable selection and regularization. In the Bayesian framework, regularization is naturally integrated through the prior distribution. For example, the Bayesian LASSO uses a double-exponential prior that induces sparsity by assigning higher probability mass near zero. These models are particularly useful in settings with more predictors than observations or when there is a need to identify the most influential variables.

**Models for Non-Normal Data**

In many applications, the response variable does not follow a normal distribution. Binary outcomes, count data, and categorical responses require specialized models. Bayesian generalized linear models (GLMs) extend the linear model framework to accommodate different types of response variables through appropriate link functions and likelihood distributions. For binary outcomes, the logistic regression model with a logit link is commonly used. For count data, Poisson and negative binomial models are appropriate, with the latter providing a flexible alternative in the presence of overdispersion. Multinomial and ordinal regression models are used for categorical outcomes, with the choice depending on whether the categories are ordered. These models are readily implemented in Stan and `brms`, allowing users to specify the appropriate family and link function. Model selection in this context should be guided by the distributional characteristics of the response variable and the research question at hand.

**Multilevel and Hierarchical Models**

Hierarchical data structures are common in social sciences, education, and biomedical research. In such settings, observations are nested within higher-level units, such as students within schools or patients within hospitals. Ignoring this structure can lead to biased inferences and underestimated uncertainty.

Bayesian multilevel models explicitly account for the hierarchical structure by including group-level effects. These models allow for partial pooling of information across groups, balancing between complete pooling (ignoring group differences) and no pooling (treating each group separately). The `brms` package offers a user-friendly interface for fitting multilevel models, handling complex random effects structures with ease. The flexibility of Bayesian multilevel modeling also facilitates the inclusion of varying slopes, cross-level interactions, and non-linear effects. When the data structure suggests hierarchical dependencies, multilevel modeling should be the default approach.

**Canonical Correlation Analysis**

When the research objective involves understanding the relationship between two sets of multivariate observations, Bayesian canonical correlation analysis provides a principled framework for multivariate association. This approach identifies linear combinations of variables from each set that are maximally correlated, revealing the underlying structure of relationships between variable groups. Bayesian canonical correlation extends traditional methods by providing uncertainty quantification for canonical correlations and loadings, allowing for more robust inference about multivariate relationships.

The Bayesian formulation typically employs matrix-variate priors on the canonical vectors and incorporates shrinkage to prevent overfitting in high-dimensional settings. This approach is particularly valuable when exploring relationships between groups of predictors and outcomes, such as relating brain imaging measures to cognitive assessments or linking genomic profiles to phenotypic characteristics. The posterior distribution provides natural measures of uncertainty for the canonical correlations and facilitates model comparison through Bayes factors or information criteria.

**Time Series Models: ARIMA and Seasonal Decomposition**

Time series data require specialized modeling approaches that account for temporal dependence and potential seasonality. Bayesian autoregressive integrated moving average (ARIMA) models provide a flexible framework for modeling univariate time series with trend and seasonal components. The Bayesian approach to ARIMA modeling offers advantages over classical methods by naturally incorporating parameter uncertainty and allowing for informative priors based on domain knowledge.

Seasonal decomposition within the Bayesian framework explicitly separates time series into trend, seasonal, and irregular components. This decomposition can be achieved through state space formulations that treat each component as a latent process with its own dynamics. The Bayesian treatment provides posterior distributions for each component, enabling uncertainty quantification for trend estimates and seasonal patterns. This approach is particularly useful for time series exhibiting clear seasonal patterns, such as economic indicators, climate data, or retail sales.

The implementation of Bayesian ARIMA and seasonal decomposition models often requires careful specification of priors on autoregressive and moving average parameters, ensuring stationarity and invertibility constraints are respected. Modern packages like `bsts` in R provide convenient interfaces for these models, handling the complexities of state space formulations and MCMC sampling.

**Exponential Smoothing: Bayesian Holt-Winters**

The Holt-Winters exponential smoothing method can be formulated within a Bayesian framework to provide uncertainty quantification for forecasts. Bayesian Holt-Winters models treat the level, trend, and seasonal components as evolving parameters with their own prior distributions. This approach naturally handles the adaptive nature of exponential smoothing while providing credible intervals for predictions.

The Bayesian formulation typically specifies prior distributions on the smoothing parameters (alpha, beta, gamma) and the initial state values. This allows for the incorporation of prior beliefs about the relative importance of recent versus historical observations and the stability of trend and seasonal patterns. The resulting posterior distributions provide not only point forecasts but also prediction intervals that reflect both parameter uncertainty and inherent noise in the time series.

Bayesian Holt-Winters models are particularly valuable for forecasting applications where quantifying prediction uncertainty is crucial, such as supply chain planning, financial forecasting, or resource allocation. The ability to incorporate prior knowledge about seasonal patterns and trend behavior makes this approach especially suitable for business applications with strong domain expertise.

**Bayesian Structural Time Series (BSTS)**

Bayesian structural time series models represent a comprehensive approach to time series analysis that combines the flexibility of state space models with the principled uncertainty quantification of Bayesian inference. BSTS models decompose time series into interpretable components such as trends, seasonal patterns, regression effects, and external interventions, all within a unified probabilistic framework.

The key advantage of BSTS lies in its ability to handle complex time series structures while maintaining interpretability. The model can incorporate external regressors, detect structural breaks, and account for irregular seasonal patterns through flexible specification of state evolution equations. The Bayesian treatment provides posterior distributions for all model components, enabling inference about the relative importance of different factors driving the time series.

BSTS models are particularly powerful for causal inference in time series settings, as they can incorporate control variables and assess the impact of interventions through counterfactual analysis. The spike-and-slab priors commonly used in BSTS facilitate automatic variable selection among potential regressors, making the approach suitable for high-dimensional settings with many candidate predictors.

Implementation of BSTS models typically requires careful specification of prior distributions on state variances and regression coefficients. The `bsts` package in R provides a mature implementation with extensive functionality for model specification, fitting, and diagnostic checking. The resulting models offer both excellent predictive performance and interpretable decomposition of time series components.

**Nonlinear and Nonparametric Models**

In some applications, the relationship between predictors and the response variable is inherently nonlinear or unknown. Bayesian nonparametric models, such as Gaussian process regression, offer a flexible solution by modeling the function space directly. Gaussian processes define a prior over functions and use observed data to update this prior, resulting in a posterior distribution over functions. Gaussian process regression is particularly powerful when the form of the relationship is unknown or when modeling smooth, nonlinear trends is important. However, it comes at a higher computational cost and may not scale well with large datasets. Nevertheless, for problems involving spatial data, temporal trends, or complex functional relationships, Gaussian processes provide a valuable modeling tool.

**Mixture Models and Latent Structure**

Data arising from heterogeneous populations may be better modeled using mixture models. Bayesian Gaussian mixture models, for instance, assume that the data are generated from a mixture of several Gaussian distributions, each representing a subpopulation. These models can uncover latent structure in the data, such as clusters or subtypes.

Mixture models introduce additional complexity due to the need to estimate both the component parameters and the mixing proportions. Bayesian inference provides a principled framework for dealing with this uncertainty, often using techniques such as latent variable augmentation and label switching adjustments.

When there is reason to believe that the data comprise distinct subgroups with different underlying characteristics, mixture models offer an effective approach to modeling such heterogeneity.

**Comparative Summary Table**

| Model Type | Use Case | Key Assumptions | Priors | Limitations |
|---------------|---------------|---------------|---------------|---------------|
| Linear Regression | Continuous outcome, low noise | Linearity, normal errors | Normal, Inverse-Gamma | Poor with outliers |
| Robust Regression | Heavy-tailed residuals | t-distributed residuals | Prior on ν | Increased complexity |
| Regularized Regression | High-dimensional predictors | Sparsity | Laplace, Gaussian | Shrinkage may hide effects |
| GLMs | Binary/count/categorical outcomes | Appropriate link function | Varied | Can overfit without strong priors |
| Hierarchical Models | Nested/grouped data | Partial pooling | Hierarchical priors | Sensitive to group size |
| Canonical Correlation | Multivariate association | Linear relationships | Matrix-variate priors | Assumes linear associations |
| ARIMA | Univariate time series | Stationarity after diff. | Constrained AR/MA priors | Limited to linear dynamics |
| Seasonal Decomposition | Time series with seasonality | Additive/multiplicative | Component-specific priors | Assumes stable patterns |
| Holt-Winters | Time series forecasting | Exponential smoothing | Smoothing parameter priors | Limited to trend/seasonal forms |
| BSTS | Complex time series | State space structure | Spike-and-slab, others | Computationally intensive |
| Gaussian Processes | Unknown nonlinear function | Smoothness in kernel | GP prior | Poor scaling (O(n³)) |
| Mixture Models | Latent structure/clustering | Finite components | Dirichlet, etc. | Label switching, identifiability |

**Model Diagnostics and Comparison**

Choosing the right model also involves evaluating its performance and comparing it to alternative specifications. Bayesian model diagnostics include posterior predictive checks, which assess how well the model reproduces the observed data. Graphical comparisons between observed and replicated data can reveal model misfit or systematic discrepancies.

Information criteria such as the Widely Applicable Information Criterion (WAIC) and Leave-One-Out Cross-Validation (LOO-CV) provide tools for model comparison, balancing fit and complexity. These criteria estimate the expected out-of-sample predictive performance and are particularly useful for selecting among nested or non-nested models (Vehtari, Gelman, & Gabry, 2017).

Bayes factors offer another method for model comparison, based on the ratio of marginal likelihoods. However, they are sensitive to prior specification and can be computationally intensive. In practice, WAIC and LOO-CV are often preferred for their robustness and ease of computation.

**A Decision Framework for Model Selection**

To aid practitioners in selecting the appropriate Bayesian model, a structured decision framework can be employed. This framework begins with identifying the type of response variable: continuous, binary, count, or categorical. Next, the data should be assessed for features such as outliers, overdispersion, hierarchical structure, and nonlinearity. Based on these characteristics, the modeler can then choose among linear models, robust regressions, generalized linear models, multilevel models, or nonparametric approaches.

This decision process is iterative and should incorporate model diagnostics and domain knowledge. Starting with a simple model and progressively introducing complexity allows for a more transparent understanding of the data and the modeling assumptions. Each modeling choice should be justified in terms of its contribution to answering the research question and improving model fit.

**Conclusion**

Bayesian modeling offers unparalleled flexibility and rigor in statistical inference, but this power comes with the responsibility of thoughtful model selection. This guide has outlined the key considerations for choosing among the diverse array of Bayesian models available in tools like Stan and `brms`. By grounding model selection in the objectives of the analysis, the characteristics of the data, and robust diagnostic procedures, practitioners can make informed choices that enhance both the interpretability and predictive performance of their models. As with all statistical modeling, the process is iterative and benefits from a combination of statistical insight, computational tools, and substantive expertise. With this guide, researchers are better equipped to navigate the Bayesian modeling landscape and apply the appropriate models to their specific challenges.

<!--chapter:end:index.Rmd-->

# Bayesian Modeling in R and Stan

The aim of this post is to provide a quick overview and introduction to fitting Bayesian models using `stan` and R. For this, I strongly recommend installing Rstudio, an integrated development environment that allows a "user-friendly" interaction with R.

## What is `stan`?

`stan` is a tool for analysing Bayesian models using Markov Chain Monte Carlo (MCMC) methods. MCMC is a sampling method for estimating a probability distribution without knowing all of the features of the distribution. STAN is a probabilistic programming language and free software for specifying statistical models utilising Hamiltonian Monte Carlo methods (HMC), a type of MCMC algorithm. Stan works with the most widely used data-analysis languages including R and Python. In this quick overview, we'll focus on the rstan package and demonstrate how to fit `stan` models with it.

For an example dataset, here we simulate our own data in R. We firsty create a continuous outcome variable y as a function of one predictor x and a disturbance term ϵ. I simulate a dataset with 100 observations. Create the error term, the predictor and the outcome using a linear form with an intercept β0, slope β1, etc. coefficients.

``` r
set.seed(123)
n <- 1000
a <- 40  #intercept
b <- c(-2, 3, 4, 1 , 0.25) #slopes
sigma2 <- 25  #residual variance (sd=5)
x <- matrix(rnorm(5000),1000,5)
eps <- rnorm(n, mean = 0, sd = sqrt(sigma2))  #residuals
y <- a +x%*%b+ eps  #response variable
data <- data.frame(y, x)  #dataset
head(data)%>%kableExtra::kable()
```

+----------+------------+------------+------------+------------+------------+
| y        | X1         | X2         | X3         | X4         | X5         |
+=========:+===========:+===========:+===========:+===========:+===========:+
| 33.51510 | -0.5604756 | -0.9957987 | -0.5116037 | -0.1503075 | 0.1965498  |
+----------+------------+------------+------------+------------+------------+
| 43.76098 | -0.2301775 | -1.0399550 | 0.2369379  | -0.3277571 | 0.6501132  |
+----------+------------+------------+------------+------------+------------+
| 27.64712 | 1.5587083  | -0.0179802 | -0.5415892 | -1.4481653 | 0.6710042  |
+----------+------------+------------+------------+------------+------------+
| 50.72614 | 0.0705084  | -0.1321751 | 1.2192276  | -0.6972846 | -1.2841578 |
+----------+------------+------------+------------+------------+------------+
| 39.46286 | 0.1292877  | -2.5493428 | 0.1741359  | 2.5984902  | -2.0261096 |
+----------+------------+------------+------------+------------+------------+
| 39.42009 | 1.7150650  | 1.0405735  | -0.6152683 | -0.0374150 | 2.2053261  |
+----------+------------+------------+------------+------------+------------+

## Model file

STAN models are written in an imperative programming language, which means the order in which you write the elements in your model file matters, i.e. you must first define your variables (e.g. integers, vectors, matrices, etc.), then the constraints that define the range of values your variable can take (e.g. only positive values for standard deviations), and finally the relationship between the variables.

A Stan model is defined by different blocks including:

-   Data (required): The data block reads information from the outside world, such as data vectors, matrices, integers, and so on. We also need to define the lengths and dimensions of objects, which may appear unusual to those who are used to R or Python. The number of observations is first declared as an integer variable N: int N; (note the use of semicolon to denote the end of a line). The number of predictors in our model, which is K. The intercept is included in this count, so we end up with two predictors (2 columns in the model matrix).
-   Transformed Data (optional): The converted data block enables data preprocessing, such as data transformation or rescaling.
-   Parameters (required): The parameters block specifies the parameters that must be assigned to prior distributions.
-   Transformed parameters (optional): Before computing the posterior, the changed parameters block provides for parameter processing, such as transformation or rescaling of the parameters.

```{r}
   model_stan = "
   data {
     // declare the input data / parameters
   }
   transformed data {
     // optional - for transforming/scaling input data
   }
   parameters {
     // define model parameters
   }
   transformed parameters {
     // optional - for deriving additional non-model parameters
     //            note however, as they are part of the sampling chain
     //            transformed parameters slow sampling down.
   }
   model {
     // specifying priors and likelihood as well as the linear predictor
   }
   generated quantities {
     // optional - derivatives (posteriors) of the samples
   }
   "
   cat(model_stan)
```

For this introduction, I'll use a very simple model in the STAN model that only involves the specification of three blocks. In the data block, I declare the variables y and x as reals (or vectors) with length equal to N and declare the sample size n sim as a positive integer number using the phrase int n sim. I define the coefficients for the linear regression alpha and beta (as real values) and the standard deviation parameter sigma in the parameters block (as a positive real number). Finally, in the model block, I give the regression coefficients and standard deviation parameters weakly informative priors, and I use a normal distribution indexed by the conditional mean mu and standard deviation sigma parameters to model the outcome data y. I give full recognition to McElreath's outstanding Statistical Rethinking (2020) book for this section.

``` r
stan_mod = "data {
  int<lower=0> N;   // number of observations
  int<lower=0> K;   // number of predictors
  matrix[N, K] X;   // predictor matrix
  vector[N] y;      // outcome vector
}
parameters {
  real alpha;           // intercept
  vector[K] beta;       // coefficients for predictors
  real<lower=0> sigma;  // error scale
}
model {
  y ~ normal(alpha + X * beta, sigma);  // target density
}"

writeLines(stan_mod, con = "stan_mod.stan")

cat(stan_mod)
```

## Fit the model

We'll need to organise the information into a list for Stan. This list should contain everything we defined in the data block of our Stan code. Then it is possible to run the model with the stan function.

``` r
library(tidyverse)
predictors <- data %>%
  select(-y)

stan_data <- list(
  N = 1000,
  K = 5,
  X = predictors,
  y = data$y
)

fit_rstan <- rstan::stan(
  file = "stan_mod.stan",
  data = stan_data
)
```

``` r
fit_rstan
```

```         
## Inference for Stan model: anon_model.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##             mean se_mean   sd     2.5%      25%      50%      75%    97.5%
## alpha      40.19    0.00 0.15    39.89    40.08    40.19    40.29    40.48
## beta[1]    -2.07    0.00 0.16    -2.39    -2.18    -2.07    -1.97    -1.76
## beta[2]     2.73    0.00 0.16     2.42     2.62     2.73     2.84     3.04
## beta[3]     3.83    0.00 0.16     3.51     3.72     3.83     3.93     4.13
## beta[4]     1.23    0.00 0.16     0.92     1.11     1.23     1.34     1.54
## beta[5]     0.34    0.00 0.16     0.04     0.23     0.34     0.45     0.65
## sigma       4.96    0.00 0.11     4.74     4.88     4.95     5.03     5.17
## lp__    -2098.32    0.04 1.88 -2102.75 -2099.39 -2097.99 -2096.92 -2095.67
##         n_eff Rhat
## alpha    5280    1
## beta[1]  5552    1
## beta[2]  6251    1
## beta[3]  5988    1
## beta[4]  6172    1
## beta[5]  6343    1
## sigma    5687    1
## lp__     1916    1
## 
## Samples were drawn using NUTS(diag_e) at Wed May  4 13:23:35 2022.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).
```

## MCMC diagnostics

For Bayesian analysis, it is required to investigate the properties of the MCMC chains and the sampler in general, in addition to the standard model diagnostic checks (such as residual plots). Remember that the goal of MCMC sampling is to reproduce the posterior distribution of the model likelihood and priors by formulating a probability distribution. This method reliable, only if the MCMC samples accurately reflect the posterior. For each parameter, traceplots show the MCMC sample values after each iteration along the chain. Bad chain mixing (any pattern) indicates that the MCMC sample chains may not have spanned all aspects of the posterior distribution and that further iterations are needed to ensure that the distribution is accurately represented. Each parameter's autocorrelation graphic shows the degree of correlation between MCMC samples separated by different lags. The degree of correlation between each MCMC sample and itself, for example, is represented by a lag of (obviously this will be a correlation of ). The MCMC samples should be independent in order to obtain unbiased parameter estimations (uncorrelated). For each parameter, the potential scale reduction factor (Rhat) statistic offers a measure of sampling efficiency/effectiveness. All values should, in theory, be less than 1, if the sampler has values of or greater than 1, it is likely that it was not particularly efficient or effective. A misspecified model or extremely unclear priors that lead to misspecified parameter space might lead to this. We should have looked at the convergence diagnostics before looking at the summaries. For the effects model, we utilise the package mcmcplots to generate density and trace graphs. It's crucial to evaluate if the chains have converged when using MCMC to fit a model. To visually inspect MCMC diagnostics, we propose the bayesplot software. The bayesplot package includes routines for displaying MCMC diagnostics and supports model objects from both rstan and rstanarm. We'll show how to make a trace plot with the mcmc_trace() function and a plot of Rhat values with the mcmc rhat() function. By printing the model fit, we can examine the parameters in the console. For each parameter, we derive posterior means, standard errors, and quantiles. n eff and Rhat are two other terms we use. These are the results of Stan's engine's exploration of the parameter space. For the time being, it's enough to know that when Rhat is 1, everything is well.

``` r
fit_rstan %>%
  mcmc_trace()
```

![](/images/bayesplots-1.png)

``` r
fit_rstan %>%
  rhat() %>%
  mcmc_rhat() +
  yaxis_text()
```

## Conclusions

To finish this post, I'd like to point out that the rstanarm package makes it possible to fit STAN models without having to write them down and instead using standard R syntax, such as that found in a glm(). So, what's the point of learning all this STAN jargon? It depends: if you're merely fitting "classical" models to your data with no fanfare, just use rstanarm; it'll save you time and the models in this package are unquestionably better parametrized (i.e. faster) than the one I presented here. Learning STAN, on the other hand, is a good approach to get into a very flexible and strong language that will continue to evolve if you believe you will need to fit your own models one day. rstanarm is a package that acts as a user interface for Stan on the front end, and enables R users to create Bayesian models without needing to learn how to code in Stan. Using the standard formula and data, you can fit a model in rstanarm. If you want to use rstan to fit a different model type, you'll have to code it yourself. The prefix *stan\_* precedes the model fitting functions and is followed by the model type. stan glm() and stan glmer() are two examples. A complete list of rstanarm functions can be found on Cran's package guide. We provided only a quick overview of how to fit simple linear regression models with the Bayesian software STAN and the rstan library and how to get a collection of useful summaries from the models. This is just a taste of the many models that STAN can fit; in future postings, we'll look at generalised linear models, as well as non-normal models with various link functions and hierarchical models. The STAN model as provided here is quite adaptable and able to accommodate datasets of various sizes. Although this may appear to be a more difficult technique than just fitting a linear model in a frequentist framework, the real benefits of Bayesian methods become apparent as the analysis becomes more sophisticated (which is often the case in real applications), the flexibility of Bayesian modelling makes it very simple to account for increasingly complicated models.

## References

-   [Stan Functions Reference](https://mc-stan.org/docs/2_18/functions-reference/)

-   [Stan Users Guide](https://mc-stan.org/docs/2_29/stan-users-guide/index.html#overview)

-   [Some things i've learned about stan](https://www.alexpghayes.com/blog/some-things-ive-learned-about-stan/)

-   [Stan (+R) Workshop](https://rpruim.github.io/StanWorkshop/)\# The pool of tears

<!--chapter:end:02-Stan.Rmd-->

# Bayesian Regression Models for Non-Normal Data


Our last post covered how to do bayesian regression for normally distributed data in R using STAN. In this post, we'll take a look at how to fit a regression model adapted to non-normal model in STAN using two common distributions seen in empirical data, namely, binomial and negative-binomial. As mentioned before in Bayesian modelling we use a set of sampling methods known as Markov Chain Monte Carlo (MCMC), we define a statistical model and find probabilistic estimates for the parameters. 


## Logistic Regression

The likelihood of a binary outcome, such as pass or fail, is estimated using logistic models (but this model can be extended to include more than two outcomes). This is accomplished by using the logit function to convert a standard regression. The main parameter that we focus on here describes odds of the outcome derived from probabilities and transformed using a logit function.  The following is the code for a logistic regression model with one predictor and an intercept.



```r
library(tidyverse)
```


```r
library(kableExtra)
library(arm) 
library(emdbook) 
library(rstan)
library(rstanarm) 

set.seed(1234)
 x1 = rnorm(10000)           

 z = 1 + 2*x1       
 pr = 1/(1+exp(-z))         
 y = rbinom(10000,1,pr)     

 df = data.frame(y=y,x1=x1)
 

 
head(df)%>%kableExtra::kable()
```

<table>
 <thead>
  <tr>
   <th style="text-align:right;"> y </th>
   <th style="text-align:right;"> x1 </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> -1.2070657 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0.2774292 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 1.0844412 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> -2.3456977 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0.4291247 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0.5060559 </td>
  </tr>
</tbody>
</table>



```r
 glm( y~x1,data=df,family="binomial")%>%summary()%>%pluck(coefficients)%>%kableExtra::kable()
```

<table>
 <thead>
  <tr>
   <th style="text-align:left;">   </th>
   <th style="text-align:right;"> Estimate </th>
   <th style="text-align:right;"> Std. Error </th>
   <th style="text-align:right;"> z value </th>
   <th style="text-align:right;"> Pr(&gt;|z|) </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> (Intercept) </td>
   <td style="text-align:right;"> 1.036922 </td>
   <td style="text-align:right;"> 0.0294863 </td>
   <td style="text-align:right;"> 35.16621 </td>
   <td style="text-align:right;"> 0 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> x1 </td>
   <td style="text-align:right;"> 1.979352 </td>
   <td style="text-align:right;"> 0.0417219 </td>
   <td style="text-align:right;"> 47.44162 </td>
   <td style="text-align:right;"> 0 </td>
  </tr>
</tbody>
</table>






```r
   model_stan = "
   data {
  int<lower=0> N;
  vector[N] x;
  int<lower=0,upper=1> y[N];
}
parameters {
  real alpha;
  real beta;
}
model {
  y ~ bernoulli_logit(alpha + beta * x);
}
   "
writeLines(model_stan, con = "model_stan.stan")
   cat(model_stan)
```

```
## 
##    data {
##   int<lower=0> N;
##   vector[N] x;
##   int<lower=0,upper=1> y[N];
## }
## parameters {
##   real alpha;
##   real beta;
## }
## model {
##   y ~ bernoulli_logit(alpha + beta * x);
## }
## 
```


```r
stan_data <- list(
  N = 10000,
  x = df$x1,
  y = df$y
)

fit_rstan <- rstan::stan(
  file = "model_stan.stan",
  data = stan_data
)
```


```r
fit_rstan
```

```
## Inference for Stan model: anon_model.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##           mean se_mean   sd     2.5%      25%      50%      75%    97.5% n_eff
## alpha     1.04    0.00 0.03     0.98     1.02     1.04     1.06     1.10  1936
## beta      1.98    0.00 0.04     1.90     1.95     1.98     2.01     2.06  1804
## lp__  -4339.45    0.03 1.05 -4342.29 -4339.80 -4339.12 -4338.74 -4338.49  1479
##       Rhat
## alpha    1
## beta     1
## lp__     1
## 
## Samples were drawn using NUTS(diag_e) at Tue Jun 21 14:03:59 2022.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).
```

This also can be expanded to several predictors. Below you can find the code as well as some recommendations for making sense of priors.




```r
set.seed(1234)
 x1 = rnorm(10000)           
 x2 = rnorm(10000)
 x3 = rnorm(10000)
 z = 1 + 2*x1 + 3*x2 - 1*x3       
 pr = 1/(1+exp(-z))         
 y = rbinom(10000,1,pr)     

 df2 = data.frame(y=y,x1=x1,x2=x2,x3=x3)
 

 
head(df2)%>%kableExtra::kable()
```

<table>
 <thead>
  <tr>
   <th style="text-align:right;"> y </th>
   <th style="text-align:right;"> x1 </th>
   <th style="text-align:right;"> x2 </th>
   <th style="text-align:right;"> x3 </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> -1.2070657 </td>
   <td style="text-align:right;"> -1.8168975 </td>
   <td style="text-align:right;"> -1.6878627 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0.2774292 </td>
   <td style="text-align:right;"> 0.6271668 </td>
   <td style="text-align:right;"> -0.9552011 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 1.0844412 </td>
   <td style="text-align:right;"> 0.5180921 </td>
   <td style="text-align:right;"> -0.6480572 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> -2.3456977 </td>
   <td style="text-align:right;"> 0.1409218 </td>
   <td style="text-align:right;"> 0.2610342 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0.4291247 </td>
   <td style="text-align:right;"> 1.4572719 </td>
   <td style="text-align:right;"> -1.2196940 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 1 </td>
   <td style="text-align:right;"> 0.5060559 </td>
   <td style="text-align:right;"> -0.4935965 </td>
   <td style="text-align:right;"> -1.5501888 </td>
  </tr>
</tbody>
</table>




```r
 glm( y~x1+x2+x3,data=df2,family="binomial")%>%summary()%>%pluck(coefficients)%>%kableExtra::kable()
```

<table>
 <thead>
  <tr>
   <th style="text-align:left;">   </th>
   <th style="text-align:right;"> Estimate </th>
   <th style="text-align:right;"> Std. Error </th>
   <th style="text-align:right;"> z value </th>
   <th style="text-align:right;"> Pr(&gt;|z|) </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> (Intercept) </td>
   <td style="text-align:right;"> 1.0108545 </td>
   <td style="text-align:right;"> 0.0367817 </td>
   <td style="text-align:right;"> 27.48253 </td>
   <td style="text-align:right;"> 0 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> x1 </td>
   <td style="text-align:right;"> 1.9471836 </td>
   <td style="text-align:right;"> 0.0494298 </td>
   <td style="text-align:right;"> 39.39289 </td>
   <td style="text-align:right;"> 0 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> x2 </td>
   <td style="text-align:right;"> 2.9598129 </td>
   <td style="text-align:right;"> 0.0641890 </td>
   <td style="text-align:right;"> 46.11088 </td>
   <td style="text-align:right;"> 0 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> x3 </td>
   <td style="text-align:right;"> -0.9448316 </td>
   <td style="text-align:right;"> 0.0372918 </td>
   <td style="text-align:right;"> -25.33619 </td>
   <td style="text-align:right;"> 0 </td>
  </tr>
</tbody>
</table>




```r
model_stan2 ="


data {
  int<lower=0> N;   // number of observations
  int<lower=0> K;   // number of predictors
  matrix[N, K] X;   // predictor matrix
  int<lower=0,upper=1> y[N];      // outcome vector
}
parameters {
  real alpha;           // intercept
  vector[K] beta;       // coefficients for predictors
}
model {
  y ~ bernoulli_logit(alpha + X * beta); 
}

"

writeLines(model_stan2, con = "model_stan2.stan")
cat(model_stan2)
```

```
## 
## 
## 
## data {
##   int<lower=0> N;   // number of observations
##   int<lower=0> K;   // number of predictors
##   matrix[N, K] X;   // predictor matrix
##   int<lower=0,upper=1> y[N];      // outcome vector
## }
## parameters {
##   real alpha;           // intercept
##   vector[K] beta;       // coefficients for predictors
## }
## model {
##   y ~ bernoulli_logit(alpha + X * beta); 
## }
```




```r
predictors <- df2[,2:4]

stan_data2 <- list(
  N = 10000,
  K = 3,
  X = predictors,
  y = df2$y
)


fit_rstan2 <- rstan::stan(
  file = "model_stan2.stan",
  data = stan_data2
)
```

```
## Trying to compile a simple C file
```


```r
fit_rstan2
```

```
## Inference for Stan model: anon_model.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##             mean se_mean   sd     2.5%      25%      50%      75%    97.5%
## alpha       1.01    0.00 0.04     0.94     0.99     1.01     1.04     1.08
## beta[1]     1.95    0.00 0.05     1.85     1.92     1.95     1.98     2.05
## beta[2]     2.96    0.00 0.06     2.84     2.92     2.96     3.00     3.09
## beta[3]    -0.95    0.00 0.04    -1.02    -0.97    -0.95    -0.92    -0.87
## lp__    -3006.39    0.03 1.38 -3009.82 -3007.07 -3006.08 -3005.37 -3004.68
##         n_eff Rhat
## alpha    2845    1
## beta[1]  2338    1
## beta[2]  2221    1
## beta[3]  2160    1
## lp__     1788    1
## 
## Samples were drawn using NUTS(diag_e) at Tue Jun 21 14:05:38 2022.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).
```



## Negative Binomial 



The Poisson distribution, which assumes that the variance is equal to the mean, is a popular choice for modelling count data. The data are said to be overdispersed when the variance exceeds the mean, the Negative Binomial distribution can be used in this case. Note that the negative binomial distribution is a probability distribution of discrete random variables. 
 
Let's say we have a response variable y that has a negative binomial distribution and is influenced by a collection of k explanatory variables X. The negative binomial distribution to model this data has two parameters: 
The μ or the expected value that need to be positive so a log link function can be used to map the linear predictor (the explanatory variables times the regression parameters) and ϕ which is the overdispersion parameter, where a small value means a large deviation from a Poisson distribution, as ϕ gets larger the negative binomial looks more and more like a Poisson distribution.

Let’s simulate some data and fit a STAN model to them:



```r
N<-100000
df3 <-data.frame(x1=runif(N,-2,2),x2=runif(N,-2,2))
#the model
X<-model.matrix(~x1*x2,df3)
K<-dim(X)[2] #number of regression params
#the regression slopes
betas<-runif(K,-1,1)
#the overdispersion for the simulated data
phi<-5
#simulate the response
y_nb<-rnbinom(N,size=phi,mu=exp(X%*%betas))
hist(y_nb)
```

![](/images/stan2_hist-1.png)


```r
MASS::glm.nb(y_nb ~ X[,2:K])%>%summary()%>%pluck(coefficients)%>%kableExtra::kable()
```

<table>
 <thead>
  <tr>
   <th style="text-align:left;">   </th>
   <th style="text-align:right;"> Estimate </th>
   <th style="text-align:right;"> Std. Error </th>
   <th style="text-align:right;"> z value </th>
   <th style="text-align:right;"> Pr(&gt;|z|) </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> (Intercept) </td>
   <td style="text-align:right;"> 0.0788902 </td>
   <td style="text-align:right;"> 0.0039192 </td>
   <td style="text-align:right;"> 20.12938 </td>
   <td style="text-align:right;"> 0 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> X[, 2:K]x1 </td>
   <td style="text-align:right;"> -0.0815968 </td>
   <td style="text-align:right;"> 0.0032703 </td>
   <td style="text-align:right;"> -24.95109 </td>
   <td style="text-align:right;"> 0 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> X[, 2:K]x2 </td>
   <td style="text-align:right;"> 0.7941334 </td>
   <td style="text-align:right;"> 0.0031740 </td>
   <td style="text-align:right;"> 250.20341 </td>
   <td style="text-align:right;"> 0 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> X[, 2:K]x1:x2 </td>
   <td style="text-align:right;"> 0.4301282 </td>
   <td style="text-align:right;"> 0.0026264 </td>
   <td style="text-align:right;"> 163.77308 </td>
   <td style="text-align:right;"> 0 </td>
  </tr>
</tbody>
</table>


```
data {
  int N; //the number of observations
  int K; //the number of columns in the model matrix
  int y[N]; //the response
  matrix[N,K] X; //the model matrix
}
parameters {
  vector[K] beta; //the regression parameters
  real phi; //the overdispersion parameters
}
transformed parameters {
  vector[N] mu;//the linear predictor
  mu <- exp(X*beta); //using the log link 
}
model {  
  beta[1] ~ cauchy(0,10); //prior for the intercept following Gelman 2008

  for(i in 2:K)
   beta[i] ~ cauchy(0,2.5);//prior for the slopes following Gelman 2008
  
  y ~ neg_binomial_2(mu,phi);
}
generated quantities {
 vector[N] y_rep;
 for(n in 1:N){
  y_rep[n] <- neg_binomial_2_rng(mu[n],phi); //posterior draws to get posterior predictive checks
 }
}

```


```r
stan_mod = "data {
  int N; //the number of observations
  int K; //the number of columns in the model matrix
  int y[N]; //the response
  matrix[N,K] X; //the model matrix
}
parameters {
  vector[K] beta; //the regression parameters
  real phi; //the overdispersion parameters
}
transformed parameters {
  vector[N] mu;//the linear predictor
  mu <- exp(X*beta); //using the log link 
}
model {  
  beta[1] ~ cauchy(0,10); //prior for the intercept following Gelman 2008

  for(i in 2:K)
   beta[i] ~ cauchy(0,2.5);//prior for the slopes following Gelman 2008
  
  y ~ neg_binomial_2(mu,phi);
}
"
writeLines(stan_mod, con = "stan_mod.stan")

cat(stan_mod)
```

```
## data {
##   int N; //the number of observations
##   int K; //the number of columns in the model matrix
##   int y[N]; //the response
##   matrix[N,K] X; //the model matrix
## }
## parameters {
##   vector[K] beta; //the regression parameters
##   real phi; //the overdispersion parameters
## }
## transformed parameters {
##   vector[N] mu;//the linear predictor
##   mu <- exp(X*beta); //using the log link 
## }
## model {  
##   beta[1] ~ cauchy(0,10); //prior for the intercept following Gelman 2008
## 
##   for(i in 2:K)
##    beta[i] ~ cauchy(0,2.5);//prior for the slopes following Gelman 2008
##   
##   y ~ neg_binomial_2(mu,phi);
## }
```


```r
stan_data3 <- list(
  N = N,
  K = K,
  X = X,
  y = y_nb
)
```


```r
fit_rstan3 <- rstan::stan(
  file = "stan_mod.stan",
  data = stan_data3
)
```

```r
fit_rstan3%>%summary()
```

```
## $summary
##                     mean      se_mean           sd          2.5%           25%
## beta[1]     7.899069e-02 6.825217e-05 0.0039010484  7.162531e-02  7.635923e-02
## beta[2]    -8.157657e-02 5.689044e-05 0.0032236243 -8.786688e-02 -8.376046e-02
## beta[3]     7.941039e-01 5.618629e-05 0.0031476117  7.880475e-01  7.919520e-01
## beta[4]     4.301036e-01 4.552321e-05 0.0026265744  4.248578e-01  4.283603e-01
## phi         4.908958e+00 2.039012e-03 0.0831883094  4.745765e+00  4.854542e+00

```



## Conclusion

This tutorial provided only a quick overview of how to fit logistic and negative binomial regression models with the Bayesian software STAN using the rstan library/API and to extract a collection of useful summaries from the models. Future postings will address the question of outliers and the use of robust linear models.

## References

+ [Stan Functions Reference](https://mc-stan.org/docs/2_18/functions-reference/)

+ [Stan Users Guide](https://mc-stan.org/docs/2_29/stan-users-guide/index.html#overview)

+ [Some things i've learned about stan](https://www.alexpghayes.com/blog/some-things-ive-learned-about-stan/)

+ [Stan (+R) Workshop](https://rpruim.github.io/StanWorkshop/)

<!--chapter:end:03-nonnormal.Rmd-->


# Robust t-regression

## Introduction

Simple linear regression is a widely used method for estimating the linear relation between two (or more) variables and for predicting the value of one variable (the response variable) based on the value of the other (the explanatory variable). The explanatory variable is typically represented on the x-axis and the response variable on the y-axis for visualising the results of linear regression.

The regression coefficients can be distorted by outlying data points. Due to the normality assumption used by traditional regression techniques, noisy or outlier data can greatly affect their accuracy. Since the normal distribution must move to a new place in the parameter space to best accommodate the outliers in the data, this frequently leads to an underestimate of the relationship between the variables. In a frequentist framework, creating a linear regression model that is resistant to outliers necessitates the use of very complex statistical techniques; however, in a Bayesian framework, we can achieve robustness by simply using the t-distribution. in this context, **Robust Regression** refers to regression methods which are less sensitive to outliers. Bayesian robust regression uses distributions with wider tails than the normal. This means that outliers will have less of an affect on the models and the regression line would need to move less incorporate those observations since the error distribution will not consider them as unusual.


Utilizing Student's t density with an unidentified degrees of freedom parameter is a well-liked substitution for normal errors in regression investigations. The Student's t distribution has heavier tails than the normal for low degrees of freedom, but it leans toward the normal as the degrees of freedom parameter rises. A check on the suitability of the normal is thus made possible by treating the degrees of freedom parameter as an unknown quantity that must be approximated. The degrees of freedom, or parameter ν, of this probability distribution determines how close to normal the distribution is: Low small values of produce a distribution with thicker tails (that is, a greater spread around the mean) than the normal distribution, but big values of (approximately > 30) give a distribution that is quite similar to the normal distribution. As a result, we can allow the distribution of the regression line to be as normal or non-normal as the data imply while still capturing the underlying relationship between the variables by substituting the normal distribution above with a t-distribution and adding as an extra parameter to the model.


## Concepts and code

The standard approach to linear regression is defining the equation for a straight line that represents the relationship between the variables as accurately as possible. The equation for the line defines y (the response variable) as a linear function of x (the explanatory variable):

<p align="center">  𝑦 = 𝛼 + 𝛽𝑥 + 𝜀</p>

In this equation, ε represents the error in the linear relationship: if no noise were allowed, then the paired x- and y-values would need to be arranged in a perfect straight line (for example, as in y = 2x + 1). Because we assume that the relationship between x and y is truly linear, any variation observed around the regression line must be random noise, and therefore normally distributed. From a probabilistic standpoint, such relationship between the variables could be formalised as

<p align="center"> 𝑦 ~ 𝓝(𝛼 + 𝛽𝑥, 𝜎)</p>

That is, the response variable follows a normal distribution with mean equal to the regression line, and some standard deviation σ. Such a probability distribution of the regression line is illustrated in the figure below.

The formulation of the robust simple linear regression Bayesian model is given below. We define a t likelihood for the response variable, y, and suitable vague priors on all the model parameters: normal for α and β, half-normal for σ and gamma for ν.

<p align="center"> 𝑦 ~ 𝓣(𝛼 + 𝛽𝑥, 𝜎, 𝜈) </p> 
<p align="center"> 𝛼, 𝛽 ~ 𝓝(0, 1000) </p> 
<p align="center"> 𝜎 ~ 𝓗𝓝(0, 1000) </p> 
<p align="center"> 𝜈 ~ 𝚪(2, 0.1) </p>

Below you can find R and Stan code for a simple Bayesian t-regression model with nu unknown.

First let's create data with and without ourliers

```r
library(readr)
library(tidyverse)
library(gridExtra)
library(kableExtra)
library(arm) 
library(emdbook) 
library(rstan)
library(rstanarm) 
library(brms) 
```

```r
s <- matrix(c(1, .8, 
              .8, 1), 
            nrow = 2, ncol = 2)
m <- c(3, 3)
set.seed(1234)

data_n <- MASS::mvrnorm(n = 100, mu = m, Sigma = s) %>%
  as_tibble() %>%
  rename(y = V1, x = V2)
```



```r
data_n <-
  data_n %>%
  arrange(x)

head(data_n)%>%kableExtra::kable()
```

<table>
 <thead>
  <tr>
   <th style="text-align:right;"> y </th>
   <th style="text-align:right;"> x </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:right;"> 0.9335732 </td>
   <td style="text-align:right;"> 0.6157783 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 1.0317982 </td>
   <td style="text-align:right;"> 0.8318674 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 1.9763140 </td>
   <td style="text-align:right;"> 0.9326985 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 2.3439117 </td>
   <td style="text-align:right;"> 1.1117327 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 1.4059413 </td>
   <td style="text-align:right;"> 1.1673553 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 2.0360302 </td>
   <td style="text-align:right;"> 1.3253002 </td>
  </tr>
</tbody>
</table>

```r
data_o <- data_n
data_o[c(1:2), 1] <- c(7.5, 8.5)

head(data_o)%>%kableExtra::kable()
```

<table>
 <thead>
  <tr>
   <th style="text-align:right;"> y </th>
   <th style="text-align:right;"> x </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:right;"> 7.500000 </td>
   <td style="text-align:right;"> 0.6157783 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 8.500000 </td>
   <td style="text-align:right;"> 0.8318674 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 1.976314 </td>
   <td style="text-align:right;"> 0.9326985 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 2.343912 </td>
   <td style="text-align:right;"> 1.1117327 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 1.405941 </td>
   <td style="text-align:right;"> 1.1673553 </td>
  </tr>
  <tr>
   <td style="text-align:right;"> 2.036030 </td>
   <td style="text-align:right;"> 1.3253002 </td>
  </tr>
</tbody>
</table>

```r
ols_n <- lm(data = data_n, y ~ 1 + x)
ols_o <- lm(data = data_o, y ~ 1 + x)


p1 <-
  ggplot(data = data_n, aes(x = x, y = y)) +
  stat_smooth(method = "lm", color = "grey92", fill = "grey67", alpha = 1, fullrange = T) +
  geom_point(size = 1, alpha = 3/4) +
  scale_x_continuous(limits = c(0, 9)) +
  coord_cartesian(xlim = c(0, 9), 
                  ylim = c(0, 9)) +
  labs(title = "No Outliers") +
  theme(panel.grid = element_blank())

# the data with two outliers
p2 <-
  ggplot(data = data_o, aes(x = x, y = y, color = y > 7)) +
  stat_smooth(method = "lm", color = "grey92", fill = "grey67", alpha = 1, fullrange = T) +
  geom_point(size = 1, alpha = 3/4) +
  scale_color_viridis_d(option = "A", end = 4/7) +
  scale_x_continuous(limits = c(0, 9)) +
  coord_cartesian(xlim = c(0, 9), 
                  ylim = c(0, 9)) +
  labs(title = "Two Outliers") +
  theme(panel.grid = element_blank(),
        legend.position = "none")
grid.arrange(p1 ,p2)
```

```
## `geom_smooth()` using formula 'y ~ x'
```

```
## `geom_smooth()` using formula 'y ~ x'
```

![](/images/RobReg-1-1.png)



```r
model_stan = "
   data {
    int<lower=1> N;    
    int<lower=0> M;    
    int<lower=0> P;    
    vector[N] x;       
    vector[N] y;       
    vector[M] x_cred;  
    vector[P] x_pred;  
}

parameters {
    real alpha;           
    real beta;            
    real<lower=0> sigma;  
    real<lower=1> nu;     
}

transformed parameters {
    vector[N] mu = alpha + beta * x;            
    vector[M] mu_cred = alpha + beta * x_cred;  
    vector[P] mu_pred = alpha + beta * x_pred;  
}

model {
    y ~ student_t(nu, mu, sigma);
    alpha ~ normal(0, 1000);
    beta ~ normal(0, 1000);
    sigma ~ normal(0, 1000);
    nu ~ gamma(2, 0.1);
}

generated quantities {
    real y_pred[P];
    for (p in 1:P) {
        y_pred[p] = student_t_rng(nu, mu_pred[p], sigma);
    }
}
   "
writeLines(model_stan, con = "model_stan.stan")
   cat(model_stan)
```

```
## 
##    data {
##     int<lower=1> N;    
##     int<lower=0> M;    
##     int<lower=0> P;    
##     vector[N] x;       
##     vector[N] y;       
##     vector[M] x_cred;  
##     vector[P] x_pred;  
## }
## 
## parameters {
##     real alpha;           
##     real beta;            
##     real<lower=0> sigma;  
##     real<lower=1> nu;     
## }
## 
## transformed parameters {
##     vector[N] mu = alpha + beta * x;            
##     vector[M] mu_cred = alpha + beta * x_cred;  
##     vector[P] mu_pred = alpha + beta * x_pred;  
## }
## 
## model {
##     y ~ student_t(nu, mu, sigma);
##     alpha ~ normal(0, 1000);
##     beta ~ normal(0, 1000);
##     sigma ~ normal(0, 1000);
##     nu ~ gamma(2, 0.1);
## }
## 
## generated quantities {
##     real y_pred[P];
##     for (p in 1:P) {
##         y_pred[p] = student_t_rng(nu, mu_pred[p], sigma);
##     }
## }
## 
```



```r
stan_data <- list(x=data_o$x,
     y=data_o$y,
     N=length(data_o$y),
     M=0, P=0, x_cred=numeric(0), x_pred=numeric(0))



fit_rstan <- rstan::stan(
  file = "model_stan.stan",
  data = stan_data
)
```

```
## Trying to compile a simple C file
```

```
## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c
## clang -mmacosx-version-min=10.13 -I"/Library/Frameworks/R.framework/Resources/include" -DNDEBUG   -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include/"  -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/"  -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/unsupported"  -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/BH/include" -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/src/"  -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/"  -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppParallel/include/"  -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/rstan/include" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -DBOOST_NO_AUTO_PTR  -include '/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include   -fPIC  -Wall -g -O2  -c foo.c -o foo.o
## In file included from <built-in>:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:
## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:88:
## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name 'namespace'
## namespace Eigen {
## ^
## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected ';' after top level declarator
## namespace Eigen {
##                ^
##                ;
## In file included from <built-in>:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:
## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1:
## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: 'complex' file not found
## #include <complex>
##          ^~~~~~~~~
## 3 errors generated.
## make: *** [foo.o] Error 1
## 
## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 3.9e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.39 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.114 seconds (Warm-up)
## Chain 1:                0.122 seconds (Sampling)
## Chain 1:                0.236 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 1.3e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.126 seconds (Warm-up)
## Chain 2:                0.11 seconds (Sampling)
## Chain 2:                0.236 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 9e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.122 seconds (Warm-up)
## Chain 3:                0.111 seconds (Sampling)
## Chain 3:                0.233 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 1.1e-05 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.134 seconds (Warm-up)
## Chain 4:                0.113 seconds (Sampling)
## Chain 4:                0.247 seconds (Total)
## Chain 4:
```


```r
trace <- stan_trace(fit_rstan, pars=c("alpha", "beta", "sigma", "nu"))
trace + scale_color_brewer(type = "div") + theme(legend.position = "none")
```

```
## Scale for 'colour' is already present. Adding another scale for 'colour',
## which will replace the existing scale.
```

![](/images/RobReg-4-1.png)


```r
stan_dens(fit_rstan, pars=c("alpha", "beta", "sigma", "nu"), fill = "skyblue")
```

![](/images/RobReg-5-1.png)


```r
stan_plot(fit_rstan, pars=c("alpha", "beta", "sigma", "nu"), show_density = TRUE, fill_color = "maroon")
```

```
## ci_level: 0.8 (80% intervals)
```

```
## outer_level: 0.95 (95% intervals)
```

![](/images/RobReg-6-1.png)

Bayesian regression models can be used to estimate highest posterior density  or credible intervals (intervals of the distribution of the regression linefor), and prediction values, through predictive posterior distributions. More specifically, the prediction intervals are obtained by first drawing samples of the mean response at specific x-values of interest and then, for each of these samples, randomly selecting a y-value from a t-distribution with location mu pred. In contrast, the credible intervals are obtained by drawing MCMC samples of the mean response at regularly spaced points along the x-axis. The distributions of mu cred and y pred are represented, respectively, by the credible and prediction intervals.



```r
x.cred = seq(from=min(data_o$x),
             to=max(data_o$x),
             length.out=50)


x.pred = c(0, 8)
```


```r
stan_data2 <- list(x=data_o$x,
                  y=data_o$y,
                  N=length(data_o$y),
                  x_cred=x.cred,
                  x_pred=x.pred,
                  M=length(x.cred),
                  P=length(x.pred))
```


```r
fit_rstan2 <- rstan::stan(
  file = "model_stan.stan",
  data = stan_data2
)
```

```
## 
## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 2.6e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.157 seconds (Warm-up)
## Chain 1:                0.134 seconds (Sampling)
## Chain 1:                0.291 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 1e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.148 seconds (Warm-up)
## Chain 2:                0.101 seconds (Sampling)
## Chain 2:                0.249 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 1.4e-05 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.165 seconds (Warm-up)
## Chain 3:                0.132 seconds (Sampling)
## Chain 3:                0.297 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 1.2e-05 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.151 seconds (Warm-up)
## Chain 4:                0.145 seconds (Sampling)
## Chain 4:                0.296 seconds (Total)
## Chain 4:
```

For each value in x.cred, the mu cred parameter's MCMC samples are contained in a separate column of mu.cred. In a similar manner, the columns of y.pred include the MCMC samples of the posterior expected response values (y pred values) for the x-values in x.pred. 



```r
summary(extract(fit_rstan2, "mu_cred")[[1]])%>%kableExtra::kable()
```

<table>
 <thead>
  <tr>
   <th style="text-align:left;">   </th>
   <th style="text-align:left;">       V1 </th>
   <th style="text-align:left;">       V2 </th>
   <th style="text-align:left;">       V3 </th>
   <th style="text-align:left;">       V4 </th>
   <th style="text-align:left;">       V5 </th>
   <th style="text-align:left;">       V6 </th>
   <th style="text-align:left;">       V7 </th>
   <th style="text-align:left;">       V8 </th>
   <th style="text-align:left;">       V9 </th>
   <th style="text-align:left;">      V10 </th>
   <th style="text-align:left;">      V11 </th>
   <th style="text-align:left;">      V12 </th>
   <th style="text-align:left;">      V13 </th>
   <th style="text-align:left;">      V14 </th>
   <th style="text-align:left;">      V15 </th>
   <th style="text-align:left;">      V16 </th>
   <th style="text-align:left;">      V17 </th>
   <th style="text-align:left;">      V18 </th>
   <th style="text-align:left;">      V19 </th>
   <th style="text-align:left;">      V20 </th>
   <th style="text-align:left;">      V21 </th>
   <th style="text-align:left;">      V22 </th>
   <th style="text-align:left;">      V23 </th>
   <th style="text-align:left;">      V24 </th>
   <th style="text-align:left;">      V25 </th>
   <th style="text-align:left;">      V26 </th>
   <th style="text-align:left;">      V27 </th>
   <th style="text-align:left;">      V28 </th>
   <th style="text-align:left;">      V29 </th>
   <th style="text-align:left;">      V30 </th>
   <th style="text-align:left;">      V31 </th>
   <th style="text-align:left;">      V32 </th>
   <th style="text-align:left;">      V33 </th>
   <th style="text-align:left;">      V34 </th>
   <th style="text-align:left;">      V35 </th>
   <th style="text-align:left;">      V36 </th>
   <th style="text-align:left;">      V37 </th>
   <th style="text-align:left;">      V38 </th>
   <th style="text-align:left;">      V39 </th>
   <th style="text-align:left;">      V40 </th>
   <th style="text-align:left;">      V41 </th>
   <th style="text-align:left;">      V42 </th>
   <th style="text-align:left;">      V43 </th>
   <th style="text-align:left;">      V44 </th>
   <th style="text-align:left;">      V45 </th>
   <th style="text-align:left;">      V46 </th>
   <th style="text-align:left;">      V47 </th>
   <th style="text-align:left;">      V48 </th>
   <th style="text-align:left;">      V49 </th>
   <th style="text-align:left;">      V50 </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;"> Min.   :0.4535 </td>
   <td style="text-align:left;"> Min.   :0.553 </td>
   <td style="text-align:left;"> Min.   :0.6525 </td>
   <td style="text-align:left;"> Min.   :0.752 </td>
   <td style="text-align:left;"> Min.   :0.8515 </td>
   <td style="text-align:left;"> Min.   :0.951 </td>
   <td style="text-align:left;"> Min.   :1.051 </td>
   <td style="text-align:left;"> Min.   :1.150 </td>
   <td style="text-align:left;"> Min.   :1.250 </td>
   <td style="text-align:left;"> Min.   :1.349 </td>
   <td style="text-align:left;"> Min.   :1.449 </td>
   <td style="text-align:left;"> Min.   :1.548 </td>
   <td style="text-align:left;"> Min.   :1.642 </td>
   <td style="text-align:left;"> Min.   :1.734 </td>
   <td style="text-align:left;"> Min.   :1.826 </td>
   <td style="text-align:left;"> Min.   :1.918 </td>
   <td style="text-align:left;"> Min.   :2.010 </td>
   <td style="text-align:left;"> Min.   :2.102 </td>
   <td style="text-align:left;"> Min.   :2.194 </td>
   <td style="text-align:left;"> Min.   :2.285 </td>
   <td style="text-align:left;"> Min.   :2.377 </td>
   <td style="text-align:left;"> Min.   :2.458 </td>
   <td style="text-align:left;"> Min.   :2.531 </td>
   <td style="text-align:left;"> Min.   :2.603 </td>
   <td style="text-align:left;"> Min.   :2.675 </td>
   <td style="text-align:left;"> Min.   :2.747 </td>
   <td style="text-align:left;"> Min.   :2.820 </td>
   <td style="text-align:left;"> Min.   :2.892 </td>
   <td style="text-align:left;"> Min.   :2.964 </td>
   <td style="text-align:left;"> Min.   :3.037 </td>
   <td style="text-align:left;"> Min.   :3.109 </td>
   <td style="text-align:left;"> Min.   :3.181 </td>
   <td style="text-align:left;"> Min.   :3.245 </td>
   <td style="text-align:left;"> Min.   :3.297 </td>
   <td style="text-align:left;"> Min.   :3.349 </td>
   <td style="text-align:left;"> Min.   :3.401 </td>
   <td style="text-align:left;"> Min.   :3.453 </td>
   <td style="text-align:left;"> Min.   :3.505 </td>
   <td style="text-align:left;"> Min.   :3.557 </td>
   <td style="text-align:left;"> Min.   :3.609 </td>
   <td style="text-align:left;"> Min.   :3.661 </td>
   <td style="text-align:left;"> Min.   :3.713 </td>
   <td style="text-align:left;"> Min.   :3.765 </td>
   <td style="text-align:left;"> Min.   :3.817 </td>
   <td style="text-align:left;"> Min.   :3.869 </td>
   <td style="text-align:left;"> Min.   :3.921 </td>
   <td style="text-align:left;"> Min.   :3.973 </td>
   <td style="text-align:left;"> Min.   :4.025 </td>
   <td style="text-align:left;"> Min.   :4.077 </td>
   <td style="text-align:left;"> Min.   :4.129 </td>
  </tr>
  <tr>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;"> 1st Qu.:0.9464 </td>
   <td style="text-align:left;"> 1st Qu.:1.029 </td>
   <td style="text-align:left;"> 1st Qu.:1.1113 </td>
   <td style="text-align:left;"> 1st Qu.:1.193 </td>
   <td style="text-align:left;"> 1st Qu.:1.2762 </td>
   <td style="text-align:left;"> 1st Qu.:1.359 </td>
   <td style="text-align:left;"> 1st Qu.:1.441 </td>
   <td style="text-align:left;"> 1st Qu.:1.524 </td>
   <td style="text-align:left;"> 1st Qu.:1.605 </td>
   <td style="text-align:left;"> 1st Qu.:1.688 </td>
   <td style="text-align:left;"> 1st Qu.:1.769 </td>
   <td style="text-align:left;"> 1st Qu.:1.851 </td>
   <td style="text-align:left;"> 1st Qu.:1.931 </td>
   <td style="text-align:left;"> 1st Qu.:2.012 </td>
   <td style="text-align:left;"> 1st Qu.:2.093 </td>
   <td style="text-align:left;"> 1st Qu.:2.174 </td>
   <td style="text-align:left;"> 1st Qu.:2.254 </td>
   <td style="text-align:left;"> 1st Qu.:2.335 </td>
   <td style="text-align:left;"> 1st Qu.:2.415 </td>
   <td style="text-align:left;"> 1st Qu.:2.495 </td>
   <td style="text-align:left;"> 1st Qu.:2.574 </td>
   <td style="text-align:left;"> 1st Qu.:2.654 </td>
   <td style="text-align:left;"> 1st Qu.:2.733 </td>
   <td style="text-align:left;"> 1st Qu.:2.810 </td>
   <td style="text-align:left;"> 1st Qu.:2.888 </td>
   <td style="text-align:left;"> 1st Qu.:2.966 </td>
   <td style="text-align:left;"> 1st Qu.:3.043 </td>
   <td style="text-align:left;"> 1st Qu.:3.119 </td>
   <td style="text-align:left;"> 1st Qu.:3.195 </td>
   <td style="text-align:left;"> 1st Qu.:3.270 </td>
   <td style="text-align:left;"> 1st Qu.:3.344 </td>
   <td style="text-align:left;"> 1st Qu.:3.419 </td>
   <td style="text-align:left;"> 1st Qu.:3.493 </td>
   <td style="text-align:left;"> 1st Qu.:3.569 </td>
   <td style="text-align:left;"> 1st Qu.:3.644 </td>
   <td style="text-align:left;"> 1st Qu.:3.717 </td>
   <td style="text-align:left;"> 1st Qu.:3.792 </td>
   <td style="text-align:left;"> 1st Qu.:3.867 </td>
   <td style="text-align:left;"> 1st Qu.:3.942 </td>
   <td style="text-align:left;"> 1st Qu.:4.015 </td>
   <td style="text-align:left;"> 1st Qu.:4.089 </td>
   <td style="text-align:left;"> 1st Qu.:4.163 </td>
   <td style="text-align:left;"> 1st Qu.:4.237 </td>
   <td style="text-align:left;"> 1st Qu.:4.311 </td>
   <td style="text-align:left;"> 1st Qu.:4.386 </td>
   <td style="text-align:left;"> 1st Qu.:4.460 </td>
   <td style="text-align:left;"> 1st Qu.:4.534 </td>
   <td style="text-align:left;"> 1st Qu.:4.608 </td>
   <td style="text-align:left;"> 1st Qu.:4.681 </td>
   <td style="text-align:left;"> 1st Qu.:4.753 </td>
  </tr>
  <tr>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;"> Median :1.0605 </td>
   <td style="text-align:left;"> Median :1.138 </td>
   <td style="text-align:left;"> Median :1.2165 </td>
   <td style="text-align:left;"> Median :1.295 </td>
   <td style="text-align:left;"> Median :1.3724 </td>
   <td style="text-align:left;"> Median :1.450 </td>
   <td style="text-align:left;"> Median :1.528 </td>
   <td style="text-align:left;"> Median :1.606 </td>
   <td style="text-align:left;"> Median :1.684 </td>
   <td style="text-align:left;"> Median :1.762 </td>
   <td style="text-align:left;"> Median :1.840 </td>
   <td style="text-align:left;"> Median :1.918 </td>
   <td style="text-align:left;"> Median :1.996 </td>
   <td style="text-align:left;"> Median :2.074 </td>
   <td style="text-align:left;"> Median :2.152 </td>
   <td style="text-align:left;"> Median :2.230 </td>
   <td style="text-align:left;"> Median :2.308 </td>
   <td style="text-align:left;"> Median :2.386 </td>
   <td style="text-align:left;"> Median :2.463 </td>
   <td style="text-align:left;"> Median :2.542 </td>
   <td style="text-align:left;"> Median :2.619 </td>
   <td style="text-align:left;"> Median :2.698 </td>
   <td style="text-align:left;"> Median :2.775 </td>
   <td style="text-align:left;"> Median :2.853 </td>
   <td style="text-align:left;"> Median :2.931 </td>
   <td style="text-align:left;"> Median :3.010 </td>
   <td style="text-align:left;"> Median :3.087 </td>
   <td style="text-align:left;"> Median :3.165 </td>
   <td style="text-align:left;"> Median :3.243 </td>
   <td style="text-align:left;"> Median :3.322 </td>
   <td style="text-align:left;"> Median :3.400 </td>
   <td style="text-align:left;"> Median :3.478 </td>
   <td style="text-align:left;"> Median :3.557 </td>
   <td style="text-align:left;"> Median :3.634 </td>
   <td style="text-align:left;"> Median :3.713 </td>
   <td style="text-align:left;"> Median :3.790 </td>
   <td style="text-align:left;"> Median :3.868 </td>
   <td style="text-align:left;"> Median :3.947 </td>
   <td style="text-align:left;"> Median :4.024 </td>
   <td style="text-align:left;"> Median :4.102 </td>
   <td style="text-align:left;"> Median :4.180 </td>
   <td style="text-align:left;"> Median :4.257 </td>
   <td style="text-align:left;"> Median :4.335 </td>
   <td style="text-align:left;"> Median :4.413 </td>
   <td style="text-align:left;"> Median :4.491 </td>
   <td style="text-align:left;"> Median :4.569 </td>
   <td style="text-align:left;"> Median :4.647 </td>
   <td style="text-align:left;"> Median :4.726 </td>
   <td style="text-align:left;"> Median :4.803 </td>
   <td style="text-align:left;"> Median :4.881 </td>
  </tr>
  <tr>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;"> Mean   :1.0605 </td>
   <td style="text-align:left;"> Mean   :1.138 </td>
   <td style="text-align:left;"> Mean   :1.2164 </td>
   <td style="text-align:left;"> Mean   :1.294 </td>
   <td style="text-align:left;"> Mean   :1.3723 </td>
   <td style="text-align:left;"> Mean   :1.450 </td>
   <td style="text-align:left;"> Mean   :1.528 </td>
   <td style="text-align:left;"> Mean   :1.606 </td>
   <td style="text-align:left;"> Mean   :1.684 </td>
   <td style="text-align:left;"> Mean   :1.762 </td>
   <td style="text-align:left;"> Mean   :1.840 </td>
   <td style="text-align:left;"> Mean   :1.918 </td>
   <td style="text-align:left;"> Mean   :1.996 </td>
   <td style="text-align:left;"> Mean   :2.074 </td>
   <td style="text-align:left;"> Mean   :2.152 </td>
   <td style="text-align:left;"> Mean   :2.230 </td>
   <td style="text-align:left;"> Mean   :2.308 </td>
   <td style="text-align:left;"> Mean   :2.386 </td>
   <td style="text-align:left;"> Mean   :2.464 </td>
   <td style="text-align:left;"> Mean   :2.542 </td>
   <td style="text-align:left;"> Mean   :2.619 </td>
   <td style="text-align:left;"> Mean   :2.697 </td>
   <td style="text-align:left;"> Mean   :2.775 </td>
   <td style="text-align:left;"> Mean   :2.853 </td>
   <td style="text-align:left;"> Mean   :2.931 </td>
   <td style="text-align:left;"> Mean   :3.009 </td>
   <td style="text-align:left;"> Mean   :3.087 </td>
   <td style="text-align:left;"> Mean   :3.165 </td>
   <td style="text-align:left;"> Mean   :3.243 </td>
   <td style="text-align:left;"> Mean   :3.321 </td>
   <td style="text-align:left;"> Mean   :3.399 </td>
   <td style="text-align:left;"> Mean   :3.477 </td>
   <td style="text-align:left;"> Mean   :3.555 </td>
   <td style="text-align:left;"> Mean   :3.633 </td>
   <td style="text-align:left;"> Mean   :3.711 </td>
   <td style="text-align:left;"> Mean   :3.789 </td>
   <td style="text-align:left;"> Mean   :3.867 </td>
   <td style="text-align:left;"> Mean   :3.945 </td>
   <td style="text-align:left;"> Mean   :4.023 </td>
   <td style="text-align:left;"> Mean   :4.101 </td>
   <td style="text-align:left;"> Mean   :4.178 </td>
   <td style="text-align:left;"> Mean   :4.256 </td>
   <td style="text-align:left;"> Mean   :4.334 </td>
   <td style="text-align:left;"> Mean   :4.412 </td>
   <td style="text-align:left;"> Mean   :4.490 </td>
   <td style="text-align:left;"> Mean   :4.568 </td>
   <td style="text-align:left;"> Mean   :4.646 </td>
   <td style="text-align:left;"> Mean   :4.724 </td>
   <td style="text-align:left;"> Mean   :4.802 </td>
   <td style="text-align:left;"> Mean   :4.880 </td>
  </tr>
  <tr>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;"> 3rd Qu.:1.1739 </td>
   <td style="text-align:left;"> 3rd Qu.:1.247 </td>
   <td style="text-align:left;"> 3rd Qu.:1.3213 </td>
   <td style="text-align:left;"> 3rd Qu.:1.394 </td>
   <td style="text-align:left;"> 3rd Qu.:1.4688 </td>
   <td style="text-align:left;"> 3rd Qu.:1.543 </td>
   <td style="text-align:left;"> 3rd Qu.:1.616 </td>
   <td style="text-align:left;"> 3rd Qu.:1.691 </td>
   <td style="text-align:left;"> 3rd Qu.:1.766 </td>
   <td style="text-align:left;"> 3rd Qu.:1.840 </td>
   <td style="text-align:left;"> 3rd Qu.:1.915 </td>
   <td style="text-align:left;"> 3rd Qu.:1.989 </td>
   <td style="text-align:left;"> 3rd Qu.:2.063 </td>
   <td style="text-align:left;"> 3rd Qu.:2.137 </td>
   <td style="text-align:left;"> 3rd Qu.:2.212 </td>
   <td style="text-align:left;"> 3rd Qu.:2.286 </td>
   <td style="text-align:left;"> 3rd Qu.:2.362 </td>
   <td style="text-align:left;"> 3rd Qu.:2.436 </td>
   <td style="text-align:left;"> 3rd Qu.:2.512 </td>
   <td style="text-align:left;"> 3rd Qu.:2.587 </td>
   <td style="text-align:left;"> 3rd Qu.:2.663 </td>
   <td style="text-align:left;"> 3rd Qu.:2.740 </td>
   <td style="text-align:left;"> 3rd Qu.:2.817 </td>
   <td style="text-align:left;"> 3rd Qu.:2.895 </td>
   <td style="text-align:left;"> 3rd Qu.:2.973 </td>
   <td style="text-align:left;"> 3rd Qu.:3.052 </td>
   <td style="text-align:left;"> 3rd Qu.:3.132 </td>
   <td style="text-align:left;"> 3rd Qu.:3.212 </td>
   <td style="text-align:left;"> 3rd Qu.:3.292 </td>
   <td style="text-align:left;"> 3rd Qu.:3.372 </td>
   <td style="text-align:left;"> 3rd Qu.:3.453 </td>
   <td style="text-align:left;"> 3rd Qu.:3.535 </td>
   <td style="text-align:left;"> 3rd Qu.:3.616 </td>
   <td style="text-align:left;"> 3rd Qu.:3.698 </td>
   <td style="text-align:left;"> 3rd Qu.:3.779 </td>
   <td style="text-align:left;"> 3rd Qu.:3.861 </td>
   <td style="text-align:left;"> 3rd Qu.:3.943 </td>
   <td style="text-align:left;"> 3rd Qu.:4.025 </td>
   <td style="text-align:left;"> 3rd Qu.:4.107 </td>
   <td style="text-align:left;"> 3rd Qu.:4.189 </td>
   <td style="text-align:left;"> 3rd Qu.:4.271 </td>
   <td style="text-align:left;"> 3rd Qu.:4.353 </td>
   <td style="text-align:left;"> 3rd Qu.:4.436 </td>
   <td style="text-align:left;"> 3rd Qu.:4.518 </td>
   <td style="text-align:left;"> 3rd Qu.:4.600 </td>
   <td style="text-align:left;"> 3rd Qu.:4.682 </td>
   <td style="text-align:left;"> 3rd Qu.:4.765 </td>
   <td style="text-align:left;"> 3rd Qu.:4.847 </td>
   <td style="text-align:left;"> 3rd Qu.:4.929 </td>
   <td style="text-align:left;"> 3rd Qu.:5.012 </td>
  </tr>
  <tr>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;"> Max.   :1.6368 </td>
   <td style="text-align:left;"> Max.   :1.697 </td>
   <td style="text-align:left;"> Max.   :1.7566 </td>
   <td style="text-align:left;"> Max.   :1.817 </td>
   <td style="text-align:left;"> Max.   :1.8764 </td>
   <td style="text-align:left;"> Max.   :1.936 </td>
   <td style="text-align:left;"> Max.   :1.996 </td>
   <td style="text-align:left;"> Max.   :2.056 </td>
   <td style="text-align:left;"> Max.   :2.116 </td>
   <td style="text-align:left;"> Max.   :2.176 </td>
   <td style="text-align:left;"> Max.   :2.236 </td>
   <td style="text-align:left;"> Max.   :2.296 </td>
   <td style="text-align:left;"> Max.   :2.356 </td>
   <td style="text-align:left;"> Max.   :2.416 </td>
   <td style="text-align:left;"> Max.   :2.475 </td>
   <td style="text-align:left;"> Max.   :2.535 </td>
   <td style="text-align:left;"> Max.   :2.595 </td>
   <td style="text-align:left;"> Max.   :2.655 </td>
   <td style="text-align:left;"> Max.   :2.729 </td>
   <td style="text-align:left;"> Max.   :2.811 </td>
   <td style="text-align:left;"> Max.   :2.893 </td>
   <td style="text-align:left;"> Max.   :2.974 </td>
   <td style="text-align:left;"> Max.   :3.056 </td>
   <td style="text-align:left;"> Max.   :3.138 </td>
   <td style="text-align:left;"> Max.   :3.220 </td>
   <td style="text-align:left;"> Max.   :3.301 </td>
   <td style="text-align:left;"> Max.   :3.383 </td>
   <td style="text-align:left;"> Max.   :3.465 </td>
   <td style="text-align:left;"> Max.   :3.547 </td>
   <td style="text-align:left;"> Max.   :3.628 </td>
   <td style="text-align:left;"> Max.   :3.710 </td>
   <td style="text-align:left;"> Max.   :3.792 </td>
   <td style="text-align:left;"> Max.   :3.874 </td>
   <td style="text-align:left;"> Max.   :3.955 </td>
   <td style="text-align:left;"> Max.   :4.037 </td>
   <td style="text-align:left;"> Max.   :4.123 </td>
   <td style="text-align:left;"> Max.   :4.222 </td>
   <td style="text-align:left;"> Max.   :4.321 </td>
   <td style="text-align:left;"> Max.   :4.420 </td>
   <td style="text-align:left;"> Max.   :4.520 </td>
   <td style="text-align:left;"> Max.   :4.619 </td>
   <td style="text-align:left;"> Max.   :4.718 </td>
   <td style="text-align:left;"> Max.   :4.818 </td>
   <td style="text-align:left;"> Max.   :4.917 </td>
   <td style="text-align:left;"> Max.   :5.016 </td>
   <td style="text-align:left;"> Max.   :5.116 </td>
   <td style="text-align:left;"> Max.   :5.215 </td>
   <td style="text-align:left;"> Max.   :5.314 </td>
   <td style="text-align:left;"> Max.   :5.414 </td>
   <td style="text-align:left;"> Max.   :5.513 </td>
  </tr>
</tbody>
</table>


```r
summary(extract(fit_rstan2, "y_pred")[[1]])%>%kableExtra::kable()
```

<table>
 <thead>
  <tr>
   <th style="text-align:left;">   </th>
   <th style="text-align:left;">       V1 </th>
   <th style="text-align:left;">       V2 </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;"> Min.   :-36.0432 </td>
   <td style="text-align:left;"> Min.   :-7.818 </td>
  </tr>
  <tr>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;"> 1st Qu.:  0.1482 </td>
   <td style="text-align:left;"> 1st Qu.: 6.466 </td>
  </tr>
  <tr>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;"> Median :  0.5704 </td>
   <td style="text-align:left;"> Median : 6.954 </td>
  </tr>
  <tr>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;"> Mean   :  0.5951 </td>
   <td style="text-align:left;"> Mean   : 6.946 </td>
  </tr>
  <tr>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;"> 3rd Qu.:  1.0130 </td>
   <td style="text-align:left;"> 3rd Qu.: 7.431 </td>
  </tr>
  <tr>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;"> Max.   : 19.3150 </td>
   <td style="text-align:left;"> Max.   :23.107 </td>
  </tr>
</tbody>
</table>



These models can be also etimated through *brms* package's API for Stan as follows


```r
M_gaussian <- 
  brm(data = data_o, 
      family = gaussian,
      y ~ 1 + x,
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(cauchy(0, 1),  class = sigma)),
      seed = 1)
```



```
## 
## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 2.3e-05 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.02 seconds (Warm-up)
## Chain 1:                0.019 seconds (Sampling)
## Chain 1:                0.039 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 5e-06 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.019 seconds (Warm-up)
## Chain 2:                0.016 seconds (Sampling)
## Chain 2:                0.035 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 5e-06 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.019 seconds (Warm-up)
## Chain 3:                0.016 seconds (Sampling)
## Chain 3:                0.035 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 4e-06 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.019 seconds (Warm-up)
## Chain 4:                0.018 seconds (Sampling)
## Chain 4:                0.037 seconds (Total)
## Chain 4:
```




```r
summary(M_gaussian)
```

```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: y ~ 1 + x 
##    Data: data_o (Number of observations: 100) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.56      0.34     0.88     2.22 1.00     3889     2871
## x             0.50      0.11     0.28     0.72 1.00     3941     2995
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.13      0.08     0.98     1.30 1.00     3719     2831
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```




```r
M_student <- 
  brm(data = data_o, family = student,
      y ~ 1 + x,
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(gamma(4, 1), class = nu),
                prior(cauchy(0, 1),  class = sigma)),
      seed = 1)
```

```
## Compiling Stan program...
```

```
## Trying to compile a simple C file
```

```
## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c
## clang -mmacosx-version-min=10.13 -I"/Library/Frameworks/R.framework/Resources/include" -DNDEBUG   -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include/"  -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/"  -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/unsupported"  -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/BH/include" -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/src/"  -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/"  -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppParallel/include/"  -I"/Library/Frameworks/R.framework/Versions/4.0/Resources/library/rstan/include" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -DBOOST_NO_AUTO_PTR  -include '/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include   -fPIC  -Wall -g -O2  -c foo.c -o foo.o
## In file included from <built-in>:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:
## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:88:
## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name 'namespace'
## namespace Eigen {
## ^
## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected ';' after top level declarator
## namespace Eigen {
##                ^
##                ;
## In file included from <built-in>:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:
## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1:
## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: 'complex' file not found
## #include <complex>
##          ^~~~~~~~~
## 3 errors generated.
## make: *** [foo.o] Error 1
```

```
## Start sampling
```

```
## 
## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0.000456 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 4.56 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.07 seconds (Warm-up)
## Chain 1:                0.063 seconds (Sampling)
## Chain 1:                0.133 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 1.8e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.069 seconds (Warm-up)
## Chain 2:                0.066 seconds (Sampling)
## Chain 2:                0.135 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 1.3e-05 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.071 seconds (Warm-up)
## Chain 3:                0.063 seconds (Sampling)
## Chain 3:                0.134 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 2.5e-05 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.07 seconds (Warm-up)
## Chain 4:                0.065 seconds (Sampling)
## Chain 4:                0.135 seconds (Total)
## Chain 4:
```




```r
summary(M_student)
```

```
##  Family: student 
##   Links: mu = identity; sigma = identity; nu = identity 
## Formula: y ~ 1 + x 
##    Data: data_o (Number of observations: 100) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.56      0.20     0.17     0.96 1.00     3835     2836
## x             0.80      0.07     0.66     0.93 1.00     3836     3084
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.50      0.06     0.39     0.62 1.00     3183     2799
## nu        2.80      0.70     1.71     4.49 1.00     3564     2917
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```



## Conclusion

In this post, we have provided a simple Bayesian approach to robustly estimate both parameters β and σ of a simple linear regression where the estiamtes are robust to the variance of the error term. The specificity of this approach is to replace the traditional normal assumption on the dependant variable by a heavy-tailed t-distribution assumption. Robusness against outliers comes at a price of a loss of efficiency, especially when the observations are normally distributed. This is a low premium that comes with the robust alternatives that offers a large protection against over-fiting. 



## References

+ [Bayesian Robustness to Outliers in Linear Regression](https://arxiv.org/pdf/1612.05307.pdf)

+ [A New Bayesian Approach to Robustness Against Outliers in Linear Regression](https://dms.umontreal.ca/~bedard/Robustness.pdf)

+ [Robust Noise Models from stan manual](https://mc-stan.org/docs/2_18/stan-users-guide/robust-noise-models.html)

<!--chapter:end:04-Robust.Rmd-->

# Bayesian Regularized Regression

## Introduction

In this post, we will explore Bayesian analogues of regularized linear regression models such as LASSO and Ridge regression, which extend traditional linear regression to handle challenging modeling scenarios. These methods prove particularly valuable for improving prediction accuracy, estimating regression models with many variables, and providing alternatives to traditional model selection approaches.

The foundation of our discussion rests on the standard linear regression model $\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$, where $\mathbf{y} \in \mathbb{R}^n$ represents the response vector, $\mathbf{X} \in \mathbb{R}^{n \times p}$ is the design matrix, $\boldsymbol{\beta} \in \mathbb{R}^p$ contains the coefficients of interest, and $\boldsymbol{\epsilon} \sim \mathcal{N}(0, \sigma^2\mathbf{I})$ captures the random error. Traditional linear regression treats coefficients as independent, producing what statisticians call "unbiased" estimates. However, this independence assumption can lead to poor performance in settings with limited data, high-dimensional predictor spaces, or substantial noise.

Regularized linear regression models deliberately introduce bias into coefficient estimates by pooling information across parameters, causing them to shrink toward a common value, typically zero. This shrinkage represents a fundamental trade-off captured by the bias-variance decomposition: $\text{MSE}(\hat{\boldsymbol{\beta}}) = \text{Bias}^2(\hat{\boldsymbol{\beta}}) + \text{Var}(\hat{\boldsymbol{\beta}})$. While regularization increases bias, it often reduces variance substantially enough that the overall mean squared error decreases, particularly in high-dimensional or noisy settings where traditional methods tend to overfit.

The Bayesian perspective reframes regularization as a problem of specifying appropriate prior distributions for model parameters. Rather than viewing penalty terms as arbitrary constraints on optimization, Bayesian regularization emerges naturally from the choice of prior distribution over coefficients. This connection provides both theoretical insight into why regularization works and practical advantages through full posterior distributions for all parameters.

## Ridge Regression and Gaussian Priors

Ridge regression modifies the ordinary least squares objective by adding a penalty proportional to the sum of squared coefficients. In the frequentist framework, ridge regression minimizes $\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|_2^2 + \lambda \|\boldsymbol{\beta}\|_2^2$, yielding the closed-form solution $\hat{\boldsymbol{\beta}}_{\text{ridge}} = (\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}$. The penalty parameter $\lambda$ controls the strength of regularization, with larger values producing more shrinkage toward zero.

The Bayesian equivalent emerges by placing independent Gaussian priors on each coefficient: $\beta_j \sim \mathcal{N}(0, \tau^2)$ for $j = 1, \ldots, p$. Under this prior specification, the posterior distribution becomes $p(\boldsymbol{\beta}|\mathbf{y}, \mathbf{X}, \sigma^2, \tau^2) \propto \exp\left(-\frac{1}{2\sigma^2}\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|_2^2 - \frac{1}{2\tau^2}\|\boldsymbol{\beta}\|_2^2\right)$. The mathematical equivalence becomes clear when we recognize that $\lambda = \sigma^2/\tau^2$, establishing a direct correspondence between the frequentist penalty parameter and the ratio of error variance to prior variance.

This Bayesian formulation offers several advantages over its frequentist counterpart. Most importantly, it provides full posterior distributions for all parameters rather than just point estimates, enabling principled uncertainty quantification without requiring bootstrap procedures. The prior variance $\tau^2$ also admits a natural hierarchical extension by treating it as an unknown parameter with its own prior distribution, allowing the data to inform the appropriate level of shrinkage.

## LASSO Regression and Laplace Priors

LASSO regression replaces the ridge penalty with an L1 penalty on the coefficient magnitudes, minimizing $\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|_2^2 + \lambda \|\boldsymbol{\beta}\|_1$. This seemingly minor modification produces dramatically different behavior: while ridge regression shrinks coefficients toward zero, LASSO can set them exactly to zero, performing automatic variable selection. The geometric intuition reveals that the L1 penalty creates a diamond-shaped constraint region whose corners lie on the coordinate axes, making exact zeros more likely when the unconstrained optimum lies in particular regions of the parameter space.

The Bayesian LASSO emerges by specifying independent Laplace priors for each coefficient: $\beta_j \sim \text{Laplace}(0, b)$ for $j = 1, \ldots, p$. The Laplace density $p(\beta_j|b) = \frac{1}{2b}\exp\left(-\frac{|\beta_j|}{b}\right)$ places much more probability mass at zero compared to a Gaussian distribution with the same variance, naturally encouraging sparsity. The relationship between the frequentist penalty and Bayesian scale parameter follows $\lambda = \sigma^2/b$.

While the Bayesian LASSO provides the same point estimates as its frequentist counterpart through the posterior mode, its real advantage lies in proper uncertainty quantification for the selected variables. The full posterior distribution accounts for both parameter uncertainty and model uncertainty, providing more honest assessments of prediction intervals and coefficient significance than frequentist methods that condition on the selected model.

## Hierarchical Shrinkage and Adaptive Priors

Both ridge and LASSO impose uniform shrinkage across all coefficients, but real data often contains a mixture of large, moderate, and negligible effects that would benefit from adaptive shrinkage. Hierarchical shrinkage priors address this limitation by allowing different coefficients to experience different amounts of shrinkage based on their apparent signal strength in the data.

The general framework specifies $\beta_j \sim \mathcal{N}(0, \tau^2 \lambda_j^2)$ for $j = 1, \ldots, p$, where $\tau$ represents a global shrinkage parameter affecting all coefficients and $\lambda_j$ represents local shrinkage parameters that can vary across coefficients. This hierarchical structure enables the model to shrink small coefficients aggressively while leaving large coefficients relatively unshrunk.

The horseshoe prior represents one particularly successful implementation of this idea, specifying $\lambda_j \sim \text{Cauchy}^+(0, 1)$ and $\tau \sim \text{Cauchy}^+(0, 1)$. The Cauchy distribution's heavy tails ensure that large coefficients can escape shrinkage while still encouraging small coefficients toward zero. The horseshoe prior gets its name from the shape of its shrinkage function, which resembles an inverted horseshoe when plotted against the signal-to-noise ratio.

## Implementation

```{r}
library(tidymodels)
library(rstan)
library(tidyverse)

# Data simulation
set.seed(123)
n <- 1000
n_train <- 700
n_test <- 300
alpha_true <- 40  # intercept
beta_true <- c(-2, 3, 4, 1, 0.25)  # slopes
sigma_true <- 5  # residual standard deviation
p <- length(beta_true)

# Generate design matrix and response
X <- matrix(rnorm(n * p), n, p)
epsilon <- rnorm(n, mean = 0, sd = sigma_true)
y <- alpha_true + X %*% beta_true + epsilon

# Create dataset
data <- data.frame(y = y, X)
colnames(data) <- c("y", paste0("X", 1:p))

# Train-test split
set.seed(42)
data_split <- initial_split(data, prop = 0.7)
train_data <- training(data_split)
test_data <- testing(data_split)

# Prepare data for Stan
X_train <- as.matrix(train_data[, -1])
X_test <- as.matrix(test_data[, -1])
y_train <- train_data$y
y_test <- test_data$y
```

### Bayesian Ridge Regression

```{r}
# Stan model for Bayesian Ridge Regression
stan_ridge <- "
data {
  int<lower=0> N_train;
  int<lower=0> N_test;
  int<lower=0> p;
  matrix[N_train, p] X_train;
  matrix[N_test, p] X_test;
  vector[N_train] y_train;
}
parameters {
  real alpha;
  vector[p] beta;
  real<lower=0> sigma;
  real<lower=0> tau;  // prior standard deviation for beta
}
model {
  // Priors
  alpha ~ normal(0, 10);
  tau ~ cauchy(0, 1);
  sigma ~ cauchy(0, 5);
  beta ~ normal(0, tau);  // Ridge prior
  
  // Likelihood
  y_train ~ normal(alpha + X_train * beta, sigma);
}
generated quantities {
  vector[N_test] y_pred;
  
  for (i in 1:N_test) {
    y_pred[i] = normal_rng(alpha + X_test[i] * beta, sigma);
  }
}
"

# Prepare data
stan_data_ridge <- list(
  N_train = n_train,
  N_test = n_test,
  p = p,
  X_train = X_train,
  X_test = X_test,
  y_train = y_train
)

# Fit model
fit_ridge <- stan(model_code = stan_ridge, 
                  data = stan_data_ridge,
                  chains = 4, iter = 2000,
                  cores = parallel::detectCores())

print(fit_ridge, pars = c("alpha", "beta", "sigma", "tau"))
```

### Bayesian LASSO Regression

```{r}
# Stan model for Bayesian LASSO
stan_lasso <- "
data {
  int<lower=0> N_train;
  int<lower=0> N_test;
  int<lower=0> p;
  matrix[N_train, p] X_train;
  matrix[N_test, p] X_test;
  vector[N_train] y_train;
}
parameters {
  real alpha;
  vector[p] beta;
  real<lower=0> sigma;
  real<lower=0> b;  // Laplace scale parameter
}
model {
  // Priors
  alpha ~ normal(0, 10);
  b ~ cauchy(0, 1);
  sigma ~ cauchy(0, 5);
  beta ~ double_exponential(0, b);  // LASSO prior
  
  // Likelihood
  y_train ~ normal(alpha + X_train * beta, sigma);
}
generated quantities {
  vector[N_test] y_pred;
  
  for (i in 1:N_test) {
    y_pred[i] = normal_rng(alpha + X_test[i] * beta, sigma);
  }
}
"

# Prepare data (same as Ridge)
stan_data_lasso <- stan_data_ridge

# Fit model
fit_lasso <- stan(model_code = stan_lasso,
                  data = stan_data_lasso,
                  chains = 4, iter = 2000,
                  cores = parallel::detectCores())

print(fit_lasso, pars = c("alpha", "beta", "sigma", "b"))
```

### Hierarchical Shrinkage (Horseshoe Prior)

```{r}
# Stan model for Horseshoe prior
stan_horseshoe <- "
data {
  int<lower=0> N_train;
  int<lower=0> N_test;
  int<lower=0> p;
  matrix[N_train, p] X_train;
  matrix[N_test, p] X_test;
  vector[N_train] y_train;
}
parameters {
  real alpha;
  vector[p] beta_raw;
  real<lower=0> sigma;
  real<lower=0> tau;  // global shrinkage
  vector<lower=0>[p] lambda;  // local shrinkage
}
transformed parameters {
  vector[p] beta;
  beta = beta_raw .* lambda * tau;  // hierarchical shrinkage
}
model {
  // Priors
  alpha ~ normal(0, 10);
  tau ~ cauchy(0, 1);  // global shrinkage
  lambda ~ cauchy(0, 1);  // local shrinkage
  beta_raw ~ normal(0, 1);
  sigma ~ cauchy(0, 5);
  
  // Likelihood
  y_train ~ normal(alpha + X_train * beta, sigma);
}
generated quantities {
  vector[N_test] y_pred;
  
  for (i in 1:N_test) {
    y_pred[i] = normal_rng(alpha + X_test[i] * beta, sigma);
  }
}
"

# Prepare data (same as before)
stan_data_horseshoe <- stan_data_ridge

# Fit model
fit_horseshoe <- stan(model_code = stan_horseshoe,
                      data = stan_data_horseshoe,
                      chains = 4, iter = 2000,
                      cores = parallel::detectCores())

print(fit_horseshoe, pars = c("alpha", "beta", "sigma", "tau"))
```

## Empirical Comparison and Model Evaluation

Understanding the theoretical properties of different regularization approaches provides essential intuition, but empirical evaluation reveals how these methods perform in practice. The code below demonstrates how to compare ridge regression, LASSO, and horseshoe priors using both predictive accuracy and coefficient recovery metrics.

```r
# Extract predictions and compute metrics
extract_metrics <- function(fit, true_values) {
  y_pred <- extract(fit)$y_pred
  y_pred_mean <- colMeans(y_pred)
  
  # RMSE
  rmse <- sqrt(mean((y_pred_mean - true_values)^2))
  
  # Coverage (95% credible intervals)
  y_pred_lower <- apply(y_pred, 2, quantile, 0.025)
  y_pred_upper <- apply(y_pred, 2, quantile, 0.975)
  coverage <- mean(true_values >= y_pred_lower & true_values <= y_pred_upper)
  
  return(list(rmse = rmse, coverage = coverage))
}

# True test values (computed in R, not passed to Stan)
y_test_true <- alpha_true + X_test %*% beta_true

# Compare models
metrics_ridge <- extract_metrics(fit_ridge, y_test_true)
metrics_lasso <- extract_metrics(fit_lasso, y_test_true)
metrics_horseshoe <- extract_metrics(fit_horseshoe, y_test_true)


# Print comparison
cat("Model Comparison (Test Set):\n")
cat("Ridge - RMSE:", round(metrics_ridge$rmse, 3), 
    "Coverage:", round(metrics_ridge$coverage, 3), "\n")
cat("LASSO - RMSE:", round(metrics_lasso$rmse, 3), 
    "Coverage:", round(metrics_lasso$coverage, 3), "\n")
cat("Horseshoe - RMSE:", round(metrics_horseshoe$rmse, 3), 
    "Coverage:", round(metrics_horseshoe$coverage, 3), "\n")

# Plot coefficient estimates
library(bayesplot)
library(ggplot2)

# Extract posterior samples
beta_ridge <- extract(fit_ridge)$beta
beta_lasso <- extract(fit_lasso)$beta
beta_horseshoe <- extract(fit_horseshoe)$beta

# Create comparison plot
beta_df <- data.frame(
  coefficient = rep(paste0("beta", 1:p), 3),
  method = rep(c("Ridge", "LASSO", "Horseshoe"), each = p),
  mean = c(colMeans(beta_ridge), colMeans(beta_lasso), colMeans(beta_horseshoe)),
  lower = c(apply(beta_ridge, 2, quantile, 0.025),
            apply(beta_lasso, 2, quantile, 0.025),
            apply(beta_horseshoe, 2, quantile, 0.025)),
  upper = c(apply(beta_ridge, 2, quantile, 0.975),
            apply(beta_lasso, 2, quantile, 0.975),
            apply(beta_horseshoe, 2, quantile, 0.975)),
  true_value = rep(beta_true, 3)
)

ggplot(beta_df, aes(x = coefficient, y = mean, color = method)) +
  geom_point(position = position_dodge(0.3)) +
  geom_errorbar(aes(ymin = lower, ymax = upper), 
                position = position_dodge(0.3), width = 0.2) +
  geom_point(aes(y = true_value), color = "black", shape = 4, size = 3) +
  labs(title = "Coefficient Estimates Comparison",
       subtitle = "Black crosses show true values",
       x = "Coefficient", y = "Estimate") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The empirical comparison typically reveals distinct patterns in how each method handles different types of signals. Ridge regression tends to shrink all coefficients proportionally, maintaining the relative magnitudes while reducing overall scale. This behavior works well when most predictors contribute meaningfully to the response, even if their individual effects are modest. LASSO regression exhibits more dramatic behavior, often setting smaller coefficients to exactly zero while leaving larger ones relatively unshrunk. This creates sparse solutions that can be easier to interpret but may sacrifice some predictive accuracy when the true model is dense.

Hierarchical shrinkage methods like the horseshoe prior attempt to get the best of both worlds by adapting the amount of shrinkage to each coefficient individually. Large coefficients experience minimal shrinkage, allowing them to maintain their full predictive power, while small coefficients get shrunk aggressively toward zero, reducing noise in the model. This adaptivity often leads to superior predictive performance, especially in settings where the true coefficient vector contains a mixture of large, moderate, and negligible effects.

## Understanding Posterior Behavior

The mathematical structure of each prior determines not just point estimates but the entire posterior distribution. Ridge regression produces a posterior mean with the closed form $E[\boldsymbol{\beta}|\mathbf{y}] = \left(\frac{1}{\sigma^2}\mathbf{X}^T\mathbf{X} + \frac{1}{\tau^2}\mathbf{I}\right)^{-1}\frac{1}{\sigma^2}\mathbf{X}^T\mathbf{y}$, which clearly shows how the prior variance $\tau^2$ balances against the data-driven term $\mathbf{X}^T\mathbf{X}$. When $\tau^2$ is small relative to $\sigma^2$, the prior dominates and coefficients shrink strongly toward zero. When $\tau^2$ is large, the data term dominates and estimates approach ordinary least squares.

LASSO regression does not admit a closed-form posterior mean due to the non-conjugate Laplace prior, but the posterior mode corresponds exactly to the frequentist LASSO solution. The full posterior distribution, obtained through MCMC sampling, provides uncertainty intervals that properly account for both the sparsity-inducing prior and the inherent variability in coefficient estimates. This represents a substantial improvement over frequentist LASSO inference, which requires complex procedures to obtain valid confidence intervals for selected variables.

The horseshoe prior creates particularly interesting posterior behavior through its adaptive shrinkage mechanism. Each coefficient experiences shrinkage according to $E[\beta_j|\text{data}] \approx (1 - \kappa_j) \hat{\beta}_j^{\text{OLS}}$, where $\kappa_j \in (0,1)$ represents a data-dependent shrinkage factor. Coefficients with strong signals in the data have shrinkage factors close to zero, leaving them essentially unshrunk, while coefficients with weak signals have shrinkage factors approaching one, causing aggressive shrinkage toward zero.

## Choosing Among Regularization Methods

The choice between ridge regression, LASSO, and hierarchical shrinkage depends critically on the structure expected in the true coefficient vector and the goals of the analysis. Ridge regression works best when most predictors contribute meaningfully to the response, even if some effects are small. This commonly occurs in settings where the predictors represent different aspects of the same underlying phenomenon or when domain knowledge suggests that excluding variables entirely would be inappropriate.

LASSO regression excels when the true model is sparse, meaning that only a subset of predictors have non-zero effects. The automatic variable selection property makes LASSO particularly attractive for exploratory analyses where identifying the most important predictors is as important as achieving good predictive performance. However, LASSO can struggle when several predictors are highly correlated, as it tends to select one arbitrarily and zero out the others, potentially missing important relationships.

Hierarchical shrinkage methods like the horseshoe prior offer the most flexibility by allowing the data to determine the appropriate level of sparsity. These methods work well across a wide range of scenarios, from dense models where most predictors matter to sparse models where only a few predictors have substantial effects. The primary cost of this flexibility is computational complexity, as hierarchical models typically require more sophisticated MCMC sampling schemes and longer chains to achieve convergence.

## Conclusion

Bayesian regularized regression represents a natural and principled approach to handling the challenges that arise in modern statistical modeling. By reformulating penalty-based methods in terms of prior distributions, the Bayesian framework provides not only point estimates equivalent to their frequentist counterparts but also full posterior distributions that enable proper uncertainty quantification and model comparison. The connection between prior specifications and regularization behavior offers valuable insight into why different methods work well in different contexts and how to choose appropriately among them.

The three approaches examined here span a useful range of assumptions about coefficient behavior. Ridge regression assumes all coefficients are a priori similar and should be shrunk proportionally, LASSO assumes that many coefficients are exactly zero and should be eliminated entirely, and hierarchical shrinkage assumes that coefficients vary in importance and should be shrunk adaptively. Modern computing makes all three approaches feasible for most practical applications, shifting the primary challenge from computational limitations to thoughtful model selection based on domain knowledge and data characteristics.

These Bayesian methods also integrate naturally into larger modeling workflows that require uncertainty quantification, model averaging, or decision-making under uncertainty. The posterior distributions provide the building blocks for more complex analyses while maintaining the computational efficiency and theoretical elegance that make regularized regression so appealing in the first place.

## References

- Carvalho, C. M., Polson, N. G., & Scott, J. G. (2010). The horseshoe estimator for sparse signals. *Biometrika*, 97(2), 465-480.
- Park, T., & Casella, G. (2008). The Bayesian lasso. *Journal of the American Statistical Association*, 103(482), 681-686.
- Piironen, J., & Vehtari, A. (2017). Sparsity information and regularization in the horseshoe and other shrinkage priors. *Electronic Journal of Statistics*, 11(2), 5018-5051.
- Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. *Journal of the Royal Statistical Society*, 58(1), 267-288.

<!--chapter:end:05-Regularization.Rmd-->

# Bayesian Quantile Regression

## Introduction


Quantile regression extends traditional regression analysis beyond the conditional mean to model the entire conditional distribution of the response variable. For a response variable $Y$ and predictor variables $\mathbf{X}$, the $\tau$-th quantile function is defined as $Q_\tau(Y|\mathbf{X}) = \inf\{y : F_{Y|\mathbf{X}}(y) \geq \tau\}$, where $F_{Y|\mathbf{X}}(y)$ represents the conditional cumulative distribution function and $\tau \in (0,1)$ denotes the quantile level. This formulation allows researchers to examine how predictor variables influence different portions of the response distribution, providing insights that may be obscured when focusing solely on conditional means.

The mathematical foundation of quantile regression rests on the asymmetric Laplace distribution (ALD), which serves as the working likelihood. For the $\tau$-th quantile, the ALD has the probability density function:

$$f(y|\mu, \sigma, \tau) = \frac{\tau(1-\tau)}{\sigma} \exp\left(-\rho_\tau\left(\frac{y-\mu}{\sigma}\right)\right)$$

where $\mu$ represents the location parameter (the $\tau$-th quantile), $\sigma > 0$ is the scale parameter, and $\rho_\tau(u) = u(\tau - I(u < 0))$ is the check function with $I(\cdot)$ being the indicator function. This check function is fundamental to quantile regression as it provides the loss function that is minimized during estimation: $\sum_{i=1}^n \rho_\tau(y_i - \mathbf{x}_i^T\boldsymbol{\beta})$.

The Bayesian framework transforms quantile regression into a hierarchical model where uncertainty is explicitly modeled through prior distributions. Consider the linear quantile regression model $Q_\tau(Y_i|\mathbf{x}_i) = \mathbf{x}_i^T\boldsymbol{\beta}_\tau$, where $\boldsymbol{\beta}_\tau$ represents the vector of regression coefficients specific to the $\tau$-th quantile. In the Bayesian setting, we specify prior distributions for these parameters, typically $\boldsymbol{\beta}_\tau \sim \mathcal{N}(\boldsymbol{\mu}_0, \boldsymbol{\Sigma}_0)$ for the regression coefficients and $\sigma \sim \text{Inv-Gamma}(a, b)$ for the scale parameter. The posterior distribution is then proportional to the product of the likelihood and priors: $p(\boldsymbol{\beta}_\tau, \sigma | \mathbf{y}, \mathbf{X}) \propto \prod_{i=1}^n f(y_i|\mathbf{x}_i^T\boldsymbol{\beta}_\tau, \sigma, \tau) \cdot p(\boldsymbol{\beta}_\tau) \cdot p(\sigma)$.

Stan implements this Bayesian quantile regression through efficient Hamiltonian Monte Carlo (HMC) sampling, which provides several computational advantages over traditional optimization-based methods. The HMC algorithm exploits the geometry of the posterior distribution by using gradient information to propose new states in the Markov chain, resulting in more efficient exploration of the parameter space. This is particularly beneficial for quantile regression models, which often exhibit multimodal or skewed posterior distributions due to the asymmetric nature of the loss function. The resulting MCMC samples provide a complete characterization of parameter uncertainty, enabling probabilistic statements about quantile estimates and facilitating hypothesis testing through posterior probability calculations.

The incorporation of prior information represents a significant advantage of the Bayesian approach, particularly in healthcare applications where domain expertise can inform model specification. Informative priors can be constructed based on previous studies, clinical knowledge, or expert opinion. For instance, if prior research suggests that a treatment effect is positive but with considerable uncertainty, this can be encoded through a normal prior centered at a positive value with appropriate variance. Mathematically, this might be expressed as $\beta_{\text{treatment}} \sim \mathcal{N}(0.5, 1.0^2)$, indicating a belief that the treatment effect is likely positive but allowing for substantial uncertainty. Such priors serve as regularization mechanisms, particularly valuable when dealing with limited data or complex models prone to overfitting.

The Bayesian framework naturally provides uncertainty quantification through the posterior distribution, which contrasts sharply with frequentist approaches that rely on asymptotic approximations. For any function of the parameters $g(\boldsymbol{\theta})$, credible intervals can be constructed directly from the posterior samples without relying on distributional assumptions. This is particularly valuable for quantile regression, where the sampling distribution of quantile estimates can be complex and non-normal. The posterior predictive distribution $p(\tilde{y}|\mathbf{x}, \mathbf{y}, \mathbf{X})$ provides a complete description of uncertainty in future observations, enabling probabilistic forecasting and risk assessment.

Model comparison and selection within the Bayesian quantile regression framework can be performed using information-theoretic criteria such as the Widely Applicable Information Criterion (WAIC) or Leave-One-Out Cross-Validation (LOO-CV). These criteria balance model fit against complexity, providing guidance for variable selection and model specification. For nested models, Bayes factors can be computed to quantify the evidence in favor of one model over another: $BF_{12} = \frac{p(\mathbf{y}|M_1)}{p(\mathbf{y}|M_2)}$, where $p(\mathbf{y}|M_i)$ represents the marginal likelihood under model $M_i$. Additionally, posterior predictive checking enables model validation by comparing observed data patterns with those generated from the fitted model.

Healthcare applications particularly benefit from Bayesian quantile regression due to the heterogeneous nature of clinical data and the importance of understanding distributional effects rather than just average treatment effects. Consider a clinical trial where the primary outcome exhibits heteroscedasticity across patient subgroups. Traditional mean regression might indicate no overall treatment effect, while quantile regression could reveal significant benefits for patients in the upper quantiles of the outcome distribution, corresponding perhaps to those with more severe disease. The Bayesian approach allows for the incorporation of prior knowledge about biological mechanisms, previous trial results, and regulatory requirements, while providing uncertainty estimates that are crucial for clinical decision-making.

The computational implementation in Stan leverages automatic differentiation to compute gradients of the log-posterior density, enabling efficient HMC sampling even for complex models with hundreds of parameters. Stan's modeling language allows for flexible specification of hierarchical structures, time-varying coefficients, and non-linear relationships, making it particularly suitable for the complex models often encountered in healthcare research. The software automatically handles the transformation of constrained parameters, monitors convergence diagnostics, and provides comprehensive posterior summaries, making advanced Bayesian quantile regression accessible to applied researchers.

To illustrate the practical implementation of Bayesian quantile regression with Stan, we now examine a synthetic heteroskedastic dataset that demonstrates the method's ability to capture distributional heterogeneity across the predictor space. This example will showcase how the Bayesian framework handles uncertainty quantification while revealing patterns that would be missed by traditional mean-based regression approaches.


## Model Specification

### Using bayesQR package


Simulate data from heteroskedastic regression

```{r}
    set.seed(66)
library(brms)
library(bayesQR)
    n <- 200
    X <- runif(n=n,min=0,max=10)
    X <- X
    y <- 1 + 2*X + rnorm(n=n, mean=0, sd=.6*X)
```

Estimate series of quantile regressions with adaptive lasso
to limit execution time of the example, ndraw is set to a very low value. Set value to 5000 for a better approximation of the posterior distirubtion.

```{r} 
library(brms)
library(bayesQR)
    out <- bayesQR(y~X, quantile=c(.05,.25,.5,.75,.95), alasso=TRUE, ndraw=500)
```
Initiate plot

```{r}
    plot(X, y, main="", cex=.6, xlab="X")
    ## Add quantile regression lines to the plot (exclude first 500 burn-in draws)
    sum <- summary(out, burnin=50)
    for (i in 1:length(sum)){
      abline(a=sum[[i]]$betadraw[1,1],b=sum[[i]]$betadraw[2,1],lty=i,col=i)
    }
```

```{r}
outOLS = lm(y~X)
plot(X, y, pch = 19, col = "blue")

    abline(outOLS,lty=1,lwd=2,col=6)
    # Add legend to plot
    legend(x=0,y=max(y),legend=c(.05,.25,.50,.75,.95,"OLS"),lty=c(1,2,3,4,5,1),
           lwd=c(1,1,1,1,1,2),col=c(1:6),title="Quantile")
```

### Using brms package
```{r}
n <- 200
x <- runif(n = n, min = 0, max = 10)
y <- 1 + 2 * x + rnorm(n = n, mean = 0, sd = 0.6*x)
dat <- data.frame(x, y)
# fit the 20%-quantile
fit <- brm(bf(y ~ x, quantile = 0.2), data = dat, family = asym_laplace())
summary(fit)
```
### Backend stan model

```stan
functions {
  /* helper function for asym_laplace_lpdf
   * Args:
   *   y: the response value
   *   quantile: quantile parameter in (0, 1)
   */
   real rho_quantile(real y, real quantile) {
     if (y < 0) {
       return y * (quantile - 1);
     } else {
       return y * quantile;
     }
   }
  /* asymmetric laplace log-PDF for a single response
   * Args:
   *   y: the response value
   *   mu: location parameter
   *   sigma: positive scale parameter
   *   quantile: quantile parameter in (0, 1)
   * Returns:
   *   a scalar to be added to the log posterior
   */
   real asym_laplace_lpdf(real y, real mu, real sigma, real quantile) {
     return log(quantile * (1 - quantile)) -
            log(sigma) -
            rho_quantile((y - mu) / sigma, quantile);
   }
  /* asymmetric laplace log-CDF for a single quantile
   * Args:
   *   y: a quantile
   *   mu: location parameter
   *   sigma: positive scale parameter
   *   quantile: quantile parameter in (0, 1)
   * Returns:
   *   a scalar to be added to the log posterior
   */
   real asym_laplace_lcdf(real y, real mu, real sigma, real quantile) {
     if (y < mu) {
       return log(quantile) + (1 - quantile) * (y - mu) / sigma;
     } else {
       return log1m((1 - quantile) * exp(-quantile * (y - mu) / sigma));
     }
   }
  /* asymmetric laplace log-CCDF for a single quantile
   * Args:
   *   y: a quantile
   *   mu: location parameter
   *   sigma: positive scale parameter
   *   quantile: quantile parameter in (0, 1)
   * Returns:
   *   a scalar to be added to the log posterior
   */
   real asym_laplace_lccdf(real y, real mu, real sigma, real quantile) {
     if (y < mu) {
       return log1m(quantile * exp((1 - quantile) * (y - mu) / sigma));
     } else {
       return log1m(quantile) - quantile * (y - mu) / sigma;
     }
   }
}
data {
  int<lower=1> N;  // total number of observations
  vector[N] Y;  // response variable
  int<lower=1> K;  // number of population-level effects
  matrix[N, K] X;  // population-level design matrix
  int prior_only;  // should the likelihood be ignored?
}
transformed data {
  int Kc = K - 1;
  matrix[N, Kc] Xc;  // centered version of X without an intercept
  vector[Kc] means_X;  // column means of X before centering
  for (i in 2:K) {
    means_X[i - 1] = mean(X[, i]);
    Xc[, i - 1] = X[, i] - means_X[i - 1];
  }
}
parameters {
  vector[Kc] b;  // population-level effects
  real Intercept;  // temporary intercept for centered predictors
  real<lower=0> sigma;  // dispersion parameter
}
transformed parameters {
  real quantile = 0.2;  // quantile parameter
  real lprior = 0;  // prior contributions to the log posterior
  lprior += student_t_lpdf(Intercept | 3, 11, 7.8);
  lprior += student_t_lpdf(sigma | 3, 0, 7.8)
    - 1 * student_t_lccdf(0 | 3, 0, 7.8);
}
model {
  // likelihood including constants
  if (!prior_only) {
    // initialize linear predictor term
    vector[N] mu = rep_vector(0.0, N);
    mu += Intercept + Xc * b;
    for (n in 1:N) {
      target += asym_laplace_lpdf(Y[n] | mu[n], sigma, quantile);
    }
  }
  // priors including constants
  target += lprior;
}
generated quantities {
  // actual population-level intercept
  real b_Intercept = Intercept - dot_product(means_X, b);
}
```
This Stan code specifies a Bayesian model for asymmetric Laplace regression, where the main goal is to estimate the population-level effects and dispersion parameter of the model from the provided data. The asymmetric Laplace distribution is used as the likelihood function for the response variable. The `functions` block contains three helper functions: `rho_quantile`, `asym_laplace_lpdf`, and `asym_laplace_lccdf`. These functions are used to calculate the asymmetric Laplace log-PDF, log-CDF, and log-CCDF for a single response variable.  The `data` block defines the input data for the model, including the total number of observations `N`, the response variable `Y`, the number of population-level effects `K`, the population-level design matrix `X`, and a binary variable `prior_only` that indicates whether to ignore the likelihood (for prior-only sampling). The `transformed data` block preprocesses the data. It calculates the centered version of the design matrix `Xc`, removes the intercept from the design matrix `X`, and stores the column means of `X` before centering in the vector `means_X`. The `parameters` block defines the parameters to be estimated in the model. It includes the population-level effects `b`, the temporary intercept for centered predictors `Intercept`, and the dispersion parameter `sigma`. The `transformed parameters` block calculates the quantile parameter `quantile` (set to 0.2 in this case) and the prior contributions to the log posterior (`lprior`). The `lprior` term includes the priors for the `Intercept` and `sigma` parameters, which are specified as Student's t-distributions. The `model` block defines the likelihood and priors for the model. The likelihood accounts for the asymmetric Laplace distribution for the response variable `Y`, given the linear predictor `mu` (calculated using the population-level effects `b` and `Intercept`) and the dispersion parameter `sigma`. If `prior_only` is true, the likelihood is ignored, and the model only considers the priors. The `generated quantities` block computes the actual population-level intercept `b_Intercept` by removing the effect of the centered predictors from the temporary intercept `Intercept`.


## Conclusion


Bayesian quantile regression extends classical regression analysis by allowing researchers to model the entire conditional distribution of outcomes rather than restricting inference to the mean. By leveraging the asymmetric Laplace distribution as a working likelihood and incorporating prior information, this approach offers a principled framework for uncertainty quantification, flexible model specification, and probabilistic forecasting. The use of Hamiltonian Monte Carlo, as implemented in Stan, enables efficient exploration of complex posterior distributions, making Bayesian quantile regression both theoretically rigorous and computationally feasible. The examples with simulated heteroskedastic data highlight its capacity to reveal heterogeneous effects across the outcome distribution that would remain hidden under mean-based methods, underscoring its practical relevance for applied domains such as healthcare research, where distributional differences often carry critical substantive implications.

## References

+ Yu, K., & Moyeed, R. A. (2001). Bayesian quantile regression. Statistics & Probability Letters, 54(4), 437-447.
+ Kottas, A., & Gelfand, A. E. (2001). Bayesian semiparametric median regression modeling. Journal of the American Statistical Association, 97(457), 109-121.
+ Koenker, R., & Xiao, Z. (2006). Quantile autoregression. Journal of the American Statistical Association, 101(475), 980-990.
+ Yu, K., & Moyeed, R. A. (2000). Bayesian quantile regression. Journal of the Royal Statistical Society: Series D (The Statistician), 49(3), 385-392.
+ Koenker, R., & Xiao, Z. (2004). Inference on the quantile regression process. Econometrica, 72(1), 71-104.
+ https://cran.r-project.org/web/packages/bayesQR/bayesQR.pdf

<!--chapter:end:06-Quentile.Rmd-->

# Bayesian Multilevel Regression in Stan 

## Introduction

When analyzing data with inherent hierarchical structures—such as students nested within schools, patients within hospitals, or repeated measurements within individuals—traditional linear regression falls short by assuming independence among all observations. This assumption becomes problematic when observations within groups are more similar to each other than to observations from other groups, leading to underestimated standard errors and inflated Type I error rates.

Bayesian multilevel (mixed effects) regression models address this challenge by explicitly modeling the hierarchical structure of data through the incorporation of both fixed effects (population-level parameters) and random effects (group-specific parameters). Unlike their frequentist counterparts, Bayesian multilevel models leverage prior distributions to achieve partial pooling of information across groups, resulting in more stable parameter estimates and better predictive performance, particularly when dealing with small sample sizes within groups.

The fundamental insight of multilevel modeling lies in partial pooling—a middle ground between complete pooling (ignoring group structure) and no pooling (estimating parameters independently for each group). Through hierarchical priors, Bayesian models naturally shrink group-specific estimates toward the population mean in proportion to the uncertainty in group-level estimates, effectively borrowing strength across groups while preserving important group-level variation.

This comprehensive guide explores three fundamental approaches to Bayesian multilevel modeling: random intercepts models that allow baseline differences across groups, random slopes models that permit varying treatment effects, and hierarchical priors models that capture correlations between group-level parameters. We implement each approach in Stan, providing mathematical foundations, practical R code, and visualization techniques to illuminate the behavior of these models. Consider a dataset with $i = 1, \ldots, n_j$ observations nested within $j = 1, \ldots, J$ groups, where $y_{ij}$ represents the outcome for observation $i$ in group $j$, and $x_{ij}$ represents the corresponding predictor.

The general form of a multilevel model can be written as:

$$y_{ij} = \alpha_j + \beta_j x_{ij} + \epsilon_{ij}$$

where $\epsilon_{ij} \sim N(0, \sigma^2)$ represents the individual-level residual. The group-specific parameters $\alpha_j$ and $\beta_j$ are themselves modeled as random variables:

$$\alpha_j = \alpha + u_{j}^{(\alpha)}$$
$$\beta_j = \beta + u_{j}^{(\beta)}$$

where $\alpha$ and $\beta$ are population-level (fixed) effects, and $u_{j}^{(\alpha)}$ and $u_{j}^{(\beta)}$ are group-level random effects typically assumed to follow a multivariate normal distribution:

$$\begin{pmatrix} u_{j}^{(\alpha)} \\ u_{j}^{(\beta)} \end{pmatrix} \sim \mathcal{N}\left(\begin{pmatrix} 0 \\ 0 \end{pmatrix}, \mathbf{\Sigma}\right)$$

The covariance matrix $\mathbf{\Sigma}$ captures both the variability of group-level effects and their potential correlation. This hierarchical structure enables the model to pool information across groups while accounting for group-specific variation.

## Implementation in R
#### Data Generation and Preparation

To demonstrate these concepts, we'll work with simulated data that exhibits realistic hierarchical structure. Our simulation includes correlated group effects to showcase the full capability of multilevel models.

```{r}
library(tidyverse)
library(tidymodels)
library(rstan)
library(bayesplot)
library(ggplot2)
library(corrplot)
library(patchwork)

# Set random seed for reproducibility
set.seed(123)

# Data generation parameters
n_total <- 1000
n_groups <- 20
n_per_group <- n_total / n_groups

# Population-level parameters
alpha_pop <- 40    # population intercept
beta_pop <- 3      # population slope
sigma_y <- 4       # residual standard deviation

# Group-level variation parameters
sigma_alpha <- 8   # SD of group intercepts
sigma_beta <- 2    # SD of group slopes  
rho <- 0.3         # correlation between random intercepts and slopes

# Create covariance matrix for group effects
Sigma_group <- matrix(c(sigma_alpha^2, rho * sigma_alpha * sigma_beta,
                       rho * sigma_alpha * sigma_beta, sigma_beta^2), 
                     nrow = 2, ncol = 2)

# Generate group effects
group_effects <- MASS::mvrnorm(n_groups, mu = c(0, 0), Sigma = Sigma_group)
group_intercepts <- group_effects[, 1]
group_slopes <- group_effects[, 2]

# Generate individual-level data
group_id <- rep(1:n_groups, each = n_per_group)
x <- rnorm(n_total, mean = 0, sd = 1.5)
y <- (alpha_pop + group_intercepts[group_id]) + 
     (beta_pop + group_slopes[group_id]) * x + 
     rnorm(n_total, mean = 0, sd = sigma_y)

# Create dataset
data_full <- tibble(
  y = y,
  x = x,
  group_id = factor(group_id),
  group_intercept_true = group_intercepts[group_id],
  group_slope_true = group_slopes[group_id]
)

# Train-test split
set.seed(42)
data_split <- initial_split(data_full, prop = 0.75, strata = group_id)
train_data <- training(data_split)
test_data <- testing(data_split)

# Display data structure
cat("Dataset structure:\n")
cat("Total observations:", nrow(data_full), "\n")
cat("Number of groups:", n_groups, "\n")
cat("Average observations per group:", n_per_group, "\n")
cat("Training set size:", nrow(train_data), "\n")
cat("Test set size:", nrow(test_data), "\n")

# Visualize the hierarchical structure
p1 <- ggplot(train_data, aes(x = x, y = y, color = group_id)) +
  geom_point(alpha = 0.6, size = 1.2) +
  geom_smooth(method = "lm", se = FALSE, size = 0.8) +
  labs(title = "Data Structure: Group-Specific Regression Lines",
       subtitle = "Each color represents a different group",
       x = "Predictor (x)", y = "Outcome (y)") +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_color_viridis_d()

print(p1)
```

### Random Intercepts Model: Modeling Baseline Differences

The random intercepts model represents the simplest multilevel structure, allowing each group to have its own baseline level while constraining all groups to have the same slope. This model is particularly useful when we believe the effect of our predictor is consistent across groups, but groups differ in their baseline outcomes.

Mathematically, the random intercepts model is specified as:

$$y_{ij} = (\alpha + u_j) + \beta x_{ij} + \epsilon_{ij}$$

where $u_j \sim N(0, \sigma_\alpha^2)$ represents the group-specific deviation from the population intercept $\alpha$. In the Bayesian framework, we place priors on all parameters, including hyperpriors on the variance components.

```{r}
# Stan model for random intercepts
stan_code_intercepts <- "
data {
  int<lower=0> N;                    // number of observations
  int<lower=0> J;                    // number of groups
  vector[N] y;                       // outcome variable
  vector[N] x;                       // predictor variable
  array[N] int<lower=1, upper=J> group_id;  // group indicators
}

parameters {
  real alpha;                        // population intercept
  real beta;                         // population slope
  real<lower=0> sigma_y;             // residual standard deviation
  real<lower=0> sigma_alpha;         // standard deviation of group intercepts
  vector[J] alpha_raw;               // non-centered group intercepts
}

transformed parameters {
  vector[J] alpha_group = alpha + sigma_alpha * alpha_raw;  // centered parameterization
}

model {
  // Priors
  alpha ~ normal(40, 10);
  beta ~ normal(3, 5);
  sigma_y ~ exponential(0.2);
  sigma_alpha ~ exponential(0.1);
  alpha_raw ~ std_normal();          // non-centered parameterization
  
  // Likelihood
  for (n in 1:N) {
    y[n] ~ normal(alpha_group[group_id[n]] + beta * x[n], sigma_y);
  }
}

generated quantities {
  vector[N] y_pred;                  // posterior predictive samples
  vector[N] log_lik;                 // log-likelihood for model comparison
  
  for (n in 1:N) {
    y_pred[n] = normal_rng(alpha_group[group_id[n]] + beta * x[n], sigma_y);
    log_lik[n] = normal_lpdf(y[n] | alpha_group[group_id[n]] + beta * x[n], sigma_y);
  }
}
"

# Prepare data for Stan
stan_data_intercepts <- list(
  N = nrow(train_data),
  J = length(unique(train_data$group_id)),
  y = train_data$y,
  x = train_data$x,
  group_id = as.numeric(train_data$group_id)
)

# Compile and fit the model
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

fit_intercepts <- stan(
  model_code = stan_code_intercepts,
  data = stan_data_intercepts,
  chains = 4,
  iter = 2000,
  warmup = 1000,
  seed = 42
)

# Model diagnostics
print(fit_intercepts, pars = c("alpha", "beta", "sigma_y", "sigma_alpha"))

# Extract posterior samples
posterior_intercepts <- rstan::extract(fit_intercepts)

# Visualization of results
alpha_group_post <- summary(fit_intercepts, pars = "alpha_group")$summary
alpha_group_est <- alpha_group_post[, "mean"]
alpha_group_lower <- alpha_group_post[, "2.5%"]
alpha_group_upper <- alpha_group_post[, "97.5%"]

```

### Random Slopes Model: Capturing Varying Effects

The random slopes model extends the random intercepts approach by allowing both intercepts and slopes to vary across groups. This flexibility is crucial when the effect of a predictor genuinely differs across groups, such as when educational interventions have varying effectiveness across different schools.

The mathematical specification becomes:

$$y_{ij} = (\alpha + u_j^{(\alpha)}) + (\beta + u_j^{(\beta)}) x_{ij} + \epsilon_{ij}$$

where both $u_j^{(\alpha)}$ and $u_j^{(\beta)}$ are group-specific random effects that may be correlated.

```{r}
# Stan model for random slopes
stan_code_slopes <- "
data {
  int<lower=0> N;
  int<lower=0> J;
  vector[N] y;
  vector[N] x;
  array[N] int<lower=1, upper=J> group_id;
}

parameters {
  real alpha;                        // population intercept
  real beta;                         // population slope
  real<lower=0> sigma_y;             // residual standard deviation
  vector<lower=0>[2] sigma_group;    // standard deviations of group effects
  cholesky_factor_corr[2] L_Rho;     // Cholesky factor of correlation matrix
  matrix[2, J] z_group;              // non-centered group effects
}

transformed parameters {
  matrix[J, 2] group_effects;
  group_effects = (diag_pre_multiply(sigma_group, L_Rho) * z_group)';
}

model {
  // Priors
  alpha ~ normal(40, 10);
  beta ~ normal(3, 5);
  sigma_y ~ exponential(0.2);
  sigma_group ~ exponential(0.1);
  L_Rho ~ lkj_corr_cholesky(2);
  to_vector(z_group) ~ std_normal();
  
  // Likelihood
  for (n in 1:N) {
    y[n] ~ normal(alpha + group_effects[group_id[n], 1] + 
                  (beta + group_effects[group_id[n], 2]) * x[n], sigma_y);
  }
}

generated quantities {
  matrix[2, 2] Rho = L_Rho * L_Rho';  // correlation matrix
  vector[N] y_pred;
  vector[N] log_lik;
  
  for (n in 1:N) {
    y_pred[n] = normal_rng(alpha + group_effects[group_id[n], 1] + 
                           (beta + group_effects[group_id[n], 2]) * x[n], sigma_y);
    log_lik[n] = normal_lpdf(y[n] | alpha + group_effects[group_id[n], 1] + 
                             (beta + group_effects[group_id[n], 2]) * x[n], sigma_y);
  }
}
"

# Fit random slopes model
fit_slopes <- stan(
  model_code = stan_code_slopes,
  data = stan_data_intercepts,
  chains = 4,
  iter = 2000,
  warmup = 1000,
  seed = 42
)

print(fit_slopes, pars = c("alpha", "beta", "sigma_y", "sigma_group", "Rho"))

# Extract and visualize group effects
group_effects_post <- summary(fit_slopes, pars = "group_effects")$summary
group_effects_matrix <- matrix(group_effects_post[, "mean"], nrow = n_groups, ncol = 2)

slopes_comparison <- tibble(
  group = 1:n_groups,
  true_intercept = group_intercepts,
  true_slope = group_slopes,
  est_intercept = group_effects_matrix[, 1],
  est_slope = group_effects_matrix[, 2]
)

p3 <- slopes_comparison %>%
  ggplot(aes(x = true_slope, y = est_slope)) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  geom_point(size = 3, alpha = 0.7, color = "steelblue") +
  labs(title = "Random Slopes: Estimated vs. True Group Slopes",
       x = "True Group Slopes", 
       y = "Estimated Group Slopes") +
  theme_minimal() +
  coord_fixed()

p4 <- slopes_comparison %>%
  ggplot(aes(x = est_intercept, y = est_slope)) +
  geom_point(size = 3, alpha = 0.7, color = "darkgreen") +
  geom_smooth(method = "lm", se = TRUE, color = "red") +
  labs(title = "Correlation Between Group Intercepts and Slopes",
       subtitle = paste("Estimated correlation:", 
                       round(summary(fit_slopes, pars = "Rho")$summary[2, "mean"], 3)),
       x = "Group Intercepts", 
       y = "Group Slopes") +
  theme_minimal()

print(p3 + p4)
```

### Model Comparison and Visualization

To understand the practical differences between these approaches, we'll compare their predictive performance and examine how they handle the bias-variance tradeoff inherent in multilevel modeling.

```{r}
# Extract log-likelihood for model comparison
log_lik_intercepts <- extract_log_lik(fit_intercepts)
log_lik_slopes <- extract_log_lik(fit_slopes)

# Calculate WAIC for model comparison
waic_intercepts <- waic(log_lik_intercepts)
waic_slopes <- waic(log_lik_slopes)

# Display model comparison
cat("Model Comparison (WAIC):\n")
cat("Random Intercepts WAIC:", round(waic_intercepts$estimates["waic", "Estimate"], 2), "\n")
cat("Random Slopes WAIC:", round(waic_slopes$estimates["waic", "Estimate"], 2), "\n")
cat("Difference:", round(waic_intercepts$estimates["waic", "Estimate"] - 
                        waic_slopes$estimates["waic", "Estimate"], 2), "\n")

# Posterior predictive checks
y_pred_intercepts <- posterior_intercepts$y_pred
y_pred_slopes <- rstan::extract(fit_slopes)$y_pred

ppc_intercepts <- ppc_dens_overlay(train_data$y, y_pred_intercepts[1:100, ]) +
  ggtitle("Posterior Predictive Check: Random Intercepts")

ppc_slopes <- ppc_dens_overlay(train_data$y, y_pred_slopes[1:100, ]) +
  ggtitle("Posterior Predictive Check: Random Slopes")

print(ppc_intercepts + ppc_slopes)

# Residual analysis
residuals_intercepts <- train_data$y - colMeans(y_pred_intercepts)
residuals_slopes <- train_data$y - colMeans(y_pred_slopes)

residual_comparison <- tibble(
  observed = rep(train_data$y, 2),
  residuals = c(residuals_intercepts, residuals_slopes),
  model = rep(c("Random Intercepts", "Random Slopes"), each = length(residuals_intercepts)),
  group_id = rep(train_data$group_id, 2)
)

p5 <- ggplot(residual_comparison, aes(x = observed, y = residuals)) +
  geom_point(alpha = 0.6) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  geom_smooth(method = "loess", se = FALSE) +
  facet_wrap(~model) +
  labs(title = "Residual Analysis: Model Comparison",
       x = "Observed Values", y = "Residuals") +
  theme_minimal()

print(p5)
```

## Visualization and Interpretation

Understanding multilevel models requires visualizing both the population-level and group-level effects. The following visualizations illuminate the key concepts of shrinkage and partial pooling.

```{r}
# Create comprehensive visualization of group effects
group_summary <- train_data %>%
  group_by(group_id) %>%
  summarise(
    n = n(),
    mean_y = mean(y),
    mean_x = mean(x),
    .groups = "drop"
  ) %>%
  mutate(
    group_num = as.numeric(group_id),
    est_intercept_ri = alpha_group_est,
    est_intercept_rs = group_effects_matrix[, 1],
    est_slope_rs = group_effects_matrix[, 2],
    true_intercept = group_intercepts,
    true_slope = group_slopes
  )

# Shrinkage visualization
p6 <- group_summary %>%
  ggplot(aes(x = n)) +
  geom_point(aes(y = abs(est_intercept_ri - true_intercept)), 
             color = "blue", alpha = 0.7, size = 2) +
  geom_smooth(aes(y = abs(est_intercept_ri - true_intercept)), 
              method = "loess", color = "blue", se = FALSE) +
  labs(title = "Shrinkage Effect: Estimation Error vs. Group Sample Size",
       subtitle = "Smaller groups show more shrinkage toward population mean",
       x = "Group Sample Size", 
       y = "Absolute Estimation Error") +
  theme_minimal()

# Correlation structure visualization
posterior_rho <- rstan::extract(fit_slopes)$Rho[, 1, 2]
p7 <- tibble(rho = posterior_rho) %>%
  ggplot(aes(x = rho)) +
  geom_histogram(bins = 30, fill = "steelblue", alpha = 0.7, color = "white") +
  geom_vline(xintercept = rho, color = "red", linetype = "dashed", size = 1) +
  geom_vline(xintercept = mean(posterior_rho), color = "blue", size = 1) +
  labs(title = "Posterior Distribution of Intercept-Slope Correlation",
       subtitle = paste("True correlation (red):", rho, 
                       "| Estimated mean (blue):", round(mean(posterior_rho), 3)),
       x = "Correlation", y = "Density") +
  theme_minimal()

print(p6 + p7)

# Group-specific predictions visualization
pred_data <- expand_grid(
  x = seq(min(train_data$x), max(train_data$x), length.out = 50),
  group_id = factor(1:min(6, n_groups))  # Show first 6 groups
) %>%
  mutate(group_num = as.numeric(group_id))

# Add predictions from random slopes model
group_effects_sample <- group_effects_matrix[pred_data$group_num, ]
pred_data$y_pred <- (alpha_pop + group_effects_sample[, 1]) + 
                    (beta_pop + group_effects_sample[, 2]) * pred_data$x

p8 <- ggplot() +
  geom_point(data = filter(train_data, as.numeric(group_id) <= 6),
             aes(x = x, y = y, color = group_id), alpha = 0.6) +
  geom_line(data = pred_data, aes(x = x, y = y_pred, color = group_id), 
            size = 1.2) +
  facet_wrap(~group_id, ncol = 3, labeller = label_both) +
  labs(title = "Group-Specific Predictions: Random Slopes Model",
       subtitle = "Points show observed data, lines show group-specific regression",
       x = "Predictor (x)", y = "Outcome (y)") +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_color_viridis_d()

print(p8)
```

## Conclusion

Bayesian multilevel regression models provide a principled approach to analyzing hierarchical data structures, offering several key advantages over traditional regression methods. Through the implementation of random intercepts, random slopes, and hierarchical priors in Stan, we have demonstrated how these models achieve partial pooling of information across groups, resulting in more robust parameter estimates and improved predictive performance.

The random intercepts model proves most useful when groups differ primarily in baseline levels but exhibit similar relationships between predictors and outcomes. This approach provides substantial computational efficiency while capturing the essential hierarchical structure. The random slopes model extends this flexibility by allowing treatment effects to vary across groups, making it ideal for situations where intervention effectiveness or predictor relationships genuinely differ between contexts.

The mathematical foundation of these models reveals their elegant handling of the bias-variance tradeoff through hierarchical shrinkage. Groups with smaller sample sizes experience greater shrinkage toward population estimates, while groups with more data retain their distinctive characteristics. This adaptive borrowing of strength across groups represents a fundamental advantage of the Bayesian approach.

From a practical standpoint, several recommendations emerge from our analysis. First, always examine the correlation structure between random effects, as this often reveals important insights about the underlying data-generating process. Second, use posterior predictive checks and residual analysis to validate model assumptions and identify potential improvements. Third, consider the computational trade-offs between model complexity and interpretability, particularly when working with large numbers of groups or complex hierarchical structures.

The Stan implementations presented here incorporate modern best practices, including non-centered parameterization for improved sampling efficiency and appropriate prior specifications that regularize estimates without overwhelming the data. These technical considerations prove crucial for achieving reliable convergence and meaningful posterior inference in practice.

Future extensions of these models might include temporal dynamics, non-linear relationships, or more complex hierarchical structures with multiple levels of nesting. The Bayesian framework naturally accommodates such extensions while maintaining coherent uncertainty quantification throughout the modeling process, making it an invaluable tool for modern data analysis in the presence of hierarchical structures.

## References

- Gelman, A., & Hill, J. (2006). *Data Analysis Using Regression and Multilevel/Hierarchical Models*. Cambridge University Press.
- Stan Development Team. (2023). *Stan User's Guide: Hierarchical Models*. https://mc-stan.org/docs/stan-users-guide/hierarchical.html
- McElreath, R. (2020). *Statistical Rethinking: A Bayesian Course with Examples in R and Stan*. CRC Press.
- Sorensen, T., & Vasishth, S. (2015). Bayesian linear mixed models using Stan. *Journal of Statistical Software*, 80(1), 1-28.

<!--chapter:end:07-MLM.Rmd-->

# Gaussian Process Regression (GPR)


## Introduction

Gaussian process regression (GPR) is a machine learning method based on non-parametric regression method that can be used to fit arbitrary scalar and vectorial quantities. GPR provides a probabilistic model that can be used to make predictions and estimate the uncertainty of those predictions. A Gaussian process is a generalization of the Gaussian probability distribution to functions, where any finite set of function values has a joint Gaussian distribution. The mean function and covariance function of the Gaussian process describe the prior distribution of the function, and the observations are used to update the prior to the posterior distribution of the function. In GPR, the output variable is assumed to be a function of the input variables, and the function is modeled as a sample from a Gaussian process. The goal is to predict the value of the output variable at a new input point, given the observed data. The predicted value is given by the posterior mean of the Gaussian process, and the uncertainty of the prediction is given by the posterior variance. GPR is particularly useful when the data is noisy or when the function being modeled is complex and nonlinear. The key advantages of GPR over other regression techniques are its flexibility and its ability to provide a probabilistic framework for uncertainty quantification. GPR can be used for both regression and classification problems, and it can handle both scalar and vector-valued outputs. Moreover, GPR can be easily extended to handle non-stationary and non-Gaussian data. In practice, GPR is often implemented using the kernlab or gpflow packages in R or Python, respectively. These packages provide functions for specifying the kernel function, which is used to model the covariance between the input variables, and for estimating the hyperparameters of the kernel function using maximum likelihood or Bayesian methods.

## Challenges

Overfitting occurs when a model is too complex and fits the training data too closely, resulting in poor generalization to new data. Like some other machine learning techniques, GPR is prone to overfitting if the model is too complex relative to the amount of data available. Specifically, if the number of hyperparameters of the Gaussian process model is large, or if the covariance function is too flexible, the model may fit the noise in the data rather than the underlying signal. This can result in poor generalization performance, where the model performs well on the training data but poorly on new, unseen data. To mitigate the risk of overfitting in GPR, it is important to carefully select the kernel function and the hyperparameters of the model based on the available data. Cross-validation can be used to estimate the generalization error of the model and to select the optimal values of the hyperparameters. Regularization techniques, such as adding a prior distribution on the hyperparameters or using Bayesian model selection, can also be used to prevent overfitting. Another way to prevent overfitting in GPR is to use a simpler covariance function that captures the key features of the data, rather than trying to fit the noise in the data. Overall, while GPR is a powerful and flexible regression technique, it requires careful tuning of the hyperparameters and selection of the kernel function to prevent overfitting and achieve good generalization performance.

GPR use in domains such as healthcare comes with certain challenges and limitations that should be considered. Computational complexity poses a significant challenge, particularly with large datasets, necessitating efficient algorithms and computational resources to handle the complexity. Hyperparameter tuning is another consideration, involving the selection of optimal values for parameters such as the kernel function and noise level. This task can be challenging and may require expert knowledge or extensive experimentation. Furthermore, as GPR models complex relationships, the interpretability of the learned models can become intricate. Understanding the underlying factors contributing to predictions becomes more challenging in highly nonlinear models. These challenges highlight the need for careful consideration and expertise when applying GPR in healthcare settings. GPRs ability to model complex relationships, estimate uncertainties, and provide interpretable predictions makes it an invaluable asset for predictive modeling in healthcare, with a postential to enhance disease progression modeling, personalize treatment plans, detect diseases early, and improve medical imaging analysis. While challenges exist, ongoing research and advancements in computational techniques are addressing these limitations, making GPR an increasingly valuable tool in healthcare. As the field continues to evolve, GPR is poised to revolutionize healthcare by enabling more accurate predictions, better decision-making, and improved patient outcomes.


## GPfit package


```
# Load necessary packages
library(kernlab)
library(GPfit)
library(ggplot2)

# Generate simulated data
set.seed(123)
x <- seq(0, 10, length = 50)
y <- sin(x) + rnorm(50, 0, 0.2)
df <- data.frame(x = x, y = y)

# Fit Gaussian process regression model
gpr_model <- gausspr(y ~ x, data = df)
y_pred <- predict(gpr_model, x)

# Visualize results
ggplot(df, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = y_pred), color = "red") +
  labs(title = "Gaussian Process Regression", x = "x", y = "y")
```

This R code performs Gaussian process regression (GPR) on simulated data and visualizes the results. Let's break down each part of the code step-by-step:

1. Load Necessary Packages:

```R
library(kernlab)
library(GPfit)
library(ggplot2)
```

This part loads the required R packages: `kernlab` for kernel-based machine learning, `GPfit` for Gaussian process modeling, and `ggplot2` for data visualization.

2. Generate Simulated Data:

```R
set.seed(123)
x <- seq(0, 10, length = 50)
y <- sin(x) + rnorm(50, 0, 0.2)
df <- data.frame(x = x, y = y)
```

Simulated data is generated for the predictor variable `x` and the response variable `y`. The `x` values are generated as a sequence from 0 to 10 with 50 points. The `y` values are generated by taking the sine of each `x` value and adding random noise from a normal distribution with mean 0 and standard deviation 0.2. The data is then combined into a data frame `df`.

3. Fit Gaussian Process Regression Model:

```R
gpr_model <- gausspr(y ~ x, data = df)
```

A Gaussian process regression model is fitted using the `gausspr` function from the `GPfit` package. The model specification is `y ~ x`, indicating that we want to model `y` as a function of `x` using Gaussian process regression.

4. Predict Values of y and Visualize Results:

```R
y_pred <- predict(gpr_model, x)

ggplot(df, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = y_pred), color = "red") +
  labs(title = "Gaussian Process Regression", x = "x", y = "y")
```

This part predicts the values of the response variable `y_pred` for the predictor variable `x` using the fitted Gaussian process regression model. The `predict` function is used to make the predictions based on the model `gpr_model`.

The results are then visualized using `ggplot2`. A scatter plot of the original data points (`x` and `y`) is created with blue points (`geom_point()`). Overlaid on the scatter plot is a red line representing the predictions of the response variable (`y_pred`) from the Gaussian process regression model (`geom_line(aes(y = y_pred), color = "red")`).



## Bayesian GPR

Gaussian process regression can also be implemented in a Bayesian context using Stan. In Bayesian GPR, we assume a prior distribution for the unknown function and then update our beliefs about the function based on the observed data. The prior distribution is typically specified as a Gaussian process with a mean function and covariance function that depend on hyperparameters. The likelihood function for the observed data is also assumed to be Gaussian with a mean function equal to the prior mean function and a covariance function equal to the sum of the prior covariance function and a noise term. The hyperparameters of the prior and likelihood functions are estimated from the data using Markov chain Monte Carlo (MCMC) methods.

Here is an example of R code for fitting a Bayesian GPR model using Stan. Let's break down each part of the code step-by-step:




```R
library(rstan)
library(ggplot2)

# Generate simulated data
set.seed(123)
x <- seq(0, 10, length = 50)
y <- sin(x) + rnorm(50, 0, 0.2)
df <- data.frame(x = x, y = y)

# Stan model code
stan_model_code <- "
functions {
  vector gp_pred_rng(array[] real x2,
                     vector y1,
                     array[] real x1,
                     real sigma_f,
                     real lengthscale_f,
                     real sigma,
                     real jitter) {
    int N1 = rows(y1);
    int N2 = size(x2);
    vector[N2] f2;
    {
      matrix[N1, N1] L_K;
      vector[N1] K_div_y1;
      matrix[N1, N2] k_x1_x2;
      matrix[N1, N2] v_pred;
      vector[N2] f2_mu;
      matrix[N2, N2] cov_f2;
      matrix[N1, N1] K;
      K = gp_exp_quad_cov(x1, sigma_f, lengthscale_f);
      for (n in 1:N1)
        K[n, n] = K[n,n] + square(sigma);
      L_K = cholesky_decompose(K);
      K_div_y1 = mdivide_left_tri_low(L_K, y1);
      K_div_y1 = mdivide_right_tri_low(K_div_y1', L_K)';
      k_x1_x2 = gp_exp_quad_cov(x1, x2, sigma_f, lengthscale_f);
      f2_mu = (k_x1_x2' * K_div_y1);
      v_pred = mdivide_left_tri_low(L_K, k_x1_x2);
      cov_f2 = gp_exp_quad_cov(x2, sigma_f, lengthscale_f) - v_pred' * v_pred;

      f2 = multi_normal_rng(f2_mu, add_diag(cov_f2, rep_vector(jitter, N2)));
    }
    return f2;
  }
}
data {
  int<lower=1> N;      // number of observations
  vector[N] x;         // univariate covariate
  vector[N] y;         // target variable
  int<lower=1> N2;     // number of test points
  vector[N2] x2;       // univariate test points
}
transformed data {
  // Normalize data
  real xmean = mean(x);
  real ymean = mean(y);
  real xsd = sd(x);
  real ysd = sd(y);
  array[N] real xn = to_array_1d((x - xmean)/xsd);
  array[N2] real x2n = to_array_1d((x2 - xmean)/xsd);
  vector[N] yn = (y - ymean)/ysd;
  real sigma_intercept = 1;
  vector[N] zeros = rep_vector(0, N);
}
parameters {
  real<lower=0> lengthscale_f; // lengthscale of f
  real<lower=0> sigma_f;       // scale of f
  real<lower=0> sigman;         // noise sigma
}
model {
  // covariances and Cholesky decompositions
  matrix[N, N] K_f = gp_exp_quad_cov(xn, sigma_f, lengthscale_f)+
                     sigma_intercept^2;
  matrix[N, N] L_f = cholesky_decompose(add_diag(K_f, sigman^2));
  // priors
  lengthscale_f ~ normal(0, 1);
  sigma_f ~ normal(0, 1);
  sigman ~ normal(0, 1);
  // model
  yn ~ multi_normal_cholesky(zeros, L_f);
}
generated quantities {
  // function scaled back to the original scale
  vector[N2] f = gp_pred_rng(x2n, yn, xn, sigma_f, lengthscale_f, sigman, 1e-9)*ysd + ymean;
  real sigma = sigman*ysd;
}
"

# Compile Stan model
gpr_stan_model <- stan_model(model_code = stan_model_code)

# Prepare data for Stan model
stan_data <- list(x=df$x,
                  x2=df$x,
                  y=df$y,
                  N=length(df$x),
                  N2=length(df$x))

# Fit Bayesian GPR model using Stan
gpr_fit <- sampling(gpr_stan_model, data = stan_data)


f_samples <- extract(gpr_fit, "f")$f
sigma_samples <- extract(gpr_fit, "sigma")$sigma


df %>%
  mutate(Ef=colMeans(f_samples),
         sigma=mean(sigma_samples)) %>%  
  ggplot(aes(x=x,y=y))+
  geom_point()+
  labs(x="Time (ms)", y="Acceleration (g)")+
  geom_line(aes(y=Ef), color='red')+
  geom_line(aes(y=Ef-2*sigma), color='red',linetype="dashed")+
  geom_line(aes(y=Ef+2*sigma), color='red',linetype="dashed")
```

1. Generate Simulated Data:

```R
set.seed(123)
x <- seq(0, 10, length = 50)
y <- sin(x) + rnorm(50, 0, 0.2)
df <- data.frame(x = x, y = y)
```

Simulated data is generated for the predictor variable `x` and the response variable `y`. The `x` values are generated as a sequence from 0 to 10 with 50 points. The `y` values are generated by taking the sine of each `x` value and adding random noise from a normal distribution with mean 0 and standard deviation 0.2. The data is then combined into a data frame `df`.

2. Specify Stan Model Code:


**1. data block**

```stan
data {
  int<lower=1> N;      // number of observations
  vector[N] x;         // univariate covariate
  vector[N] y;         // target variable
  int<lower=1> N2;     // number of test points
  vector[N2] x2;       // univariate test points
}
```

* `x`, `y` — training inputs and targets.
* `x2` — test inputs where predictions will be made.
* `N`, `N2` — number of training and test points.



**2. transformed data block**

```stan
transformed data {
  // Normalize data
  real xmean = mean(x);
  real ymean = mean(y);
  real xsd = sd(x);
  real ysd = sd(y);
  array[N] real xn = to_array_1d((x - xmean)/xsd);
  array[N2] real x2n = to_array_1d((x2 - xmean)/xsd);
  vector[N] yn = (y - ymean)/ysd;
  real sigma_intercept = 1;
  vector[N] zeros = rep_vector(0, N);
}
```

* This normalizes both inputs and outputs for numerical stability:

  * $x_n = \frac{x - \bar{x}}{sd(x)}$
  * $y_n = \frac{y - \bar{y}}{sd(y)}$
* The test inputs (`x2`) are normalized using the **training mean and sd**.
* `sigma_intercept = 1` acts like a constant term in the kernel (bias).
* `zeros` is a zero vector used as the GP mean function (mean-zero prior).

**3. parameters block**

```stan
parameters {
  real<lower=0> lengthscale_f; // lengthscale of f
  real<lower=0> sigma_f;       // scale of f
  real<lower=0> sigman;        // noise sigma
}
```

* `lengthscale_f`: controls how quickly the function varies with `x`.
* `sigma_f`: output scale (amplitude) of the GP prior.
* `sigman`: standard deviation of observation noise.


**4. model block**

```stan
model {
  // covariances and Cholesky decompositions
  matrix[N, N] K_f = gp_exp_quad_cov(xn, sigma_f, lengthscale_f) +
                     sigma_intercept^2;
  matrix[N, N] L_f = cholesky_decompose(add_diag(K_f, sigman^2));
  // priors
  lengthscale_f ~ normal(0, 1);
  sigma_f ~ normal(0, 1);
  sigman ~ normal(0, 1);
  // model
  yn ~ multi_normal_cholesky(zeros, L_f);
}
```

#### Kernel and covariance

* `gp_exp_quad_cov` computes the **squared exponential (RBF)** covariance:

  $$
  K_{ij} = \sigma_f^2 \exp\!\left(-\frac{(x_i - x_j)^2}{2\,l^2}\right)
  $$
* Adds a constant term `sigma_intercept^2` (bias kernel).
* Adds observation noise `sigman^2` to the diagonal:
  $K = K_f + \sigma_n^2 I$.

#### Likelihood

* `yn ~ multi_normal_cholesky(zeros, L_f);`
  means $y_n \sim \mathcal{N}(0, K)$.

**5. generated quantities block**

```stan
generated quantities {
  // function scaled back to the original scale
  vector[N2] f = gp_pred_rng(x2n, yn, xn, sigma_f, lengthscale_f, sigman, 1e-9)*ysd + ymean;
  real sigma = sigman*ysd;
}
```

* `gp_pred_rng` draws samples from the **posterior predictive distribution** of the GP at the normalized test inputs `x2n`.
* Predictions are **re-scaled back** to the original `y` scale using:

  $$
  f = \text{(predicted normalized value)} \times sd(y) + \bar{y}
  $$
* The posterior noise standard deviation `sigma` is also rescaled to match the original data scale.

**Summary of the model’s function**

This code:

1. Normalizes the training data for stable GP computation.
2. Defines a zero-mean GP prior with RBF covariance.
3. Learns the hyperparameters `lengthscale_f`, `sigma_f`, and `sigman` from the data.
4. Generates posterior predictive draws at new points `x2`, rescaled to the original units.


The Stan model code is specified as a character string. The model defines the data, parameters, and the statistical model for Bayesian GPR. It uses a Gaussian process kernel to model the relationship between the predictor variable `x` and the response variable `y`. The parameters `mu`, `sigma_f`, `sigma_n`, and `eta` represent the mean function, the covariance function for the underlying Gaussian process, the noise standard deviation, and the latent function values, respectively.

3. Compile Stan Model:

```R
gpr_stan_model <- stan_model(model_code = stan_model_code)
```

The Stan model is compiled using the `stan_model` function from the `rstan` package. This step converts the Stan model code into a C++ program that will be used for Bayesian inference.

4. Prepare Data for Stan Model:

```R
stan_data <- list(x=df$x,
                  x2=df$x,
                  y=df$y,
                  N=length(df$x),
                  N2=length(df$x))
```

The data is prepared as a list `stan_data` with the number of rows `N`, the predictor variable `x`, and the response variable `y`. This data will be used as input to the Stan model during sampling.

5. Fit Bayesian GPR Model using Stan:

```R
gpr_fit <- sampling(gpr_stan_model, data = stan_data)
```

The Bayesian GPR model is fitted using the `sampling` function from `rstan`. This step performs Markov chain Monte Carlo (MCMC) sampling to estimate the posterior distribution of the model parameters.

6. Extract Posterior Samples of f for Prediction:

```R
f_samples <- extract(gpr_fit, "f")$f
sigma_samples <- extract(gpr_fit, "sigma")$sigma


df %>%
  mutate(Ef=colMeans(f_samples),
         sigma=mean(sigma_samples)) %>%  
  ggplot(aes(x=x,y=y))+
  geom_point()+
  labs(x="Time (ms)", y="Acceleration (g)")+
  geom_line(aes(y=Ef), color='red')+
  geom_line(aes(y=Ef-2*sigma), color='red',linetype="dashed")+
  geom_line(aes(y=Ef+2*sigma), color='red',linetype="dashed")
```

The `extract` function is used to extract the posterior samples of the latent function `f` from the fitted GPR model. These samples will be used to predict new values of `f` for new values of `x`.




## References

+ Duvenaud, D. K., Nickisch, H., & Rasmussen, C. E. (2013). Gaussian processes for machine learning: tutorial. In S. Sra, S. Nowozin, & S. J. Wright (Eds.), Optimization for Machine Learning (pp. 133-181). MIT Press.
+ Nguyen, T. D., & Nguyen, T. T. (2018). Multi-task Gaussian process models for biomedical applications. arXiv preprint arXiv:1806.03836.
+ Alaa, A. M., & van der Schaar, M. (2018). Prognostication and risk factors for cystic fibrosis via automated machine learning and Gaussian process regression. Scientific Reports, 8(1), 1-12.
+ Nguyen, T. T., Nguyen, H. T., Nguyen, T. L., & Chetty, G. (2017). Gaussian process regression for predicting 30-day readmission of heart failure patients. Journal of Biomedical Informatics, 71, 199-209.
+ Kazemi, S., & Soltanian-Zadeh, H. (2013). A new Gaussian process regression-based method for segmentation of brain tissues from MRI. Medical Image Analysis, 17(3), 225-234.
+ [Gaussian process demonstration with Stan](https://avehtari.github.io/casestudies/Motorcycle/motorcycle_gpcourse.html)
  


<!--chapter:end:08-GPR.Rmd-->

# Bayesian Gaussian Mixture Models

A mixture model is a probabilistic framework used to describe populations composed of several unobserved subpopulations or latent components. Rather than assuming that each observation belongs to a single, known class, a mixture model treats membership as uncertain and infers the relative contribution of each latent process to the observed data. This approach is particularly useful when the overall distribution is multimodal or exhibits patterns that cannot be captured by a single parametric form. By representing the data as a weighted sum of simpler distributions, mixture models can approximate complex densities while retaining interpretability.

In the Gaussian case, the model assumes that each observation arises from one of ( K ) multivariate normal components, each with its own mean and covariance structure. Let ( y_1, \ldots, y_N \in \mathbb{R}^D ) denote the observed data points, where ( D ) is the dimensionality of the space. Each ( y_n ) is modeled as
[
p(y_n \mid \theta, {\mu_k, \Sigma_k}*{k=1}^K) = \sum*{k=1}^K \theta_k , \mathcal{N}(y_n \mid \mu_k, \Sigma_k),
]
where ( \theta = (\theta_1, \ldots, \theta_K) ) are the mixture weights satisfying ( \theta_k \ge 0 ) and ( \sum_{k=1}^K \theta_k = 1 ). The parameters ( \mu_k \in \mathbb{R}^D ) and ( \Sigma_k \in \mathbb{R}^{D \times D} ) denote the mean vector and covariance matrix of component ( k ). The overall likelihood of the dataset is thus
[
p(y_{1:N} \mid \theta, \mu_{1:K}, \Sigma_{1:K}) = \prod_{n=1}^{N} \sum_{k=1}^{K} \theta_k , \mathcal{N}(y_n \mid \mu_k, \Sigma_k).
]

The Bayesian formulation introduces priors over all unknown parameters, enabling coherent uncertainty quantification and regularization. In the Stan implementation, the mixture weights (\theta) follow a **simplex** constraint, ensuring they sum to one. The means (\mu_k) are given weakly informative Gaussian priors,
[
\mu_k \sim \mathcal{N}(0, 3^2),
]
while the covariance structure is encoded through the **Cholesky factor** of the correlation matrix (L_k). The prior
[
L_k \sim \text{LKJ}_{\text{Cholesky}}(\eta = 4)
]
induces a correlation prior with concentration parameter 4, slightly favoring correlations near zero without enforcing independence. The full covariance matrix for component (k) can then be reconstructed as (\Sigma_k = L_k L_k^\top).

Because Stan cannot sample discrete latent variables directly, the component indicators (z_n) are marginalized out analytically. For each data point, the model computes the log-density contribution across all components:
[
\text{ps}[k] = \log(\theta_k) + \log \mathcal{N}(y_n \mid \mu_k, L_k L_k^\top),
]
and then uses the **log-sum-exp** transformation to combine them stably:
[
\text{target} += \log \sum_{k=1}^K \exp(\text{ps}[k]).
]
This marginalization preserves the full probabilistic structure of the model while allowing the use of gradient-based samplers such as the No-U-Turn Sampler (NUTS) in Stan.

The model thus represents a fully Bayesian Gaussian mixture with continuous parameter inference. Each parameter sample corresponds to a plausible mixture configuration, and posterior uncertainty reflects both component overlap and weight variability. After inference, one can compute the posterior probability that observation (y_n) belongs to component (k),
[
p(z_n = k \mid y_n, \theta, \mu_{1:K}, \Sigma_{1:K}) = \frac{\theta_k , \mathcal{N}(y_n \mid \mu_k, \Sigma_k)}{\sum_{j=1}^K \theta_j , \mathcal{N}(y_n \mid \mu_j, \Sigma_j)},
]
yielding soft, probabilistic cluster assignments rather than discrete labels.

In practice, Bayesian Gaussian mixtures implemented in this way are both expressive and stable. They avoid the degeneracies common in maximum-likelihood estimation and automatically incorporate regularization through their priors. Moreover, they provide not only point estimates of the latent structure but also credible intervals for the mixture weights and component parameters, allowing for a principled assessment of uncertainty in clustering and density estimation alike.

In contrast to the Bayesian formulation, the **frequentist approach** to Gaussian mixture modeling typically relies on the **Expectation–Maximization (EM) algorithm** for maximum-likelihood estimation. The EM procedure alternates between computing *responsibilities*—the conditional probabilities (r_{nk} = p(z_n = k \mid y_n, \theta, \mu, \Sigma))—in the E-step, and updating the parameters (\theta_k, \mu_k, \Sigma_k) to maximize the expected complete-data log-likelihood in the M-step. While efficient and deterministic, this approach produces point estimates that ignore posterior uncertainty and can overfit when data are scarce or components overlap. The Bayesian mixture model, by contrast, integrates over parameter uncertainty and regularizes through its priors, yielding a full posterior distribution rather than a single best-fit configuration. In practice, this means that Bayesian inference captures the uncertainty about both component parameters and cluster assignments—whereas EM simply converges to one mode of the likelihood surface, often sensitive to initialization and prone to singular covariance estimates when clusters are small or poorly separated.


Here we first introduce how mixture models are implemented in Bayesian inference. It is noteworthy to take into consideration non-identifiability inherent these models how the non-identifiability can be tempered with principled prior information. Michael Betancourt has a blogpost describing the problems often encountered with gaussian mixture models, specifically the estimation of parameters of a mixture model and identifiability i.e. the problem with labelling [mixtures](http://mc-stan.org/documentation/case-studies/identifying_mixture_models.html). 

## Single varaible example

```
library(dplyr)
library(ggplot2)
library(ggthemes)

N <- 500

#  three clusters
mu <- c(1, 4, 9)
sigma <- c(1.2, 1, 0.8)

# probability of each cluster
Theta <- c(.3, .5, .3)

# Draw which model each belongs to
z <- sample(1:3, size = N, prob = Theta, replace = T)

# white noise
epsilon <- rnorm(N)

# Simulate the data using the fact that y ~ normal(mu, sigma) can be 
# expressed as y = mu + sigma*epsilon for epsilon ~ normal(0, 1)
y <- mu[z] + sigma[z]*epsilon

data_frame(y= y, z = as.factor(z)) %>% 
  ggplot(aes(x = y, fill = z)) +
  geom_density(alpha = 0.3) +
  ggtitle("Three clusters")
```

![](/images/gmm_1.png)


```
mixture_model<-'

// saved as finite_mixture_linear_regression.stan
data {
  int N;
  vector[N] y;
  int n_groups;
}
parameters {
  vector[n_groups] mu;
  vector<lower = 0>[n_groups] sigma;
  simplex[n_groups] Theta;
}
model {
  vector[n_groups] contributions;
  // priors
  mu ~ normal(0, 10);
  sigma ~ cauchy(0, 2);
  Theta ~ dirichlet(rep_vector(2.0, n_groups));
  
  
  // likelihood
  for(i in 1:N) {
    for(k in 1:n_groups) {
      contributions[k] = log(Theta[k]) + normal_lpdf(y[i] | mu[k], sigma[k]);
    }
    target += log_sum_exp(contributions);
  }
}'
```



**Data Block**
- `N`: Number of observations.
- `y`: Vector of observed responses.
- `n_groups`: Number of mixture components or groups.

**Parameters Block**
- `mu`: Vector of means for each mixture component.
- `sigma`: Vector of standard deviations for each mixture component.
- `Theta`: Vector of mixing proportions, representing the probability of each group.

**Model Block**
- **Priors**: Normal priors are specified for the means `mu` with a mean of 0 and a standard deviation of 10. Cauchy priors are specified for the standard deviations `sigma` with a location of 0 and a scale of 2. Dirichlet priors are specified for the mixing proportions `Theta` with equal concentration parameters of 2.0 for each group.
- **Likelihood**: The likelihood is constructed within a nested loop. For each observation `i` and each group `k`, it calculates the log-likelihood of the observation given the mean and standard deviation of that group. These log-likelihoods are stored in the `contributions` vector.
- **Log-Sum-Exp Trick**: To avoid numerical instability when dealing with small probabilities, the log-sum-exp trick is used. The `log_sum_exp` function sums up the contributions after exponentiating them. This is done to compute the log-likelihood of the data given the mixture model.
- **Target**: The `target` is incremented by the log of the sum of exponentiated contributions for each observation. The `target` is essentially the log-posterior, and the goal of Stan is to maximize it during sampling.


```
library(rstan)
options(mc.cores = parallel::detectCores())

fit=stan(model_code=mixture_model, data=list(N= N, y = y, n_groups = 3), iter=3000, warmup=500, chains=3)


print(fit)
params=extract(fit)
#density plots of the posteriors of the mixture means
par(mfrow=c(1,3))
plot(density(params$mu[,1]), ylab='', xlab='mu[1]', main='')
abline(v=c(8), lty='dotted', col='red',lwd=2)


plot(density(params$mu[,2]), ylab='', xlab='mu[1]', main='')
abline(v=c(0), lty='dotted', col='red',lwd=2)

plot(density(params$mu[,3]), ylab='', xlab='mu[1]', main='')
abline(v=c(4), lty='dotted', col='red',lwd=2)


```

```
Inference for Stan model: 9c40393d28e90e2c335fff95de690860.
3 chains, each with iter=3000; warmup=500; thin=1; 
post-warmup draws per chain=2500, total post-warmup draws=7500.

             mean se_mean   sd     2.5%      25%      50%      75%    97.5% n_eff  Rhat
mu[1]        6.35    3.06 3.76     0.65     1.21     8.96     9.02     9.11     2 21.87
mu[2]        3.72    3.05 3.74     0.59     0.94     1.25     8.96     9.09     2 13.86
mu[3]        4.03    0.00 0.17     3.71     3.92     4.03     4.14     4.36  2575  1.00
sigma[1]     0.85    0.17 0.23     0.62     0.69     0.73     1.02     1.41     2  2.32
sigma[2]     1.01    0.19 0.27     0.64     0.73     1.03     1.19     1.60     2  1.76
sigma[3]     1.13    0.00 0.12     0.92     1.05     1.12     1.20     1.39  3232  1.00
Theta[1]     0.27    0.00 0.04     0.19     0.25     0.27     0.29     0.34  1186  1.02
Theta[2]     0.27    0.00 0.05     0.18     0.24     0.26     0.29     0.40  1553  1.00
Theta[3]     0.47    0.00 0.06     0.32     0.44     0.47     0.51     0.57  1702  1.00
lp__     -1161.00    0.05 2.18 -1166.34 -1162.15 -1160.64 -1159.40 -1157.91  2064  1.00

Samples were drawn using NUTS(diag_e) at Tue Feb  6 13:03:03 2024.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
```

![](/images/gmm_2.png)

## Example with multiple variable

```
library(MASS)

#first cluster
mu1=c(0,0,0,0)
sigma1=matrix(c(0.2,0,0,0,0,0.2,0,0,0,0,0.1,0,0,0,0,0.1),ncol=4,nrow=4, byrow=TRUE)
norm1=mvrnorm(30, mu1, sigma1)

#second cluster
mu2=c(10,10,10,10)
sigma2=sigma1
norm2=mvrnorm(30, mu2, sigma2)

#third cluster
mu3=c(4,4,4,4)
sigma3=sigma1
norm3=mvrnorm(30, mu3, sigma3)

norms=rbind(norm1,norm2,norm3) #combine the 3 mixtures together
N=90 #total number of data points 
Dim=4 #number of dimensions
y=array(as.vector(norms), dim=c(N,Dim))
mixture_data=list(N=N, D=4, K=3, y=y)

as.data.frame(norms)  %>%
  pivot_longer(colnames(as.data.frame(norms)), names_to = "var", values_to = "value")%>%
  ggplot( aes(x=value, color=var)) + geom_density() +
  ggtitle("Three clusters on four variables")
```

![](/images/gmm_3.png)


```
mixture_model<-'
data {
 int D; //number of dimensions
 int K; //number of gaussians
 int N; //number of data
 vector[D] y[N]; //data
}

parameters {
 simplex[K] theta; //mixing proportions
 ordered[D] mu[K]; //mixture component means
 cholesky_factor_corr[D] L[K]; //cholesky factor of covariance
}

model {
 real ps[K];
 
 for(k in 1:K){
 mu[k] ~ normal(0,3);
 L[k] ~ lkj_corr_cholesky(4);
 }
 

 for (n in 1:N){
 for (k in 1:K){
 ps[k] = log(theta[k])+multi_normal_cholesky_lpdf(y[n] | mu[k], L[k]); 
 }
 target += log_sum_exp(ps);
 }

}'
```


**Data Block**
- `D`: Number of dimensions.
- `K`: Number of Gaussian components.
- `N`: Number of data points.
- `y`: An array of vectors, each representing a data point in D dimensions.

**Parameters Block**
- `theta`: Mixing proportions. It is a simplex, ensuring that the proportions sum to 1.
- `mu`: Mixture component means. These are ordered variables.
- `L`: Cholesky factors of the covariance matrices for each component.

**Model Block**
- **Priors**: Priors are specified for the means `mu` and the Cholesky factors `L`. Each mean is drawn from a normal distribution with a mean of 0 and a standard deviation of 3. The Cholesky factor is drawn from a LKJ correlation distribution with shape parameter 4.

- **Log-Probability Calculation**: For each data point `n` and each component `k`, the log-probability `ps[k]` is calculated. This log-probability is the logarithm of the product of the mixing proportion and the multivariate normal density of the data point under the k-th component.

- **Target Increment**: The `target` is incremented by the logarithm of the sum of exponentiated log-probabilities `ps`. This step ensures that the model assigns higher probability to data points that are well-explained by one of the Gaussian components.


```
fit=stan(model_code=mixture_model, data=mixture_data, iter=3000, warmup=1000, chains=1)
print(fit)

Inference for Stan model: f913dae683b9f29657b0863fec348d71.
1 chains, each with iter=3000; warmup=1000; thin=1; 
post-warmup draws per chain=2000, total post-warmup draws=2000.

            mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat
theta[1]    0.33    0.00 0.05    0.24    0.30    0.33    0.37    0.43  2120 1.00
theta[2]    0.33    0.00 0.05    0.24    0.30    0.33    0.36    0.44  1740 1.00
theta[3]    0.33    0.00 0.05    0.24    0.30    0.33    0.37    0.43  1854 1.00
mu[1,1]     3.80    0.00 0.11    3.55    3.74    3.80    3.86    3.99   632 1.01
mu[1,2]     3.91    0.01 0.12    3.67    3.84    3.90    3.98    4.15   236 1.00
mu[1,3]     4.02    0.01 0.13    3.78    3.92    4.02    4.11    4.29   344 1.00
mu[1,4]     4.09    0.01 0.15    3.82    3.99    4.09    4.20    4.39   259 1.00
mu[2,1]     9.77    0.01 0.12    9.51    9.70    9.79    9.86    9.98   405 1.00
mu[2,2]     9.96    0.00 0.11    9.74    9.90    9.96   10.03   10.20   794 1.00
mu[2,3]    10.09    0.00 0.11    9.91   10.01   10.07   10.15   10.33   518 1.00
mu[2,4]    10.18    0.01 0.12    9.97   10.09   10.17   10.25   10.43   498 1.00
mu[3,1]    -0.22    0.01 0.13   -0.49   -0.30   -0.21   -0.14    0.01   409 1.01
mu[3,2]    -0.10    0.01 0.13   -0.37   -0.17   -0.09   -0.02    0.17   218 1.00
mu[3,3]     0.07    0.01 0.13   -0.17    0.00    0.07    0.15    0.36    81 1.00
mu[3,4]     0.16    0.01 0.12   -0.03    0.08    0.15    0.23    0.42   541 1.00

For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
```

```

params=extract(fit)
#density plots of the posteriors of the mixture means
par(mfrow=c(1,3))
plot(density(params$mu[,1,1]), ylab='', xlab='mu[1]', main='')
lines(density(params$mu[,1,2]), col=rgb(0,0,0,0.7))
lines(density(params$mu[,1,3]), col=rgb(0,0,0,0.4))
lines(density(params$mu[,1,4]), col=rgb(0,0,0,0.1))
abline(v=c(4), lty='dotted', col='red',lwd=2)

plot(density(params$mu[,2,1]), ylab='', xlab='mu[2]', main='')
lines(density(params$mu[,2,2]), col=rgb(0,0,0,0.7))
lines(density(params$mu[,2,3]), col=rgb(0,0,0,0.4))
lines(density(params$mu[,2,4]), col=rgb(0,0,0,0.1))
abline(v=c(10), lty='dotted', col='red',lwd=2)

plot(density(params$mu[,3,1]), ylab='', xlab='mu[3]', main='')
lines(density(params$mu[,3,2]), col=rgb(0,0,0,0.7))
lines(density(params$mu[,3,3]), col=rgb(0,0,0,0.4))
lines(density(params$mu[,3,4]), col=rgb(0,0,0,0.1))
abline(v=c(0), lty='dotted', col='red',lwd=2)

```
![](/images/gmm_4.png)

## Conclusion


Bayesian mixture models offer several advantages in statistical modeling. Their inherent flexibility makes them well-suited for diverse tasks such as clustering, data compression, outlier detection, and generative classification. The Bayesian framework's ability to incorporate prior knowledge enhances model accuracy, especially when informative prior information is available. Moreover, these models effectively handle unobserved heterogeneity by integrating multiple data generating processes, proving valuable when data alone may not fully identify underlying patterns. The stability provided by Bayesian estimation ensures reliable posterior distributions, reducing sensitivity to issues like singularities, over-fitting, and violated identification criteria. Bayesian mixture models also facilitate the examination of the posterior distribution of the number of classes, offering insights into the underlying class structure of the data. However, the use of Bayesian mixture models comes with certain limitations. Applying these models demands a high level of statistical expertise to appropriately specify priors and ensure correct model formulation, presenting a challenge for practitioners lacking a strong background in Bayesian statistics. The complexity of posterior inference is compounded by label switching, a phenomenon that complicates the interpretation of results. Bayesian nonparametric mixture models, in particular, may suffer from inconsistency in estimating the number of clusters, impacting their performance in clustering applications. Additionally, model fitting challenges arise, and careful evaluation of inaccuracies in predictions and comparison with alternative models are essential to address potential shortcomings.
 
In this post, we learned to fit mixture models using Stan. We saw how to evaluate model fit using the usual prior and posterior predictive checks, and to investigate parameter recovery. Such mixture models are notoriously difficult to fit, but they have a lot of potential in cognitive science applications, especially in developing computational models of different kinds of cognitive processes. The reader interested in a deeper understanding of potential challanges in the process can refer to Betancourt discussion of identification problems in Bayesian mixture models in a [case study](https://mc-stan.org/users/documentation/case-studies/identifying_mixture_models.html). 



## References

- [Finite mixture models in Stan](https://modernstatisticalworkflow.blogspot.com/2016/10/finite-mixture-models-in-stan.html) 
- [Multivariate Gaussian Mixture Model done properly ](https://maggielieu.com/2017/03/21/multivariate-gaussian-mixture-model-done-properly/)
- [Finite Mixtures](https://mc-stan.org/docs/stan-users-guide/mixture-modeling.html) 
- [Identifying Bayesian Mixture Models](https://mc-stan.org/users/documentation/case-studies/identifying_mixture_models.html) 
- [Mixture models](https://vasishth.github.io/bayescogsci/book/ch-mixture.html) 
- [Bayesian Density Estimation (Finite Mixture Model) ](https://rpubs.com/kaz_yos/fmm2)
- [Bayesian mixture models (in)consistency for the number of clusters](https://hal.science/hal-03866434/document)
- [Advantages of a Bayesian Approach for Examining Class Structure in Finite Mixture Models](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6459682/) 



<!--chapter:end:09-GMM.Rmd-->


# Bayesian Canonical Correlation Analysis in Stan

## Introduction

Canonical Correlation Analysis (CCA) is a multivariate statistical method that identifies and quantifies the relationships between two sets of variables by finding linear combinations (canonical variates) that maximize their correlation. Unlike standard correlation analysis, which examines relationships between individual variables, CCA explores the joint structure of two variable sets, such as psychological test scores and academic performance metrics. In a Bayesian framework, CCA incorporates priors on the canonical coefficients, allowing for uncertainty quantification and regularization, which can improve estimation in small or noisy datasets. In this post, we will implement a Bayesian CCA model in Stan, focusing on estimating canonical coefficients and their correlations, and compare it to frequentist CCA. We will use synthetic data to illustrate the approach and highlight the benefits of Bayesian inference, such as full posterior distributions for uncertainty quantification.

### Canonical Correlation Analysis

CCA seeks pairs of linear combinations (canonical variates) from two sets of variables, $X \in \mathbbr^{n \times p}$ (with $p$ variables) and $Y \in \mathbbr^{n \times q}$ (with $q$ variables), such that the correlation between the variates is maximized. Mathematically, for variables $X$ and $Y$, CCA finds vectors $a_i \in \mathbbr^p$ and $b_i \in \mathbbr^q$ such that the canonical variates $U_i = X a_i$ and $V_i = Y b_i$ have maximum correlation, i.e.,

$$
(a_i, b_i) = \arg\max_{a, b} \; \text{corr}(X a, Y b),
$$

subject to the constraints that the variates are uncorrelated with previous pairs,

$$
\text{cov}(X a_i, X a_j) = 0, \quad \text{cov}(Y b_i, Y b_j) = 0 \quad \text{for } i \neq j,
$$

and have unit variance:

$$
\text{var}(X a_i) = \text{var}(Y b_i) = 1.
$$

In a Bayesian framework, we place priors on the canonical coefficients ($a_i$, $b_i$) and model the joint distribution of $X$ and $Y$, allowing for flexible regularization and uncertainty estimation.

In the Bayesian CCA model, we assume that the observed variables $X$ and $Y$ are generated from latent canonical variates with a shared structure. Specifically, let

$$
U = X A, \quad V = Y B,
$$

where $A \in \mathbbr^{p \times k}$ and $B \in \mathbbr^{q \times k}$ are matrices of canonical weights, and $k \leq \min(p, q)$ is the number of canonical components.

We place normal priors on the canonical coefficients:

$$
A_{ij} \sim \mathcal{N}(0, \sigma^2_A), \quad B_{ij} \sim \mathcal{N}(0, \sigma^2_B),
$$

and use a multivariate normal likelihood to model the joint distribution of $X$ and $Y$:

$$
\begin{bmatrix}
X \\
Y
\end{bmatrix}
\sim \mathcal{N} \left(
\begin{bmatrix}
0 \\
0
\end{bmatrix},
\Sigma
\right),
$$

with the cross-covariance structure implied by the canonical correlation:

$$
\Sigma =
\begin{bmatrix}
\Sigma_{XX} & \Sigma_{XY} \\
\Sigma_{YX} & \Sigma_{YY}
\end{bmatrix}, \quad \Sigma_{XY} = A C B^\top,
$$

where $C$ is a diagonal matrix of canonical correlations.

The model estimates the canonical coefficients and their correlations, with hyperpriors to regularize the variance parameters:

$$
\sigma_A, \sigma_B \sim \text{Half-Cauchy}(0, \tau),
$$

for some scale $\tau > 0$.

We start by generating synthetic data to demonstrate the Bayesian CCA model.

```r
# Improved Bayesian Canonical Correlation Analysis
# Load required libraries
library(tidyverse)
library(rstan)
library(rsample)
library(bayesplot)
library(ggplot2)

# Set Stan options for better performance
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

# Data Generation with more realistic structure
set.seed(123)
n <- 1000  # number of observations
p <- 4     # number of X variables (increased for more complexity)
q <- 3     # number of Y variables (increased for more complexity)

# Generate multiple latent canonical variates for richer structure
u1 <- rnorm(n, 0, 1)
u2 <- rnorm(n, 0, 0.5)  # Secondary canonical variate

# Create correlated canonical variates for Y
v1 <- 0.8 * u1 + rnorm(n, 0, sqrt(1 - 0.8^2))  # Strong correlation
v2 <- 0.5 * u2 + rnorm(n, 0, sqrt(1 - 0.5^2))  # Moderate correlation

# Generate X variables with varying loadings on canonical variates
X1 <- 0.9 * u1 + 0.3 * u2 + rnorm(n, 0, 0.5)
X2 <- 0.7 * u1 + 0.5 * u2 + rnorm(n, 0, 0.6)
X3 <- 0.5 * u1 + 0.8 * u2 + rnorm(n, 0, 0.4)
X4 <- 0.3 * u1 + 0.2 * u2 + rnorm(n, 0, 0.8)

# Generate Y variables with varying loadings
Y1 <- 0.8 * v1 + 0.2 * v2 + rnorm(n, 0, 0.4)
Y2 <- 0.6 * v1 + 0.7 * v2 + rnorm(n, 0, 0.5)
Y3 <- 0.4 * v1 + 0.9 * v2 + rnorm(n, 0, 0.3)

# Create properly structured data frame
X_matrix <- cbind(X1, X2, X3, X4)
Y_matrix <- cbind(Y1, Y2, Y3)

data <- data.frame(
  X_matrix,
  Y_matrix
)

# Rename columns for clarity
colnames(data) <- c(paste0("X", 1:p), paste0("Y", 1:q))

cat("Data structure:\n")
str(data)
cat("\nFirst few rows:\n")
head(data)

# Data splitting
set.seed(42)
data_split <- initial_split(data, prop = 0.7)
train_data <- training(data_split)
test_data <- testing(data_split)

# Extract X and Y matrices correctly
X_train <- as.matrix(train_data[, 1:p])
Y_train <- as.matrix(train_data[, (p+1):(p+q)])

X_test <- as.matrix(test_data[, 1:p])
Y_test <- as.matrix(test_data[, (p+1):(p+q)])
```

### Stan Model
Below is the Stan code for a Bayesian CCA model, which estimates the canonical coefficients for one pair of canonical variates.
```r
stan_model <- "
data {
  int<lower=0> N;               // number of observations
  int<lower=0> p;               // number of X variables
  int<lower=0> q;               // number of Y variables
  matrix[N, p] X;               // X variables (standardized)
  matrix[N, q] Y;               // Y variables (standardized)
}

parameters {
  vector[p] a;                  // canonical coefficients for X
  vector[q] b;                  // canonical coefficients for Y
  real<lower=0> sigma_u;        // SD for canonical variate u
  real<lower=0> sigma_v;        // SD for canonical variate v
  real<lower=-1, upper=1> rho;  // canonical correlation
  
  // Hierarchical priors for better regularization
  real<lower=0> tau_a;          // hierarchical scale for a
  real<lower=0> tau_b;          // hierarchical scale for b
}

transformed parameters {
  vector[N] u;                  // canonical variate for X
  vector[N] v;                  // canonical variate for Y
  
  // Compute canonical variates
  u = X * a;
  v = Y * b;
  
  // Standardize canonical variates for identifiability
  real u_mean = mean(u);
  real v_mean = mean(v);
  real u_sd = sd(u);
  real v_sd = sd(v);
}

model {
  // Hierarchical priors
  tau_a ~ cauchy(0, 1);
  tau_b ~ cauchy(0, 1);
  
  // Priors for canonical coefficients with hierarchical regularization
  a ~ normal(0, tau_a);
  b ~ normal(0, tau_b);
  
  // Priors for variance parameters
  sigma_u ~ cauchy(0, 1);
  sigma_v ~ cauchy(0, 1);
  
  // Prior for canonical correlation
  rho ~ beta(2, 2);  // Slightly informative prior favoring moderate correlations
  
  // Likelihood for canonical variates
  u ~ normal(0, sigma_u);
  v ~ normal(rho * u, sigma_v * sqrt(1 - rho^2));
  
  // Constraint for identifiability (optional - can help with convergence)
  target += -0.5 * dot_self(a) - 0.5 * dot_self(b);
}

generated quantities {
  // Posterior predictions
  vector[N] u_pred;
  vector[N] v_pred;
  matrix[N, p] X_pred;
  matrix[N, q] Y_pred;
  
  // Log likelihood for model comparison
  real log_lik = 0;
  
  // Generate predictions
  for (n in 1:N) {
    u_pred[n] = normal_rng(0, sigma_u);
    v_pred[n] = normal_rng(rho * u_pred[n], sigma_v * sqrt(1 - rho^2));
  }
  
  // Predicted X and Y based on canonical variates
  X_pred = rep_matrix(u_pred, p);
  Y_pred = rep_matrix(v_pred, q);
  
  // Calculate log likelihood
  for (n in 1:N) {
    log_lik += normal_lpdf(u[n] | 0, sigma_u);
    log_lik += normal_lpdf(v[n] | rho * u[n], sigma_v * sqrt(1 - rho^2));
  }
}
"
```
This Stan model implements Bayesian Canonical Correlation Analysis by first defining the data structure with `N` observations, `p` X variables, and `q` Y variables stored in matrices `X` and `Y`. The parameters section declares the key unknowns: canonical coefficient vectors `a` and `b` that will create linear combinations of the original variables, standard deviations `sigma_u` and `sigma_v` for the canonical variates, the canonical correlation `rho` (constrained between -1 and 1), and hierarchical regularization parameters `tau_a` and `tau_b`. In the transformed parameters block, the model computes the canonical variates `u = X * a` and `v = Y * b`, which are the linear combinations that maximize correlation between the two sets of variables, along with their means and standard deviations for standardization purposes.

The model block establishes the probabilistic relationships through priors and likelihood functions. Hierarchical priors are set with `tau_a ~ cauchy(0, 1)` and `tau_b ~ cauchy(0, 1)`, which then inform the canonical coefficients `a ~ normal(0, tau_a)` and `b ~ normal(0, tau_b)`, providing adaptive regularization. The variance parameters `sigma_u` and `sigma_v` receive Cauchy priors, while `rho` gets a Beta(2,2) prior favoring moderate correlations. The core likelihood assumes `u ~ normal(0, sigma_u)` and `v ~ normal(rho * u, sigma_v * sqrt(1 - rho^2))`, establishing that the canonical variates follow a bivariate normal distribution with correlation `rho`. The model adds an identifiability constraint `target += -0.5 * dot_self(a) - 0.5 * dot_self(b)` to prevent scaling issues. Finally, the generated quantities block creates posterior predictions by sampling new canonical variates `u_pred` and `v_pred` from their respective distributions, generates predicted data matrices `X_pred` and `Y_pred`, and computes the log-likelihood `log_lik` for model comparison purposes.

#### Fitting the Model
Prepare the data and fit the model using `rstan`.

```r
X_scaled <- scale(X_train)
Y_scaled <- scale(Y_train)

stan_data <- list(
  N = nrow(X_scaled),
  p = ncol(X_scaled),
  q = ncol(Y_scaled),
  X = X_scaled,
  Y = Y_scaled
)

cat("Stan data structure:\n")
str(stan_data)

# Compile and fit the model
cat("\nCompiling and fitting Stan model...\n")
fit_cca <- stan(
  model_code = stan_model,
  data = stan_data,
  iter = 2000,
  warmup = 1000,
  chains = 4,
  cores = 4,
  control = list(adapt_delta = 0.95, max_treedepth = 12),
  verbose = TRUE
)

# Model diagnostics
cat("\nModel Summary:\n")
print(fit_cca, pars = c("rho", "sigma_u", "sigma_v", "tau_a", "tau_b"))

# Check convergence diagnostics
cat("\nConvergence Diagnostics:\n")
cat("R-hat values:\n")
rhat_values <- rhat(fit_cca)
print(rhat_values[rhat_values > 1.1])

cat("\nEffective sample sizes:\n")
eff_samples <- neff_ratio(fit_cca)
print(eff_samples[eff_samples < 0.1])

# Extract posterior samples
posterior_samples <- extract(fit_cca)

# Visualization of results
if (require(bayesplot)) {
  # Trace plots for key parameters
  mcmc_trace(fit_cca, pars = c("rho", "sigma_u", "sigma_v"))
  
  # Posterior density plots
  mcmc_dens(fit_cca, pars = c("rho", "sigma_u", "sigma_v"))
  
  # Posterior intervals for canonical coefficients
  mcmc_intervals(fit_cca, pars = paste0("a[", 1:p, "]"))
  mcmc_intervals(fit_cca, pars = paste0("b[", 1:q, "]"))
}

# Summary statistics
cat("\nPosterior Summary for Key Parameters:\n")
cat("Canonical Correlation (rho):\n")
cat("Mean:", mean(posterior_samples$rho), "\n")
cat("95% CI:", quantile(posterior_samples$rho, c(0.025, 0.975)), "\n")

cat("\nCanonical Coefficients for X (a):\n")
print(colMeans(posterior_samples$a))

cat("\nCanonical Coefficients for Y (b):\n")
print(colMeans(posterior_samples$b))
```

### Model validation on test set

```r
X_test_scaled <- scale(X_test, center = attr(X_scaled, "scaled:center"), 
                       scale = attr(X_scaled, "scaled:scale"))
Y_test_scaled <- scale(Y_test, center = attr(Y_scaled, "scaled:center"), 
                       scale = attr(Y_scaled, "scaled:scale"))

# Calculate canonical variates for test set
a_mean <- colMeans(posterior_samples$a)
b_mean <- colMeans(posterior_samples$b)

u_test <- X_test_scaled %*% a_mean
v_test <- Y_test_scaled %*% b_mean

test_correlation <- cor(u_test, v_test)
```

## Conclusion
In this post, we explored Bayesian Canonical Correlation Analysis, a method to uncover relationships between two sets of variables through maximally correlated linear combinations. By implementing a Bayesian CCA model in Stan, we demonstrated how priors on canonical coefficients and hyperpriors on variance parameters enable robust estimation and uncertainty quantification. Compared to frequentist CCA, the Bayesian approach provides full posterior distributions, eliminating the need for bootstrapping to estimate uncertainty. This makes it particularly useful for small or noisy datasets. The Stan implementation allows for flexible extensions, such as modeling multiple canonical pairs or incorporating hierarchical structures.

## References
- Stan User’s Guide on Multivariate Models
- Gelman, A., & Hill, J. (2006). *Data Analysis Using Regression and Multilevel/Hierarchical Models*
- Krzanowski, W. J. (2000). *Principles of Multivariate Analysis*

<!--chapter:end:10-CNCLCOR.Rmd-->

# Bayesian Seasonal Decomposition in Stan and RStan

Understanding the components that make up a time series is a critical first step in analysis and forecasting. Classical seasonal decomposition techniques, such as STL or X-11, offer ways to separate a series into **trend**, **seasonal**, and **irregular** components. However, these methods are often deterministic and lack a coherent probabilistic framework for uncertainty quantification.

In contrast, **Bayesian seasonal decomposition** allows us to model uncertainty in each component explicitly. In this post, we demonstrate how to construct a **Bayesian structural time series model** in Stan, decompose a seasonal time series, and obtain posterior distributions for each latent component.


##  Simulating Seasonal Time Series Data* 

We first simulate a synthetic time series with additive trend, seasonality, and noise:

```{r}
set.seed(123)
n <- 120
t <- 1:n
trend <- 0.05 * t
seasonal_period <- 12
seasonal <- rep(sin(2 * pi * (1:seasonal_period) / seasonal_period), length.out = n)
noise <- rnorm(n, 0, 0.5)
y <- trend + seasonal + noise

ts.plot(y, main = "Simulated Time Series with Trend and Seasonality")
```



##  Bayesian Seasonal Decomposition Model in Stan 

We use an additive structural model:

$$
y_t = \mu_t + \gamma_t + \epsilon_t
$$

* $\mu_t$: Local level (trend)
* $\gamma_t$: Seasonal component
* $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$: Irregular component

We impose a **local level model** for the trend:

$$
\mu_t = \mu_{t-1} + \eta_t, \quad \eta_t \sim \mathcal{N}(0, \sigma_{\mu}^2)
$$

And we model seasonality with a constraint that its sum over one period equals zero (to ensure identifiability):

```{r}
stan_model_1 <-'data {
  int<lower=2> N;
  int<lower=2> s;           // seasonal period
  vector[N] y;
}
parameters {
  vector[N] mu;             // trend
  vector[s] season_raw;     // raw seasonal effects
  real<lower=0> sigma;
  real<lower=0> sigma_mu;
  real<lower=0> sigma_season;
}
transformed parameters {
  vector[N] season;
  vector[s] season_clean;
  
  // Center seasonal component to sum to zero
  season_clean = season_raw - mean(season_raw);
  
  for (t in 1:N)
    season[t] = season_clean[1 + ((t - 1) % s)];
}
model {
  mu[2:N] ~ normal(mu[1:(N - 1)], sigma_mu);
  season_raw ~ normal(0, sigma_season);
  y ~ normal(mu + season, sigma);
}'
```



##  Fitting the Model in R 

We fit this model using RStan:

```{r}
library(rstan)

data_list <- list(
  N = length(y),
  s = seasonal_period,
  y = y
)

fit <- stan(
  model_code = stan_model_1,
  data = data_list,
  chains = 4,
  iter = 2000,
  warmup = 1000,
  seed = 123
)

print(fit, pars = c("sigma", "sigma_mu", "sigma_season"))
```



## Extracting and Visualizing Components

We now extract the posterior estimates and visualize the trend and seasonal components:

```{r}
posterior <- extract(fit)
mu_hat <- apply(posterior$mu, 2, mean)
season_hat <- apply(posterior$season, 2, mean)

par(mfrow = c(3, 1), mar = c(4, 4, 2, 1))
plot(y, type = 'l', main = "Observed Time Series", ylab = "y")
plot(mu_hat, type = 'l', col = "blue", main = "Estimated Trend (mu)", ylab = "mu_t")
plot(season_hat, type = 'l', col = "darkgreen", main = "Estimated Seasonal Component", ylab = "season_t")
```

This visual decomposition gives insight into how much of the variation is driven by the underlying trend and how much by recurring seasonal patterns. The Bayesian framework also allows for full posterior uncertainty quantification, which can be visualized with credible intervals around each component.



## Extensions and Applications 

Bayesian structural time series models can be extended in numerous ways. One may introduce regression components to account for covariates (leading to models like BSTS), or allow the trend to include a slope (i.e., a local linear trend). Multivariate or hierarchical seasonal decomposition is also possible and particularly useful for grouped or panel time series.



## Conclusion 

Bayesian seasonal decomposition offers a principled approach to breaking down time series into interpretable components while rigorously accounting for uncertainty. Compared to classical methods, the Bayesian formulation in Stan provides flexibility, interpretability, and extensibility. Whether used for exploratory analysis or as a preprocessing step for forecasting, structural time series decomposition is a valuable tool in the Bayesian modeler’s toolkit.

<!--chapter:end:11-SDCMP.Rmd-->



# Bayesian Exponential Smoothing and Holt-Winters Models in Stan and RStan

Time series data often reflect patterns of level, trend, and seasonality. While ARIMA-style models are foundational, exponential smoothing techniques are especially appealing when forecasting with recent observations is more informative than distant ones. In this post, we explore how to model these systems in a Bayesian framework using **Stan** and **RStan**, bringing full probabilistic inference and uncertainty quantification to these well-known methods.

We walk through the following:

1. **Bayesian Simple Exponential Smoothing (SES)**
2. **Bayesian Holt-Winters Models**

   * Without seasonality (trend only)
   * With seasonality (additive seasonal component)



## Bayesian Simple Exponential Smoothing (SES)

Simple Exponential Smoothing assumes a local level that evolves recursively:

$$
\ell_t = \alpha y_{t-1} + (1 - \alpha) \ell_{t-1} \\
y_t \sim \mathcal{N}(\ell_t, \sigma^2)
$$

To simulate a series under this model:

```r
set.seed(1)
n <- 100
alpha <- 0.3
sigma <- 1
l <- numeric(n)
y <- numeric(n)
l[1] <- 10
y[1] <- l[1] + rnorm(1, 0, sigma)
for (t in 2:n) {
  l[t] <- alpha * y[t - 1] + (1 - alpha) * l[t - 1]
  y[t] <- l[t] + rnorm(1, 0, sigma)
}
ts.plot(y, main = "Simulated SES Time Series")
```

### Stan Model

```stan
data {
  int<lower=2> N;
  vector[N] y;
}
parameters {
  real<lower=0, upper=1> alpha;
  real<lower=0> sigma;
  vector[N] l;  // local level
}
model {
  l[1] ~ normal(y[1], sigma);
  for (t in 2:N)
    l[t] ~ normal(alpha * y[t-1] + (1 - alpha) * l[t-1], sigma);
  y ~ normal(l, sigma);
}
```

### Fitting in R

```r
library(rstan)
data_list <- list(N = length(y), y = y)
fit <- stan(model_code = stan_model_1, data = data_list, chains = 4, iter = 2000, warmup = 1000)
print(fit, pars = c("alpha", "sigma"))
```

### Visualization

```r
post <- extract(fit)
l_hat <- apply(post$l, 2, mean)
l_ci_lower <- apply(post$l, 2, quantile, 0.025)
l_ci_upper <- apply(post$l, 2, quantile, 0.975)

df <- data.frame(
  time = 1:n, observed = y, true_level = l,
  posterior_mean = l_hat, lower_95 = l_ci_lower, upper_95 = l_ci_upper
)

ggplot(df, aes(x = time)) +
  geom_line(aes(y = observed), linetype = "dashed", color = "black") +
  geom_line(aes(y = true_level), color = "blue", alpha = 0.5) +
  geom_line(aes(y = posterior_mean), color = "red") +
  geom_ribbon(aes(ymin = lower_95, ymax = upper_95), fill = "red", alpha = 0.2) +
  labs(title = "Bayesian SES: Posterior Level Estimates vs True Level", y = "Value") +
  theme_minimal()
```



## Bayesian Holt-Winters Models

### Without Seasonality (Additive Trend)

Holt’s method models both level and trend:

$$
\begin{aligned}
\ell_t &= \alpha y_{t-1} + (1 - \alpha)(\ell_{t-1} + b_{t-1}) \\
b_t &= \beta(\ell_t - \ell_{t-1}) + (1 - \beta)b_{t-1} \\
y_t &\sim \mathcal{N}(\ell_t + b_t, \sigma^2)
\end{aligned}
$$

```r
set.seed(2)
n <- 100
alpha <- 0.3
beta <- 0.1
sigma <- 1
l <- numeric(n)
b <- numeric(n)
y <- numeric(n)
l[1] <- 5
b[1] <- 0.5
y[1] <- l[1] + b[1] + rnorm(1, 0, sigma)
for (t in 2:n) {
  l[t] <- alpha * y[t - 1] + (1 - alpha) * (l[t - 1] + b[t - 1])
  b[t] <- beta * (l[t] - l[t - 1]) + (1 - beta) * b[t - 1]
  y[t] <- l[t] + b[t] + rnorm(1, 0, sigma)
}
ts.plot(y, main = "Simulated Holt-Winters Time Series")
```

### Stan Model

```stan
data {
  int<lower=2> N;
  vector[N] y;
}
parameters {
  real<lower=0, upper=1> alpha;
  real<lower=0, upper=1> beta;
  real<lower=0> sigma;
  vector[N] l;
  vector[N] b;
}
model {
  l[1] ~ normal(y[1], sigma);
  b[1] ~ normal(0, sigma);
  for (t in 2:N) {
    l[t] ~ normal(alpha * y[t-1] + (1 - alpha) * (l[t-1] + b[t-1]), sigma);
    b[t] ~ normal(beta * (l[t] - l[t-1]) + (1 - beta) * b[t-1], sigma);
  }
  y ~ normal(l + b, sigma);
}
```

### Visualization

```r
post <- extract(fit)
l_hat <- apply(post$l, 2, mean)
b_hat <- apply(post$b, 2, mean)
y_hat <- l_hat + b_hat
l_ci_lower <- apply(post$l, 2, quantile, 0.025)
l_ci_upper <- apply(post$l, 2, quantile, 0.975)

df <- data.frame(
  time = 1:n, observed = y,
  predicted = y_hat, level_mean = l_hat,
  level_lower = l_ci_lower, level_upper = l_ci_upper
)

ggplot(df, aes(x = time)) +
  geom_line(aes(y = observed), linetype = "dashed", color = "black") +
  geom_line(aes(y = predicted), color = "red") +
  geom_ribbon(aes(ymin = level_lower, ymax = level_upper), fill = "red", alpha = 0.2) +
  labs(title = "Bayesian Holt-Winters: Posterior Level & Trend Estimates", y = "Value") +
  theme_minimal()
```



### With Seasonality (Additive Seasonal Component)

The full Holt-Winters seasonal model captures all three components:

$$
\begin{aligned}
\ell_t &= \alpha(y_{t-1} - s_{t-m}) + (1 - \alpha)(\ell_{t-1} + b_{t-1}) \\
b_t &= \beta(\ell_t - \ell_{t-1}) + (1 - \beta)b_{t-1} \\
s_t &= \gamma(y_{t-1} - \ell_{t-1} - b_{t-1}) + (1 - \gamma)s_{t-m} \\
y_t &\sim \mathcal{N}(\ell_t + b_t + s_{t-m}, \sigma^2)
\end{aligned}
$$

```r
set.seed(3)
n <- 120
m <- 12
alpha <- 0.3; beta <- 0.1; gamma <- 0.2; sigma <- 1
l <- numeric(n); b <- numeric(n); s <- numeric(n); y <- numeric(n)
l[1] <- 10; b[1] <- 0.5; s[1:m] <- sin(2 * pi * (1:m) / m)
y[1] <- l[1] + b[1] + s[1] + rnorm(1, 0, sigma)
for (t in 2:n) {
  if (t > m) {
    l[t] <- alpha * (y[t - 1] - s[t - m]) + (1 - alpha) * (l[t - 1] + b[t - 1])
    b[t] <- beta * (l[t] - l[t - 1]) + (1 - beta) * b[t - 1]
    s[t] <- gamma * (y[t - 1] - l[t - 1] - b[t - 1]) + (1 - gamma) * s[t - m]
  } else {
    l[t] <- l[t - 1]; b[t] <- b[t - 1]; s[t] <- s[t - 1]
  }
  y[t] <- l[t] + b[t] + s[t %% m + 1] + rnorm(1, 0, sigma)
}
ts.plot(y, main = "Simulated Seasonal Holt-Winters Time Series")
```
```r
### Stan Model (Additive Seasonal)

stan_model_3='data {
  int<lower=2> N;         // number of observations
  int<lower=1> m;         // seasonal period
  vector[N] y;
}
parameters {
  real<lower=0, upper=1> alpha;
  real<lower=0, upper=1> beta;
  real<lower=0, upper=1> gamma;
  real<lower=0> sigma;
  
  vector[N] l;            // level
  vector[N] b;            // trend
  vector[N] s;            // seasonal
}
model {
  l[1] ~ normal(y[1], sigma);
  b[1] ~ normal(0, sigma);
  for (i in 1:m)
    s[i] ~ normal(0, 1);
  
  for (t in 2:N) {
    if (t > m) {
      l[t] ~ normal(alpha * (y[t - 1] - s[t - m]) + (1 - alpha) * (l[t - 1] + b[t - 1]), sigma);
      b[t] ~ normal(beta * (l[t] - l[t - 1]) + (1 - beta) * b[t - 1], sigma);
      s[t] ~ normal(gamma * (y[t - 1] - l[t - 1] - b[t - 1]) + (1 - gamma) * s[t - m], sigma);
    } else {
      l[t] ~ normal(l[t - 1], 1);
      b[t] ~ normal(b[t - 1], 1);
      s[t] ~ normal(s[t - 1], 1);
    }
  }
  
  for (t in 1:N) {
    if (t > m)
      y[t] ~ normal(l[t] + b[t] + s[t - m], sigma);
    else
      y[t] ~ normal(l[t] + b[t] + s[t], sigma);
  }
}'
```



### Fit

```r
library(rstan)

data_list <- list(N = length(y), y = y, m = m)
fit <- stan(model_code = stan_model_3, data = data_list,
            chains = 4, iter = 2000, warmup = 1000)

print(fit, pars = c("alpha", "beta", "gamma", "sigma"))

post <- rstan::extract(fit)

l_hat <- apply(post$l, 2, mean)
l_ci_lower <- apply(post$l, 2, quantile, probs = 0.025)
l_ci_upper <- apply(post$l, 2, quantile, probs = 0.975)

df <- data.frame(
  time = 1:n,
  observed = y,
  level_mean = l_hat,
  level_lower = l_ci_lower,
  level_upper = l_ci_upper
)
```


### Visualization

```r
post <- extract(fit)
l_hat <- apply(post$l, 2, mean)
l_ci_lower <- apply(post$l, 2, quantile, 0.025)
l_ci_upper <- apply(post$l, 2, quantile, 0.975)

df <- data.frame(
  time = 1:n,
  observed = y,
  level_mean = l_hat,
  level_lower = l_ci_lower,
  level_upper = l_ci_upper
)

ggplot(df, aes(x = time)) +
  geom_line(aes(y = observed), linetype = "dashed", color = "black") +
  geom_line(aes(y = level_mean), color = "red") +
  geom_ribbon(aes(ymin = level_lower, ymax = level_upper), fill = "red", alpha = 0.2) +
  labs(title = "Bayesian Seasonal Holt-Winters: Posterior Level Estimates", y = "Value") +
  theme_minimal()
```

## Conclusion

Bayesian formulations of Exponential Smoothing and Holt-Winters models offer elegant alternatives to frequentist smoothing, with the advantage of posterior inference, natural handling of uncertainty, and full probabilistic forecasting. Using **Stan** via **RStan**, we can model level, trend, and seasonality in a transparent and interpretable way—crucial for applications in forecasting and planning. These methods can be extended further to include hierarchical structure, time-varying parameters, or non-Gaussian errors for more complex real-world tasks.

<!--chapter:end:12-HOLT.Rmd-->


# Bayesian AR, ARMA, and ARIMA Models in Stan and RStan

Bayesian methods for time series modeling have gained widespread appeal for their ability to provide full posterior inference, quantify uncertainty, and incorporate prior knowledge into the analysis of temporal data. In this post, we walk through the foundational Bayesian time series models: the autoregressive (AR), autoregressive moving average (ARMA), and autoregressive integrated moving average (ARIMA) models. Each builds upon the previous, offering increased flexibility for real-world applications.

We implement each model using **Stan**, a powerful probabilistic programming language, and **RStan**, its R interface. Through these examples, we aim to offer a practical understanding of how Bayesian time series models are constructed, estimated, and interpreted.


## Bayesian AR(1) Model

We begin with the simplest dynamic model: the **AR(1)** process. This model assumes that the current value of the time series depends linearly on its previous value plus a noise term. Formally,

$$
y_t = \alpha + \phi y_{t-1} + \epsilon_t, \quad \epsilon_t \sim \mathcal{N}(0, \sigma^2)
$$

To simulate such a process in R:

```r
set.seed(123)
n <- 100
phi <- 0.7
sigma <- 1
y <- numeric(n)
y[1] <- rnorm(1, 0, sigma / sqrt(1 - phi^2))
for (t in 2:n) {
  y[t] <- phi * y[t - 1] + rnorm(1, 0, sigma)
}
ts.plot(y, main = "Simulated AR(1) Time Series")
```

We fit this model in Stan using the following code:

```stan
data {
  int<lower=1> N;
  vector[N] y;
}
parameters {
  real alpha;
  real<lower=-1, upper=1> phi;
  real<lower=0> sigma;
}
model {
  y[2:N] ~ normal(alpha + phi * y[1:N-1], sigma);
}
```

And we call the model in R as follows:

```r
library(rstan)
data_list <- list(N = length(y), y = y)
fit <- stan(
  file = "ar1_model.stan",
  data = data_list,
  chains = 4,
  iter = 2000,
  warmup = 1000
)
print(fit)
```


## Bayesian ARMA(1,1) Model

While the AR model captures persistence in the series, it cannot account for short-term shocks that decay quickly. The **ARMA(1,1)** model addresses this by including a moving average component:

$$
y_t = \alpha + \phi y_{t-1} + \epsilon_t + \theta \epsilon_{t-1}, \quad \epsilon_t \sim \mathcal{N}(0, \sigma^2)
$$

To simulate an ARMA(1,1) process in R:

```r
set.seed(123)
n <- 200
phi <- 0.6
theta <- 0.5
sigma <- 1
y <- numeric(n)
e <- rnorm(n, 0, sigma)
y[1] <- e[1]
for (t in 2:n) {
  y[t] <- phi * y[t - 1] + e[t] + theta * e[t - 1]
}
ts.plot(y, main = "Simulated ARMA(1,1) Time Series")
```

The Stan model computes residuals explicitly in a transformed parameters block, as recursion over parameters is not allowed:

```stan
data {
  int<lower=2> N;
  vector[N] y;
}
parameters {
  real alpha;
  real<lower=-1, upper=1> phi;
  real<lower=-1, upper=1> theta;
  real<lower=0> sigma;
}
transformed parameters {
  vector[N] mu;
  vector[N] eps;
  mu[1] = alpha;
  eps[1] = y[1] - mu[1];
  for (t in 2:N) {
    mu[t] = alpha + phi * y[t - 1] + theta * eps[t - 1];
    eps[t] = y[t] - mu[t];
  }
}
model {
  eps[2:N] ~ normal(0, sigma);
}
```

Model fitting proceeds in R with:

```r
data_list <- list(N = length(y), y = y)
fit <- stan(
  file = "arma11.stan",
  data = data_list,
  chains = 4,
  iter = 2000,
  warmup = 1000
)
print(fit, pars = c("alpha", "phi", "theta", "sigma"))
```


## Bayesian ARIMA(1,1,1) Model

Many time series exhibit non-stationary behavior, such as trends, that must be differenced away before modeling. The **ARIMA(1,1,1)** model applies a first-order difference to the series and models the differenced data as ARMA(1,1):

$$
\Delta y_t = y_t - y_{t-1} = \alpha + \phi \Delta y_{t-1} + \epsilon_t + \theta \epsilon_{t-1}
$$

We first simulate an ARIMA(1,1,1) process in R:

```r
set.seed(42)
n <- 200
phi <- 0.6
theta <- 0.5
sigma <- 1
eps <- rnorm(n, 0, sigma)
dy <- numeric(n)
dy[1] <- eps[1]
for (t in 2:n) {
  dy[t] <- phi * dy[t - 1] + eps[t] + theta * eps[t - 1]
}
y <- cumsum(dy)
ts.plot(y, main = "Simulated ARIMA(1,1,1) Time Series")
```

The Stan model computes the first difference internally and fits the resulting ARMA process:

```stan
data {
  int<lower=2> N;
  vector[N] y;
}
transformed data {
  vector[N - 1] dy;
  for (t in 2:N) dy[t - 1] = y[t] - y[t - 1];
}
parameters {
  real alpha;
  real<lower=-1, upper=1> phi;
  real<lower=-1, upper=1> theta;
  real<lower=0> sigma;
}
transformed parameters {
  vector[N - 1] mu;
  vector[N - 1] eps;
  mu[1] = alpha;
  eps[1] = dy[1] - mu[1];
  for (t in 2:(N - 1)) {
    mu[t] = alpha + phi * dy[t - 1] + theta * eps[t - 1];
    eps[t] = dy[t] - mu[t];
  }
}
model {
  eps[2:(N - 1)] ~ normal(0, sigma);
}
```

We estimate the model with RStan using:

```r
data_list <- list(N = length(y), y = y)
fit <- stan(
  file = "arima11.stan",
  data = data_list,
  chains = 4,
  iter = 2000,
  warmup = 1000
)
print(fit, pars = c("alpha", "phi", "theta", "sigma"))
```

## Conclusion

Bayesian time series modeling provides a flexible, interpretable framework for analyzing temporal data. Beginning with the autoregressive model, we can successively build toward more complex structures like ARMA and ARIMA. Each model expands our capacity to handle persistence, shocks, and non-stationarity in a principled probabilistic way. While Bayesian inference comes with computational cost, it also brings the benefits of full uncertainty quantification, model extensibility, and coherent predictions under uncertainty. Using Stan and RStan, we can translate classical time series models into a Bayesian framework and apply them to real-world data with transparency and rigor. Future work may involve extending these models to accommodate seasonality, hierarchical structure, or time-varying parameters.


<!--chapter:end:13-ARIMA.Rmd-->

# 14 Bayesian Structural Time Series Models 

## Introduction

Time series data pervade scientific and applied domains, from economics and epidemiology to environmental monitoring and finance. Extracting meaningful patterns from such data requires statistical models that can handle temporal dependencies, seasonality, trends, and structural breaks while providing interpretable decompositions. Bayesian Structural Time Series (BSTS) models offer a principled framework for addressing these challenges by decomposing time series into latent components while incorporating prior information and quantifying uncertainty.

Initially popularized by Google for causal impact analysis, BSTS models extend classical structural time series approaches by embedding them within a fully Bayesian inference framework. However, many real-world time series exhibit periods of varying volatility—economic indicators may experience stable growth followed by high volatility due to macroeconomic shocks, while financial markets alternate between calm and turbulent periods. Standard BSTS models assume constant volatility in trend components, which can be overly restrictive and obscure critical temporal dynamics.

In this comprehensive post, we explore both canonical BSTS models and their extension to time-varying trend volatility. We provide complete implementations in Stan, demonstrate fitting procedures in R using RStan, and illustrate applications across different scenarios. This unified treatment enables practitioners to understand the full spectrum of BSTS modeling capabilities and choose appropriate specifications for their specific time series challenges.

#### Model Specification: Core BSTS Framework

The canonical BSTS model represents an observed time series as the sum of interpretable latent components:

$$y_t = \mu_t + \tau_t + \gamma_t + \varepsilon_t, \quad \varepsilon_t \sim \mathcal{N}(0, \sigma^2)$$

where:
- $\mu_t$ is the local level component
- $\tau_t$ represents the stochastic trend (slope or drift)
- $\gamma_t$ captures seasonal effects
- $\varepsilon_t$ is the observation error term

The local level and trend components follow a local linear trend specification:

$$\mu_t = \mu_{t-1} + \tau_{t-1} + \eta_t, \quad \eta_t \sim \mathcal{N}(0, \sigma_\mu^2)$$

$$\tau_t = \tau_{t-1} + \zeta_t, \quad \zeta_t \sim \mathcal{N}(0, \sigma_\tau^2)$$

This formulation allows the trend to evolve smoothly over time while maintaining flexibility in both level and slope dynamics. The seasonal component $\gamma_t$ is typically modeled using periodic structures that ensure the seasonal effects sum to zero over each complete cycle.

#### Extension to Time-Varying Trend Volatility

In many applications, the assumption of constant trend volatility ($\sigma_\tau^2$) proves inadequate. Economic time series, for instance, may exhibit stable growth periods punctuated by high-volatility episodes during recessions or policy changes. To accommodate such dynamics, we extend the standard framework by allowing the volatility of the trend innovation to evolve over time.

We introduce a stochastic volatility process for the trend innovation variance:

$$\log \sigma_{\tau, t}^2 = h_t, \quad h_t = h_{t-1} + \nu_t, \quad \nu_t \sim \mathcal{N}(0, \sigma_h^2)$$

$$\zeta_t \sim \mathcal{N}(0, \exp(h_t))$$

This specification implements a log-random walk for the trend volatility, ensuring positive variance while allowing for gradual or abrupt changes in trend smoothness. The parameter $\sigma_h^2$ controls how quickly the trend volatility can change: larger values permit more rapid volatility shifts, while smaller values enforce smoother volatility evolution.

#### Data Simulation: Building Synthetic Time Series

To demonstrate both modeling approaches, we begin by simulating realistic time series data that incorporates the key components of interest.

- Basic BSTS Simulation

```r
set.seed(42)
n <- 120  # number of time points
m <- 12   # seasonal period

level <- numeric(n)
trend <- numeric(n)
season <- numeric(n)
y <- numeric(n)

# Initialize components
level[1] <- 10
trend[1] <- 0.5
seasonal_pattern <- sin(2 * pi * (1:m) / m)
season[1:m] <- seasonal_pattern

# Generate time series
for (t in 2:n) {
  level[t] <- level[t-1] + trend[t-1] + rnorm(1, 0, 0.3)
  trend[t] <- trend[t-1] + rnorm(1, 0, 0.05)
  season[t] <- season[t %% m + 1]  # repeat seasonal pattern
  y[t] <- level[t] + season[t] + rnorm(1, 0, 1)
}

ts.plot(y, main = "Simulated BSTS Time Series")
```

- Time-Varying Volatility Simulation

For the extended model with stochastic volatility, we modify the simulation to include time-varying trend volatility:

```r
set.seed(123)
n <- 120
m <- 12

level <- numeric(n)
trend <- numeric(n)
h <- numeric(n)
sigma_trend_t <- numeric(n)
season <- numeric(n)
y <- numeric(n)

# Initialize components
level[1] <- 10
trend[1] <- 0.5
h[1] <- log(0.05^2)  # initial log-variance
seasonal_pattern <- sin(2 * pi * (1:m) / m)
season[1:m] <- seasonal_pattern

# Generate time series with time-varying trend volatility
for (t in 2:n) {
  # Update log-volatility
  h[t] <- h[t - 1] + rnorm(1, 0, 0.1)
  sigma_trend_t[t] <- sqrt(exp(h[t]))
  
  # Update components
  trend[t] <- trend[t - 1] + rnorm(1, 0, sigma_trend_t[t])
  level[t] <- level[t - 1] + trend[t - 1] + rnorm(1, 0, 0.3)
  season[t] <- season[t %% m + 1]
  y[t] <- level[t] + season[t] + rnorm(1, 0, 1)
}

ts.plot(y, main = "Simulated Time Series with Time-Varying Trend Volatility")
```

This simulation generates periods of varying trend smoothness, providing a realistic testbed for the extended model.
##  Stan Implementation

### Stan Implementation: Basic BSTS Model

We implement the core BSTS model in Stan, emphasizing clarity and computational efficiency:

```stan
functions {
  vector seasonal_component(vector s, int N, int m) {
    vector[N] seasonal;
    for (t in 1:N) {
      if (t > m)
        seasonal[t] = s[t - m];
      else
        seasonal[t] = s[t];
    }
    return seasonal;
  }
}

data {
  int<lower=1> N;
  int<lower=1> m; // seasonal period
  vector[N] y;
}

parameters {
  real<lower=0> sigma_obs;
  real<lower=0> sigma_level;
  real<lower=0> sigma_trend;
  real<lower=0> sigma_seasonal;
  
  vector[N] level;
  vector[N] trend;
  vector[N] seasonal;
}

model {
  // State evolution equations
  level[2:N] ~ normal(level[1:(N-1)] + trend[1:(N-1)], sigma_level);
  trend[2:N] ~ normal(trend[1:(N-1)], sigma_trend);
  seasonal[(m+1):N] ~ normal(seasonal[1:(N-m)], sigma_seasonal);
  
  // Observation equation
  y ~ normal(level + seasonal_component(seasonal, N, m), sigma_obs);
  
  // Priors
  sigma_obs ~ normal(0, 1);
  sigma_level ~ normal(0, 0.5);
  sigma_trend ~ normal(0, 0.1);
  sigma_seasonal ~ normal(0, 0.5);
}
```

### Stan Implementation: Time-Varying Trend Volatility

The extended model incorporates stochastic volatility into the trend component:

```stan
functions {
  vector seasonal_component(vector s, int N, int m) {
    vector[N] seasonal;
    for (t in 1:N) {
      if (t > m)
        seasonal[t] = s[t - m];
      else
        seasonal[t] = s[t];
    }
    return seasonal;
  }
}

data {
  int<lower=1> N;
  int<lower=1> m;
  vector[N] y;
}

parameters {
  real<lower=0> sigma_obs;
  real<lower=0> sigma_level;
  real<lower=0> sigma_h;

  vector[N] level;
  vector[N] trend;
  vector[N] seasonal;
  vector[N] h; // log variance of trend innovation
}

model {
  // Priors
  sigma_obs ~ normal(0, 1);
  sigma_level ~ normal(0, 0.5);
  sigma_h ~ normal(0, 0.2);
  h[1] ~ normal(log(0.05^2), 0.1);

  // Seasonal component evolution
  seasonal[(m+1):N] ~ normal(seasonal[1:(N-m)], 0.5);

  // Latent state processes
  for (t in 2:N) {
    h[t] ~ normal(h[t-1], sigma_h);
    trend[t] ~ normal(trend[t-1], exp(0.5 * h[t]));
    level[t] ~ normal(level[t-1] + trend[t-1], sigma_level);
  }

  // Observation equation
  y ~ normal(level + seasonal_component(seasonal, N, m), sigma_obs);
}
```

The key innovation lies in the stochastic evolution of `h[t]`, which governs the time-varying volatility of trend innovations through the exponential transformation `exp(0.5 * h[t])`.

- Model Fitting in R with RStan

Both models can be fitted using RStan with similar workflows, differing primarily in their complexity and computational requirements.

- Basic BSTS Model Fitting

```r
library(rstan)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

# Prepare data
data_list_basic <- list(N = length(y), m = m, y = y)

# Fit basic model
fit_basic <- stan(model_code = stan_model_bsts_basic, 
                  data = data_list_basic,
                  chains = 4, iter = 2000, warmup = 1000, seed = 42)

print(fit_basic, pars = c("sigma_obs", "sigma_level", "sigma_trend", "sigma_seasonal"))
```

- Time-Varying Volatility Model Fitting

```r
# Fit extended model
fit_extended <- stan(model_code = stan_model_bsts_extended, 
                     data = data_list_basic,
                     chains = 4, iter = 2000, warmup = 1000, seed = 123)

print(fit_extended, pars = c("sigma_obs", "sigma_level", "sigma_h"))
```

The posterior for `sigma_h` provides insights into volatility dynamics: larger values indicate more variable trend volatility, while smaller values suggest smoother volatility evolution.

- Visualization and Interpretation

Effective visualization of BSTS results requires displaying both point estimates and uncertainty quantification for the latent components.

- Basic Model Visualization

```r
# Extract posterior samples
post_basic <- extract(fit_basic)

# Compute posterior summaries
level_mean <- apply(post_basic$level, 2, mean)
level_lower <- apply(post_basic$level, 2, quantile, probs = 0.025)
level_upper <- apply(post_basic$level, 2, quantile, probs = 0.975)

trend_mean <- apply(post_basic$trend, 2, mean)

library(ggplot2)
df_basic <- data.frame(
  time = 1:n,
  observed = y,
  level = level_mean,
  level_lower = level_lower,
  level_upper = level_upper,
  trend = trend_mean
)

# Plot level component with uncertainty
ggplot(df_basic, aes(x = time)) +
  geom_line(aes(y = observed), color = "black", linetype = "dashed", alpha = 0.7) +
  geom_line(aes(y = level), color = "blue", size = 1) +
  geom_ribbon(aes(ymin = level_lower, ymax = level_upper), 
              fill = "blue", alpha = 0.2) +
  labs(title = "BSTS Level Component with 95% Credible Intervals",
       x = "Time", y = "Value") +
  theme_minimal()

# Plot trend evolution
ggplot(df_basic, aes(x = time)) +
  geom_line(aes(y = trend), color = "red", size = 1) +
  labs(title = "Estimated Trend Component",
       x = "Time", y = "Trend") +
  theme_minimal()
```

- Time-Varying Volatility Visualization

```r
# Extract posterior samples for extended model
post_extended <- extract(fit_extended)

# Compute posterior summaries
h_mean <- apply(post_extended$h, 2, mean)
h_lower <- apply(post_extended$h, 2, quantile, probs = 0.025)
h_upper <- apply(post_extended$h, 2, quantile, probs = 0.975)

trend_mean_ext <- apply(post_extended$trend, 2, mean)

df_extended <- data.frame(
  time = 1:n,
  log_vol = h_mean,
  log_vol_lower = h_lower,
  log_vol_upper = h_upper,
  trend = trend_mean_ext
)

# Plot time-varying log-volatility
ggplot(df_extended, aes(x = time)) +
  geom_line(aes(y = log_vol), color = "red", size = 1) +
  geom_ribbon(aes(ymin = log_vol_lower, ymax = log_vol_upper), 
              fill = "red", alpha = 0.2) +
  labs(title = "Time-Varying Trend Log-Volatility with 95% Credible Intervals",
       y = "log(σ²_τ)", x = "Time") +
  theme_minimal()

# Plot corresponding trend with varying smoothness
ggplot(df_extended, aes(x = time)) +
  geom_line(aes(y = trend), color = "blue", size = 1) +
  labs(title = "Trend Component with Time-Varying Volatility",
       y = "Trend", x = "Time") +
  theme_minimal()
```

These visualizations reveal how uncertainty in trend evolution changes over time, enabling analysts to identify periods of instability or structural transitions.

#### Applications and Extensions

BSTS models find applications across diverse domains where interpretable time series decomposition is valuable. The basic framework handles missing data naturally and provides a foundation for causal impact analysis through counterfactual prediction. The time-varying volatility extension proves particularly valuable in financial econometrics, where volatility clustering and regime changes are common phenomena. In financial time series, periods of market calm alternate with high-volatility episodes during crises or major economic announcements. The time-varying trend volatility model can capture these dynamics while maintaining the interpretable decomposition that BSTS models provide. Economic indicators often exhibit structural breaks and changing volatility regimes due to policy interventions, technological changes, or external shocks. The flexible volatility specification enables more adaptive forecasting in such environments. Climate time series frequently display non-stationary variance patterns due to changing atmospheric or oceanic conditions. The stochastic volatility framework can accommodate such dynamics while preserving seasonal and trend interpretability.


Both modeling frameworks can be extended to hierarchical structures where multiple related time series share common components, or to multivariate settings where cross-series dependencies are of interest. The Stan implementations provide a flexible foundation for such extensions. BSTS models naturally accommodate regression components with spike-and-slab priors for variable selection. This capability enables automatic identification of relevant predictors while maintaining the structural decomposition that makes BSTS models interpretable. The computational complexity of BSTS models scales with the number of time points and latent states. The basic model typically converges within standard MCMC runs, while the time-varying volatility extension requires longer chains due to the additional stochastic volatility component.

Stan's efficient gradient-based sampling (HMC/NUTS) handles the high-dimensional latent state spaces more effectively than traditional Gibbs sampling approaches. However, practitioners should monitor convergence diagnostics carefully, particularly for the volatility parameters in the extended model.

For very long time series, computational efficiency can be improved through state space formulations that marginalize over latent states, though this sacrifices direct access to the latent component estimates that make BSTS models interpretable.

## Model Comparison and Selection

Comparing basic and extended BSTS models requires balancing goodness-of-fit against complexity. The Widely Applicable Information Criterion (WAIC) provides a practical tool for model comparison within the Bayesian framework:

```r
library(loo)

# Compute WAIC for model comparison
waic_basic <- waic(extract_log_lik(fit_basic))
waic_extended <- waic(extract_log_lik(fit_extended))

# Compare models
loo_compare(waic_basic, waic_extended)
```

Additionally, posterior predictive checks help assess whether the added complexity of time-varying volatility improves model adequacy for the specific application.

## Conclusion

Bayesian Structural Time Series models provide a powerful and interpretable framework for analyzing complex temporal data. The basic BSTS specification offers transparent decomposition into level, trend, and seasonal components while maintaining full uncertainty quantification through the Bayesian approach. The extension to time-varying trend volatility addresses limitations of constant volatility assumptions, enabling more adaptive modeling of time series with changing uncertainty regimes.

The Stan implementations presented here provide complete, working examples that practitioners can adapt to their specific applications. The state-space framework naturally handles missing data and irregular observation patterns, while the Bayesian approach enables principled incorporation of prior information and coherent uncertainty propagation.

As computational tools continue to improve and the demand for interpretable time series models grows, BSTS models are positioned to play an increasingly important role across scientific and applied domains. The flexibility demonstrated through the time-varying volatility extension illustrates how the basic framework can be adapted to address specific modeling challenges while preserving the interpretability that makes BSTS models valuable for practical time series analysis.

The unified treatment presented here equips practitioners with both foundational understanding and practical tools for implementing BSTS models across a spectrum of applications, from basic seasonal decomposition to advanced volatility modeling in dynamic environments.

<!--chapter:end:14-BSTS.Rmd-->

# Model Comparison and Selection in Bayesian Analysis

## Introduction

Bayesian statistical modeling offers a flexible and principled approach to quantifying uncertainty and incorporating prior knowledge. As readers gain familiarity with building a variety of models—ranging from simple linear regressions to complex hierarchical and non-parametric models—a critical question emerges: how do we choose among competing models? Bayesian model comparison and selection are fundamental steps in the modeling process, ensuring that inferences are both robust and interpretable.

Unlike frequentist paradigms, Bayesian methods provide a coherent framework for comparing models by evaluating the plausibility of the data given the model, known as model evidence. However, this task is often complicated by practical challenges in estimating model evidence and balancing predictive accuracy with model complexity. This article explores the key tools available for Bayesian model comparison, including posterior predictive checks, information criteria like WAIC and LOO-CV, and Bayes factors. Each method offers unique insights, and their strengths and limitations make them suitable for different analytical contexts.

While selecting a single model is a common goal, Bayesian modeling also allows for **model averaging** and **stacking**, approaches that combine models to account for model uncertainty and improve predictive robustness. These alternatives are especially useful when no model clearly dominates or when combining perspectives from multiple models provides more comprehensive insights.

## Posterior Predictive Checks

Posterior predictive checks are a fundamental diagnostic tool used to assess the fit of a Bayesian model to the observed data. The idea is to simulate data from the posterior predictive distribution and compare it to the actual observed data. Discrepancies between the simulated and observed data can indicate model misfit or structural inadequacies.

Formally, the posterior predictive distribution is given by:

$p(\tilde{y} \mid y) = \int p(\tilde{y} \mid \theta) p(\theta \mid y) d\theta$

where $\tilde{y}$ represents future or replicated data, $y$ is the observed data, and $\theta$ denotes the parameters.

Visualization tools such as histograms, density plots, and test statistics (e.g., discrepancy measures) are commonly used to perform these checks. In practice, packages like `bayesplot` in R make it easy to implement these diagnostics. Although posterior predictive checks are excellent for identifying model fit issues, they do not provide a direct basis for comparing multiple models.

**Transition:** While posterior predictive checks help diagnose model adequacy, they are less informative for ranking competing models. For this, we turn to information-theoretic approaches like WAIC and LOO-CV.

## Information Criteria: WAIC and LOO-CV

Two widely used Bayesian model comparison tools are the Watanabe-Akaike Information Criterion (WAIC) and Leave-One-Out Cross-Validation (LOO-CV). Both aim to estimate a model's out-of-sample predictive performance.

### WAIC (Watanabe-Akaike Information Criterion)

WAIC is a fully Bayesian criterion that estimates out-of-sample predictive accuracy while penalizing for model complexity. It is computed as:

$\text{WAIC} = -2(\text{lppd} - p_{\text{WAIC}})$

where lppd is the log pointwise predictive density and $p_{\text{WAIC}}$ is the effective number of parameters, which quantifies the flexibility of the posterior distribution in fitting the data. WAIC is asymptotically equivalent to Bayesian cross-validation and is computable directly from posterior samples.

Unlike classical AIC or BIC, which penalize complexity by parameter count, WAIC adjusts based on how flexible the posterior is in fitting the data. This leads to more nuanced complexity penalties, especially in hierarchical models where the number of effective parameters can be non-integer and context-sensitive.

### LOO-CV (Leave-One-Out Cross-Validation)

LOO-CV involves fitting the model repeatedly, leaving out one observation at a time and evaluating predictive performance on the omitted data. To avoid the computational burden of refitting the model $n$ times, Pareto-smoothed importance sampling (PSIS) offers an efficient approximation, implemented in the `loo` R package.

The `loo` package also provides diagnostic tools, such as Pareto $k$ values, to assess the reliability of importance sampling. Typically, values below 0.7 suggest stable estimates; higher values may indicate influential observations that require model reassessment.

Additionally, LOO-CV provides a natural framework for identifying influential data points. Observations with high $k$ values may point to model misspecification, data entry errors, or unaccounted-for heterogeneity. Thus, beyond model ranking, LOO-CV offers an avenue for deepening model diagnostics.

Both WAIC and LOO-CV yield estimates of expected out-of-sample deviance. Lower values indicate better predictive accuracy.

**Transition:** While WAIC and LOO-CV emphasize predictive performance, Bayes factors offer an alternative grounded in model plausibility and marginal likelihood.

## Bayes Factors vs. Information Criteria

Bayes factors provide a coherent Bayesian approach to model comparison by evaluating how well each model explains the observed data, integrating over all parameter values:

$BF_{12} = \frac{p(y \mid M_1)}{p(y \mid M_2)}$

A Bayes factor greater than 1 indicates support for model $M_1$ over $M_2$, and vice versa. Interpretation guidelines (Kass & Raftery, 1995) suggest:

* 1 to 3: Anecdotal evidence
* 3 to 10: Moderate evidence
* > 10: Strong evidence

Bayes factors incorporate prior model probabilities and are grounded in decision theory. However, they are often sensitive to the choice of priors and can be computationally intensive due to the need to estimate marginal likelihoods.

Despite their theoretical appeal, Bayes factors suffer from two key limitations. First, the *sensitivity to prior distributions* — especially priors on nuisance parameters — can lead to unstable or unintuitive conclusions. Second, the *marginal likelihood* integrates over the entire parameter space, potentially over-penalizing complex models when priors are diffuse. This makes Bayes factors ill-suited to models with weak or uninformative priors unless priors are carefully calibrated.

A useful computational shortcut in nested models is the **Savage-Dickey density ratio**, which allows Bayes factor computation via prior and posterior densities of a parameter of interest.

In contrast, WAIC and LOO-CV focus on predictive accuracy and are generally more robust to prior specification, making them preferable in prediction-oriented tasks.

## Summary of Bayesian Model Comparison Methods

| Method               | Goal                           | Sensitive to Priors | Penalizes Complexity | Emphasis                 | Tools (R Packages)              | Use Case                               |
| -------------------- | ------------------------------ | ------------------- | -------------------- | ------------------------ | ------------------------------- | -------------------------------------- |
| Posterior Predictive | Assess model fit               | No                  | No                   | Model adequacy           | `bayesplot`, `rstanarm`         | Diagnosing misfit or model structure   |
| WAIC                 | Estimate predictive accuracy   | Mildly              | Yes                  | Out-of-sample prediction | `loo`, `rstanarm`               | Model ranking for prediction           |
| LOO-CV (PSIS)        | Estimate predictive accuracy   | Mildly              | Yes                  | Out-of-sample prediction | `loo`, `rstanarm`               | Robust prediction comparison           |
| Bayes Factors        | Marginal likelihood comparison | Yes                 | Implicit             | Model evidence           | `bridgesampling`, `BayesFactor` | Hypothesis testing, model plausibility |

## Practical Example: Comparing Models in R

To illustrate these concepts, consider the following example using a synthetic dataset. We compare two models: a simple linear regression and a polynomial regression.

```r
library(rstanarm)
library(loo)
library(bayesplot)

# Simulate data
set.seed(123)
x <- rnorm(100)
y <- 2 * x + x^2 + rnorm(100)
data <- data.frame(x = x, y = y)

# Fit linear and quadratic models
fit_linear <- stan_glm(y ~ x, data = data, refresh = 0)
fit_quad <- stan_glm(y ~ x + I(x^2), data = data, refresh = 0)

# Posterior predictive checks
pp_check(fit_linear)
pp_check(fit_quad)

# WAIC comparison
waic_linear <- waic(fit_linear)
waic_quad <- waic(fit_quad)
print(waic_linear)
print(waic_quad)

# LOO-CV comparison
loo_linear <- loo(fit_linear)
loo_quad <- loo(fit_quad)
print(loo_compare(loo_linear, loo_quad))
```

```
# Bayes factor using bridgesampling

library(rstanarm)
library(bridgesampling)

# Fit model with diagnostic_file argument
fit_linear <- stan_glm(
  y ~ x, 
  data = data, 
  refresh = 0,
  diagnostic_file = file.path(tempdir(), "linear_diag.csv")
)

fit_quad <- stan_glm(
  y ~ x + I(x^2), 
  data = data, 
  refresh = 0,
  diagnostic_file = file.path(tempdir(), "quad_diag.csv")
)

# Run bridge sampling
bridge_linear <- bridge_sampler(fit_linear)
bridge_quad <- bridge_sampler(fit_quad)

# Compute Bayes factor
bf_result <- bf(bridge_quad, bridge_linear)
print(bf_result)

```

In situations where neither model clearly dominates or when both offer complementary insights, Bayesian stacking or model averaging may be employed to combine predictive strengths rather than choosing a single model. The `loo` package also supports stacking weights, enabling ensemble predictive performance evaluation. No single method is universally superior; the choice depends on the modeling context, computational resources, and analytic goals. In some cases, model averaging or stacking may offer additional robustness when no single model clearly dominates. Posterior predictive checks serve as intuitive diagnostics for model adequacy, while information criteria like WAIC and LOO-CV offer principled approaches for evaluating predictive performance. Bayes factors provide an alternative rooted in model evidence, though they require careful prior specification and computational resources. By incorporating these tools into their workflow, practitioners can enhance the credibility, interpretability, and predictive utility of their Bayesian analyses. A reasonable workflow might begin with posterior predictive checks to assess absolute model fit, followed by WAIC or LOO-CV to evaluate predictive utility. Bayes factors can be used where prior calibration is strong and the goal is hypothesis testing. When model choice is ambiguous, averaging via stacking provides a principled compromise. Ultimately, careful model comparison is as much about understanding the modeling context as it is about applying formulas.

<!--chapter:end:Appendix-II.Rmd-->


# Appendix

## Main Stan Distributions Cheatsheet

Statistical modeling in **Stan** is powered by a flexible and expressive probabilistic language grounded in **log-density functions**. While the modeling blocks (`model`, `data`, `parameters`, etc.) help structure a model, the core statistical logic is defined through **distributions**. This cheatsheet offers a practical summary of the most important distributions used in Stan, their syntax, required parameters, typical use cases, and examples of where they show up in statistical modeling.

---

| **Distribution**      | **Function**                  | **Parameters** | **Use Case**                            | **Model Type(s)**                      |                                                         |
| --------------------- | ----------------------------- | -------------- | --------------------------------------- | -------------------------------------- | ------------------------------------------------------- |
| **Bernoulli**         | \`bernoulli\_lpmf(y           | θ)\`           | `θ ∈ (0, 1)`                            | Binary outcome (0/1)                   | Logistic regression, classification                     |
| **Binomial**          | \`binomial\_lpmf(y            | n, θ)\`        | `n ∈ ℕ⁺`, `θ ∈ (0, 1)`                  | # of successes in `n` trials           | Logistic GLMs, grouped binomial models                  |
| **Categorical**       | \`categorical\_lpmf(y         | θ)\`           | `θ`: simplex vector (length K)          | Single draw from K categories          | Multinomial regression                                  |
| **Multinomial**       | \`multinomial\_lpmf(y         | θ)\`           | `y`: int vector of counts, `θ`: simplex | Category count data                    | Count models with category splits                       |
| **Normal**            | \`normal\_lpdf(y              | μ, σ)\`        | `μ ∈ ℝ`, `σ > 0`                        | Gaussian noise, residuals              | Linear regression, priors for real parameters           |
| **Student's t**       | \`student\_t\_lpdf(y          | ν, μ, σ)\`     | `ν > 0`, `μ ∈ ℝ`, `σ > 0`               | Heavy-tailed data, robust models       | Robust regression, hierarchical priors                  |
| **Cauchy**            | \`cauchy\_lpdf(y              | μ, σ)\`        | `μ ∈ ℝ`, `σ > 0`                        | Weakly informative, heavy-tailed prior | Priors on scale parameters (e.g., `τ ~ cauchy(0, 2.5)`) |
| **Exponential**       | \`exponential\_lpdf(y         | λ)\`           | `λ > 0`                                 | Time to event, memoryless processes    | Survival models, Poisson process modeling               |
| **Gamma**             | \`gamma\_lpdf(y               | α, β)\`        | `α > 0`, `β > 0`                        | Positive skewed data                   | Priors on rates or shape parameters                     |
| **Inverse Gamma**     | \`inv\_gamma\_lpdf(y          | α, β)\`        | `α > 0`, `β > 0`                        | Prior for variances                    | Priors on `σ²`, `τ²`, especially in hierarchies         |
| **Lognormal**         | \`lognormal\_lpdf(y           | μ, σ)\`        | `μ ∈ ℝ`, `σ > 0`                        | Positive, right-skewed data            | Income, durations, reliability                          |
| **Beta**              | \`beta\_lpdf(y                | α, β)\`        | `α > 0`, `β > 0`                        | Probabilities or proportions           | Priors on probabilities (`θ ∈ (0, 1)`)                  |
| **Dirichlet**         | \`dirichlet\_lpdf(θ           | α)\`           | `θ`: simplex, `α > 0` vector            | Probabilities summing to 1             | Priors for category proportions, LDA                    |
| **Poisson**           | \`poisson\_lpmf(y             | λ)\`           | `λ > 0`                                 | Count data, rare event modeling        | GLMs for count data                                     |
| **Negative Binomial** | \`neg\_binomial\_2\_lpmf(y    | μ, φ)\`        | `μ > 0`, `φ > 0`                        | Overdispersed count data               | GLMs with extra-Poisson variation                       |
| **Ordered Logistic**  | \`ordered\_logistic\_lpmf(y   | η, c)\`        | `η ∈ ℝ`, `c`: ordered cut-points        | Ordinal outcomes                       | Ordinal regression                                      |
| **Uniform**           | \`uniform\_lpdf(y             | a, b)\`        | `a < b`                                 | Flat prior within range                | Non-informative priors                                  |
| **Pareto**            | \`pareto\_lpdf(y              | y\_min, α)\`   | `y_min > 0`, `α > 0`                    | Heavy-tail data, power-law phenomena   | Extremes, outlier modeling                              |
| **Von Mises**         | \`von\_mises\_lpdf(y          | μ, κ)\`        | `μ ∈ [0, 2π)`, `κ ≥ 0`                  | Circular data (angles, wind direction) | Directional models                                      |
| **Weibull**           | \`weibull\_lpdf(y             | α, σ)\`        | `α, σ > 0`                              | Survival times, failure rates          | Survival models, reliability analysis                   |
| **LKJ Correlation**   | \`lkj\_corr\_cholesky\_lpdf(L | η)\`           | `η > 0`, `L`: Cholesky factor           | Prior for correlation matrices         | Hierarchical models with random slopes                  |
| **Wishart**           | \`wishart\_lpdf(S             | ν, Σ)\`        | `ν > dim-1`, `Σ`: scale matrix          | Prior on covariance matrices           | Multivariate Gaussian models (rarely used)              |




## Main Stan Functions Cheatsheet

Stan is a robust platform for Bayesian statistical modeling, renowned for its Hamiltonian Monte Carlo (HMC) engine and flexible modeling language. While probability distributions like `normal_lpdf` or `poisson_lpmf` define priors and likelihoods, Stan’s non-distribution functions—spanning mathematical operations, matrix algebra, utility tools, and specialized solvers—are equally critical for building efficient and expressive models. These functions enable data transformations, efficient computations, and post-processing in the `generated quantities` block.

This cheatsheet organizes Stan’s most commonly used non-distribution functions into categories, providing their purpose, example usage, and the model types where they’re most applicable. Whether you’re crafting linear regressions, hierarchical models, or dynamic systems, this guide will help you leverage Stan’s toolkit effectively. We’ll wrap up with an example model to bring these functions to life.

## Why Non-Distribution Functions?
Stan’s non-distribution functions serve several key purposes:
- **Transformations**: Functions like `log`, `exp`, and `inv_logit` map parameters to constrained spaces or perform nonlinear calculations.
- **Matrix Operations**: Functions like `dot_product` and `cholesky_decompose` enable efficient linear algebra for multivariate models.
- **Utilities**: Functions like `to_vector` and `mean` simplify data manipulation and posterior summaries.
- **Specialized Tools**: Solvers like `ode_rk45` and `integrate_1d` tackle complex systems, such as differential equations or custom likelihoods.
- **Posterior Processing**: Functions in the `generated quantities` block, like `sum` or `sd`, compute diagnostics or predictions.

This cheatsheet focuses on these functions to help you streamline model specification and analysis.

## Stan Functions Cheatsheet

### 1. Mathematical Functions
These functions perform scalar operations, often used in `transformed parameters` or `model` blocks.

| **Function**      | **Purpose**                     | **Example Usage**              | **Model Type(s)**                     |
|-------------------|---------------------------------|-------------------------------|---------------------------------------|
| `abs(x)`          | Absolute value                  | `real z = abs(x);`            | General computations, robust stats    |
| `exp(x)`          | Exponential (e^x)              | `lambda = exp(alpha);`        | Rate models, transformations          |
| `log(x)`          | Natural logarithm              | `real l = log(y);`            | Log-likelihoods, transformations     |
| `sqrt(x)`         | Square root                    | `sigma = sqrt(variance);`     | Variance computations, scaling       |
| `lgamma(x)`       | Log gamma function             | `lp += lgamma(alpha);`        | Mixture models, custom likelihoods   |
| `log_sum_exp(x)`  | Log-sum-exp for numerical stability | `lp = log_sum_exp(log_theta);` | Mixture models, marginal likelihoods |

### 2. Transformation Functions
These map parameters to constrained spaces, often in `transformed parameters`.

| **Function**      | **Purpose**                       | **Example Usage**                     | **Model Type(s)**                     |
|-------------------|-----------------------------------|--------------------------------------|---------------------------------------|
| `inv_logit(x)`    | Logistic sigmoid (ℝ → (0,1))      | `theta = inv_logit(alpha + beta*x);` | Logistic regression, probability models |
| `logit(p)`        | Log-odds ((0,1) → ℝ)              | `eta = logit(p);`                    | Logistic regression, probit models    |
| `softmax(x)`      | Normalize vector to simplex        | `theta = softmax(alpha);`            | Multinomial regression, LDA           |
| `inv(x)`          | Reciprocal (1/x)                  | `inv_sigma = inv(sigma);`            | Variance transformations             |

### 3. Matrix and Vector Operations
These enable efficient linear algebra, critical for multivariate and hierarchical models.

| **Function**                           | **Purpose**                               | **Example Usage**                                     | **Model Type(s)**                     |
|----------------------------------------|-------------------------------------------|----------------------------------------------|---------------------------------------|
| `dot_product(a, b)`                    | Inner product of two vectors              | `real z = dot_product(a, b);`                | Linear regression, similarity measures |
| `matrix_times_vector(A, v)`            | Matrix-vector multiplication              | `eta = matrix_times_vector(X, beta);`        | Multivariate regression, GLMs         |
| `cholesky_decompose(S)`                 | Cholesky factorization                    | `L = cholesky_decompose(Sigma);`             | Hierarchical models, multivariate normals |
| `multiply_lower_tri_self_transpose(L)` | Covariance from Cholesky factor           | `Sigma = multiply_lower_tri_self_transpose(L);` | Multivariate normals, hierarchical models |
| `diag_matrix(v)`                       | Diagonal matrix from vector               | `M = diag_matrix(v);`                        | Covariance priors, scaling            |
| `determinant(A)`                       | Matrix determinant                        | `det = determinant(Sigma);`                  | Model diagnostics, multivariate priors |

### 4. Utility Functions
These simplify data manipulation and posterior summaries, often in `generated quantities`.

| **Function**      | **Purpose**                     | **Example Usage**                     | **Model Type(s)**                     |
|-------------------|---------------------------------|------------------------------|---------------------------------------|
| `to_vector(x)`    | Convert matrix/array to vector   | `vec = to_vector(matrix);`           | Posterior summaries, data reshaping   |
| `to_array_1d(x)`  | Convert to 1D array             | `arr = to_array_1d(matrix);`         | Data preprocessing, summaries         |
| `sum(x)`          | Sum of elements                 | `total = sum(y);`                    | Aggregations, diagnostics            |
| `mean(x)`         | Mean of elements                | `avg = mean(y_rep);`                 | Posterior summaries, diagnostics      |
| `sd(x)`           | Standard deviation              | `std = sd(y_rep);`                   | Posterior summaries, diagnostics      |
| `int_step(x)`     | Indicator (x ≥ 0 → 1, else 0)   | `flag = int_step(x - 1);`            | Conditional logic, model diagnostics  |

### 5. Specialized Solvers
These handle advanced computations like differential equations or parallel processing.

| **Function**                     | **Purpose**                          | **Example Usage**                                    | **Model Type(s)**                     |
|----------------------------------|--------------------------------------|---------------------------------------------|---------------------------------------|
| `ode_rk45(fun, y0, t0, ts, ...)`| Solve ODEs (Runge-Kutta 45)         | `y = ode_rk45(ode_sys, y0, t0, ts, params);` | Dynamic systems, pharmacokinetics     |
| `integrate_1d(f, a, b, ...)`    | Numerical integration                | `val = integrate_1d(f, a, b, params);`       | Custom likelihoods, marginalization   |
| `map_rect(f, phi, ...)`          | Parallel computation over data shards | `results = map_rect(f, phi, theta, data);`   | Large-scale hierarchical models       |

## Example: Hierarchical Linear Regression
Here’s a Stan model for a hierarchical linear regression, using `matrix_times_vector`, `to_vector`, and `mean` to demonstrate practical function usage:

```stan
data {
  int<lower=0> N; // Number of observations
  int<lower=0> J; // Number of groups
  array[N] int<lower=1,upper=J> group; // Group indicators
  matrix[N, 2] X; // Design matrix (intercept + predictor)
  vector[N] y; // Outcome
}
parameters {
  vector[2] beta; // Fixed effects
  vector[J] alpha; // Group-level intercepts
  real<lower=0> sigma; // Residual standard deviation
  real<lower=0> tau; // Standard deviation of group intercepts
}
model {
  beta ~ normal(0, 5); // Prior on fixed effects
  tau ~ cauchy(0, 2.5); // Prior on group SD
  alpha ~ normal(0, tau); // Group-level priors
  sigma ~ cauchy(0, 2.5); // Prior on residual SD
  vector[N] mu = matrix_times_vector(X, beta) + to_vector(alpha[group]);
  y ~ normal(mu, sigma); // Likelihood
}
generated quantities {
  vector[N] y_rep; // Posterior predictive
  real mean_y_rep; // Mean of predictions
  for (n in 1:N) {
    y_rep[n] = normal_rng(matrix_times_vector(X[n], beta) + alpha[group[n]], sigma);
  }
  mean_y_rep = mean(to_vector(y_rep)); // Summary statistic
}
```

This model:
- Uses `matrix_times_vector` to compute the linear predictor efficiently.
- Employs `to_vector` to align group-level intercepts with observations.
- Computes `mean_y_rep` in `generated quantities` using `mean` and `to_vector` for posterior diagnostics.
- Generates predictions with `normal_rng` for posterior predictive checks.

## Tips for Using Stan Functions
1. **Efficiency**: Prefer vectorized operations like `matrix_times_vector` over loops for speed.
2. **Numerical Stability**: Use `log_sum_exp` for summing exponentials to avoid overflow.
3. **Posterior Analysis**: Leverage `mean`, `sd`, and `to_vector` in `generated quantities` for summaries and diagnostics.
4. **Constraints**: Ensure inputs meet requirements (e.g., `x > 0` for `log`, positive-definite matrices for `cholesky_decompose`).
5. **Advanced Modeling**: Use `ode_rk45` for dynamic systems or `map_rect` for parallelized large-scale models.
6. **Documentation**: The [Stan Reference Manual](https://mc-stan.org/docs/) (e.g., version 2.33) and Stan’s GitHub examples provide detailed guidance.

<!--chapter:end:Appendix.Rmd-->

