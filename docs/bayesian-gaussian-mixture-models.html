<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Bayesian Gaussian Mixture Models | Stan Bookdown</title>
  <meta name="description" content="Chapter 9 Bayesian Gaussian Mixture Models | Stan Bookdown" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Bayesian Gaussian Mixture Models | Stan Bookdown" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Bayesian Gaussian Mixture Models | Stan Bookdown" />
  
  
  

<meta name="author" content="Kamran Afzali" />


<meta name="date" content="2025-08-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="gaussian-process-regression-gpr.html"/>
<link rel="next" href="bayesian-canonical-correlation-analysis-in-stan.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Stan</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Choosing the Right Bayesian Model</a></li>
<li class="chapter" data-level="2" data-path="bayesian-modeling-in-r-and-stan.html"><a href="bayesian-modeling-in-r-and-stan.html"><i class="fa fa-check"></i><b>2</b> Bayesian Modeling in R and Stan</a>
<ul>
<li class="chapter" data-level="2.1" data-path="bayesian-modeling-in-r-and-stan.html"><a href="bayesian-modeling-in-r-and-stan.html#what-is-stan"><i class="fa fa-check"></i><b>2.1</b> What is <code>stan</code>?</a></li>
<li class="chapter" data-level="2.2" data-path="bayesian-modeling-in-r-and-stan.html"><a href="bayesian-modeling-in-r-and-stan.html#model-file"><i class="fa fa-check"></i><b>2.2</b> Model file</a></li>
<li class="chapter" data-level="2.3" data-path="bayesian-modeling-in-r-and-stan.html"><a href="bayesian-modeling-in-r-and-stan.html#fit-the-model"><i class="fa fa-check"></i><b>2.3</b> Fit the model</a></li>
<li class="chapter" data-level="2.4" data-path="bayesian-modeling-in-r-and-stan.html"><a href="bayesian-modeling-in-r-and-stan.html#mcmc-diagnostics"><i class="fa fa-check"></i><b>2.4</b> MCMC diagnostics</a></li>
<li class="chapter" data-level="2.5" data-path="bayesian-modeling-in-r-and-stan.html"><a href="bayesian-modeling-in-r-and-stan.html#conclusions"><i class="fa fa-check"></i><b>2.5</b> Conclusions</a></li>
<li class="chapter" data-level="2.6" data-path="bayesian-modeling-in-r-and-stan.html"><a href="bayesian-modeling-in-r-and-stan.html#references"><i class="fa fa-check"></i><b>2.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayesian-regression-models-for-non-normal-data.html"><a href="bayesian-regression-models-for-non-normal-data.html"><i class="fa fa-check"></i><b>3</b> Bayesian Regression Models for Non-Normal Data</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bayesian-regression-models-for-non-normal-data.html"><a href="bayesian-regression-models-for-non-normal-data.html#logistic-regression"><i class="fa fa-check"></i><b>3.1</b> Logistic Regression</a></li>
<li class="chapter" data-level="3.2" data-path="bayesian-regression-models-for-non-normal-data.html"><a href="bayesian-regression-models-for-non-normal-data.html#negative-binomial"><i class="fa fa-check"></i><b>3.2</b> Negative Binomial</a></li>
<li class="chapter" data-level="3.3" data-path="bayesian-regression-models-for-non-normal-data.html"><a href="bayesian-regression-models-for-non-normal-data.html#conclusion"><i class="fa fa-check"></i><b>3.3</b> Conclusion</a></li>
<li class="chapter" data-level="3.4" data-path="bayesian-regression-models-for-non-normal-data.html"><a href="bayesian-regression-models-for-non-normal-data.html#references-1"><i class="fa fa-check"></i><b>3.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="robust-t-regression.html"><a href="robust-t-regression.html"><i class="fa fa-check"></i><b>4</b> Robust t-regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="robust-t-regression.html"><a href="robust-t-regression.html#motivation"><i class="fa fa-check"></i><b>4.1</b> Motivation</a></li>
<li class="chapter" data-level="4.2" data-path="robust-t-regression.html"><a href="robust-t-regression.html#concepts-and-code"><i class="fa fa-check"></i><b>4.2</b> Concepts and code</a></li>
<li class="chapter" data-level="4.3" data-path="robust-t-regression.html"><a href="robust-t-regression.html#conclusion-1"><i class="fa fa-check"></i><b>4.3</b> Conclusion</a></li>
<li class="chapter" data-level="4.4" data-path="robust-t-regression.html"><a href="robust-t-regression.html#references-2"><i class="fa fa-check"></i><b>4.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html"><i class="fa fa-check"></i><b>5</b> Bayesian Regularized Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#ridge-regression-and-gaussian-priors"><i class="fa fa-check"></i><b>5.2</b> Ridge Regression and Gaussian Priors</a></li>
<li class="chapter" data-level="5.3" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#lasso-regression-and-laplace-priors"><i class="fa fa-check"></i><b>5.3</b> LASSO Regression and Laplace Priors</a></li>
<li class="chapter" data-level="5.4" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#hierarchical-shrinkage-and-adaptive-priors"><i class="fa fa-check"></i><b>5.4</b> Hierarchical Shrinkage and Adaptive Priors</a></li>
<li class="chapter" data-level="5.5" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#implementation"><i class="fa fa-check"></i><b>5.5</b> Implementation</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#bayesian-ridge-regression"><i class="fa fa-check"></i><b>5.5.1</b> Bayesian Ridge Regression</a></li>
<li class="chapter" data-level="5.5.2" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#bayesian-lasso-regression"><i class="fa fa-check"></i><b>5.5.2</b> Bayesian LASSO Regression</a></li>
<li class="chapter" data-level="5.5.3" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#hierarchical-shrinkage-horseshoe-prior"><i class="fa fa-check"></i><b>5.5.3</b> Hierarchical Shrinkage (Horseshoe Prior)</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#empirical-comparison-and-model-evaluation"><i class="fa fa-check"></i><b>5.6</b> Empirical Comparison and Model Evaluation</a></li>
<li class="chapter" data-level="5.7" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#understanding-posterior-behavior"><i class="fa fa-check"></i><b>5.7</b> Understanding Posterior Behavior</a></li>
<li class="chapter" data-level="5.8" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#choosing-among-regularization-methods"><i class="fa fa-check"></i><b>5.8</b> Choosing Among Regularization Methods</a></li>
<li class="chapter" data-level="5.9" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#conclusion-2"><i class="fa fa-check"></i><b>5.9</b> Conclusion</a></li>
<li class="chapter" data-level="5.10" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#references-3"><i class="fa fa-check"></i><b>5.10</b> References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bayesian-quantile-regression.html"><a href="bayesian-quantile-regression.html"><i class="fa fa-check"></i><b>6</b> Bayesian Quantile Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="bayesian-quantile-regression.html"><a href="bayesian-quantile-regression.html#model-specification"><i class="fa fa-check"></i><b>6.1</b> Model Specification</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="bayesian-quantile-regression.html"><a href="bayesian-quantile-regression.html#using-bayesqr-package"><i class="fa fa-check"></i><b>6.1.1</b> Using bayesQR package</a></li>
<li class="chapter" data-level="6.1.2" data-path="bayesian-quantile-regression.html"><a href="bayesian-quantile-regression.html#using-brms-package"><i class="fa fa-check"></i><b>6.1.2</b> Using brms package</a></li>
<li class="chapter" data-level="6.1.3" data-path="bayesian-quantile-regression.html"><a href="bayesian-quantile-regression.html#backend-stan-model"><i class="fa fa-check"></i><b>6.1.3</b> Backend stan model</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="bayesian-quantile-regression.html"><a href="bayesian-quantile-regression.html#references-4"><i class="fa fa-check"></i><b>6.2</b> References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian-multilevel-mixed-effects-regression-in-stan.html"><a href="bayesian-multilevel-mixed-effects-regression-in-stan.html"><i class="fa fa-check"></i><b>7</b> Bayesian Multilevel (Mixed Effects) Regression in Stan</a>
<ul>
<li class="chapter" data-level="7.1" data-path="bayesian-multilevel-mixed-effects-regression-in-stan.html"><a href="bayesian-multilevel-mixed-effects-regression-in-stan.html#introduction-1"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="bayesian-multilevel-mixed-effects-regression-in-stan.html"><a href="bayesian-multilevel-mixed-effects-regression-in-stan.html#multilevel-regression"><i class="fa fa-check"></i><b>7.2</b> Multilevel Regression</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="bayesian-multilevel-mixed-effects-regression-in-stan.html"><a href="bayesian-multilevel-mixed-effects-regression-in-stan.html#bayesian-random-intercepts-model"><i class="fa fa-check"></i><b>7.2.1</b> Bayesian Random Intercepts Model</a></li>
<li class="chapter" data-level="7.2.2" data-path="bayesian-multilevel-mixed-effects-regression-in-stan.html"><a href="bayesian-multilevel-mixed-effects-regression-in-stan.html#bayesian-random-slopes-model"><i class="fa fa-check"></i><b>7.2.2</b> Bayesian Random Slopes Model</a></li>
<li class="chapter" data-level="7.2.3" data-path="bayesian-multilevel-mixed-effects-regression-in-stan.html"><a href="bayesian-multilevel-mixed-effects-regression-in-stan.html#hierarchical-priors-model"><i class="fa fa-check"></i><b>7.2.3</b> Hierarchical Priors Model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="bayesian-multilevel-mixed-effects-regression-in-stan.html"><a href="bayesian-multilevel-mixed-effects-regression-in-stan.html#conclusion-3"><i class="fa fa-check"></i><b>7.3</b> Conclusion</a></li>
<li class="chapter" data-level="7.4" data-path="bayesian-multilevel-mixed-effects-regression-in-stan.html"><a href="bayesian-multilevel-mixed-effects-regression-in-stan.html#references-5"><i class="fa fa-check"></i><b>7.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="gaussian-process-regression-gpr.html"><a href="gaussian-process-regression-gpr.html"><i class="fa fa-check"></i><b>8</b> Gaussian Process Regression (GPR)</a>
<ul>
<li class="chapter" data-level="8.1" data-path="gaussian-process-regression-gpr.html"><a href="gaussian-process-regression-gpr.html#introduction-2"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="gaussian-process-regression-gpr.html"><a href="gaussian-process-regression-gpr.html#challenges"><i class="fa fa-check"></i><b>8.2</b> Challenges</a></li>
<li class="chapter" data-level="8.3" data-path="gaussian-process-regression-gpr.html"><a href="gaussian-process-regression-gpr.html#gpfit-package"><i class="fa fa-check"></i><b>8.3</b> GPfit package</a></li>
<li class="chapter" data-level="8.4" data-path="gaussian-process-regression-gpr.html"><a href="gaussian-process-regression-gpr.html#bayesian-stan"><i class="fa fa-check"></i><b>8.4</b> Bayesian Stan</a></li>
<li class="chapter" data-level="8.5" data-path="gaussian-process-regression-gpr.html"><a href="gaussian-process-regression-gpr.html#references-6"><i class="fa fa-check"></i><b>8.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="bayesian-gaussian-mixture-models.html"><a href="bayesian-gaussian-mixture-models.html"><i class="fa fa-check"></i><b>9</b> Bayesian Gaussian Mixture Models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="bayesian-gaussian-mixture-models.html"><a href="bayesian-gaussian-mixture-models.html#single-varaible-example"><i class="fa fa-check"></i><b>9.1</b> Single varaible example</a></li>
<li class="chapter" data-level="9.2" data-path="bayesian-gaussian-mixture-models.html"><a href="bayesian-gaussian-mixture-models.html#example-with-multiple-variable"><i class="fa fa-check"></i><b>9.2</b> Example with multiple variable</a></li>
<li class="chapter" data-level="9.3" data-path="bayesian-gaussian-mixture-models.html"><a href="bayesian-gaussian-mixture-models.html#conclusion-4"><i class="fa fa-check"></i><b>9.3</b> Conclusion</a></li>
<li class="chapter" data-level="9.4" data-path="bayesian-gaussian-mixture-models.html"><a href="bayesian-gaussian-mixture-models.html#references-7"><i class="fa fa-check"></i><b>9.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bayesian-canonical-correlation-analysis-in-stan.html"><a href="bayesian-canonical-correlation-analysis-in-stan.html"><i class="fa fa-check"></i><b>10</b> Bayesian Canonical Correlation Analysis in Stan</a>
<ul>
<li class="chapter" data-level="10.1" data-path="bayesian-canonical-correlation-analysis-in-stan.html"><a href="bayesian-canonical-correlation-analysis-in-stan.html#introduction-3"><i class="fa fa-check"></i><b>10.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="bayesian-canonical-correlation-analysis-in-stan.html"><a href="bayesian-canonical-correlation-analysis-in-stan.html#canonical-correlation-analysis"><i class="fa fa-check"></i><b>10.1.1</b> Canonical Correlation Analysis</a></li>
<li class="chapter" data-level="10.1.2" data-path="bayesian-canonical-correlation-analysis-in-stan.html"><a href="bayesian-canonical-correlation-analysis-in-stan.html#stan-model"><i class="fa fa-check"></i><b>10.1.2</b> Stan Model</a></li>
<li class="chapter" data-level="10.1.3" data-path="bayesian-canonical-correlation-analysis-in-stan.html"><a href="bayesian-canonical-correlation-analysis-in-stan.html#model-validation-on-test-set"><i class="fa fa-check"></i><b>10.1.3</b> Model validation on test set</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="bayesian-canonical-correlation-analysis-in-stan.html"><a href="bayesian-canonical-correlation-analysis-in-stan.html#conclusion-5"><i class="fa fa-check"></i><b>10.2</b> Conclusion</a></li>
<li class="chapter" data-level="10.3" data-path="bayesian-canonical-correlation-analysis-in-stan.html"><a href="bayesian-canonical-correlation-analysis-in-stan.html#references-8"><i class="fa fa-check"></i><b>10.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="bayesian-seasonal-decomposition-in-stan-and-rstan.html"><a href="bayesian-seasonal-decomposition-in-stan-and-rstan.html"><i class="fa fa-check"></i><b>11</b> Bayesian Seasonal Decomposition in Stan and RStan</a>
<ul>
<li class="chapter" data-level="11.1" data-path="bayesian-seasonal-decomposition-in-stan-and-rstan.html"><a href="bayesian-seasonal-decomposition-in-stan-and-rstan.html#simulating-seasonal-time-series-data"><i class="fa fa-check"></i><b>11.1</b> Simulating Seasonal Time Series Data*</a></li>
<li class="chapter" data-level="11.2" data-path="bayesian-seasonal-decomposition-in-stan-and-rstan.html"><a href="bayesian-seasonal-decomposition-in-stan-and-rstan.html#bayesian-seasonal-decomposition-model-in-stan"><i class="fa fa-check"></i><b>11.2</b> Bayesian Seasonal Decomposition Model in Stan</a></li>
<li class="chapter" data-level="11.3" data-path="bayesian-seasonal-decomposition-in-stan-and-rstan.html"><a href="bayesian-seasonal-decomposition-in-stan-and-rstan.html#fitting-the-model-in-r"><i class="fa fa-check"></i><b>11.3</b> Fitting the Model in R</a></li>
<li class="chapter" data-level="11.4" data-path="bayesian-seasonal-decomposition-in-stan-and-rstan.html"><a href="bayesian-seasonal-decomposition-in-stan-and-rstan.html#extracting-and-visualizing-components"><i class="fa fa-check"></i><b>11.4</b> Extracting and Visualizing Components</a></li>
<li class="chapter" data-level="11.5" data-path="bayesian-seasonal-decomposition-in-stan-and-rstan.html"><a href="bayesian-seasonal-decomposition-in-stan-and-rstan.html#extensions-and-applications"><i class="fa fa-check"></i><b>11.5</b> Extensions and Applications</a></li>
<li class="chapter" data-level="11.6" data-path="bayesian-seasonal-decomposition-in-stan-and-rstan.html"><a href="bayesian-seasonal-decomposition-in-stan-and-rstan.html#conclusion-6"><i class="fa fa-check"></i><b>11.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><i class="fa fa-check"></i><b>12</b> Bayesian Exponential Smoothing and Holt-Winters Models in Stan and RStan</a>
<ul>
<li class="chapter" data-level="12.1" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#bayesian-simple-exponential-smoothing-ses"><i class="fa fa-check"></i><b>12.1</b> Bayesian Simple Exponential Smoothing (SES)</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#stan-model-1"><i class="fa fa-check"></i><b>12.1.1</b> Stan Model</a></li>
<li class="chapter" data-level="12.1.2" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#fitting-in-r"><i class="fa fa-check"></i><b>12.1.2</b> Fitting in R</a></li>
<li class="chapter" data-level="12.1.3" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#visualization"><i class="fa fa-check"></i><b>12.1.3</b> Visualization</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#bayesian-holt-winters-models"><i class="fa fa-check"></i><b>12.2</b> Bayesian Holt-Winters Models</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#without-seasonality-additive-trend"><i class="fa fa-check"></i><b>12.2.1</b> Without Seasonality (Additive Trend)</a></li>
<li class="chapter" data-level="12.2.2" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#stan-model-2"><i class="fa fa-check"></i><b>12.2.2</b> Stan Model</a></li>
<li class="chapter" data-level="12.2.3" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#visualization-1"><i class="fa fa-check"></i><b>12.2.3</b> Visualization</a></li>
<li class="chapter" data-level="12.2.4" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#with-seasonality-additive-seasonal-component"><i class="fa fa-check"></i><b>12.2.4</b> With Seasonality (Additive Seasonal Component)</a></li>
<li class="chapter" data-level="12.2.5" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#fit"><i class="fa fa-check"></i><b>12.2.5</b> Fit</a></li>
<li class="chapter" data-level="12.2.6" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#visualization-2"><i class="fa fa-check"></i><b>12.2.6</b> Visualization</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#conclusion-7"><i class="fa fa-check"></i><b>12.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="bayesian-ar-arma-and-arima-models-in-stan-and-rstan.html"><a href="bayesian-ar-arma-and-arima-models-in-stan-and-rstan.html"><i class="fa fa-check"></i><b>13</b> Bayesian AR, ARMA, and ARIMA Models in Stan and RStan</a>
<ul>
<li class="chapter" data-level="13.1" data-path="bayesian-ar-arma-and-arima-models-in-stan-and-rstan.html"><a href="bayesian-ar-arma-and-arima-models-in-stan-and-rstan.html#bayesian-ar1-model"><i class="fa fa-check"></i><b>13.1</b> Bayesian AR(1) Model</a></li>
<li class="chapter" data-level="13.2" data-path="bayesian-ar-arma-and-arima-models-in-stan-and-rstan.html"><a href="bayesian-ar-arma-and-arima-models-in-stan-and-rstan.html#bayesian-arma11-model"><i class="fa fa-check"></i><b>13.2</b> Bayesian ARMA(1,1) Model</a></li>
<li class="chapter" data-level="13.3" data-path="bayesian-ar-arma-and-arima-models-in-stan-and-rstan.html"><a href="bayesian-ar-arma-and-arima-models-in-stan-and-rstan.html#bayesian-arima111-model"><i class="fa fa-check"></i><b>13.3</b> Bayesian ARIMA(1,1,1) Model</a></li>
<li class="chapter" data-level="13.4" data-path="bayesian-ar-arma-and-arima-models-in-stan-and-rstan.html"><a href="bayesian-ar-arma-and-arima-models-in-stan-and-rstan.html#conclusion-8"><i class="fa fa-check"></i><b>13.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="bayesian-structural-time-series-models.html"><a href="bayesian-structural-time-series-models.html"><i class="fa fa-check"></i><b>14</b> 14 Bayesian Structural Time Series Models</a>
<ul>
<li class="chapter" data-level="14.1" data-path="bayesian-structural-time-series-models.html"><a href="bayesian-structural-time-series-models.html#introduction-4"><i class="fa fa-check"></i><b>14.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="bayesian-structural-time-series-models.html"><a href="bayesian-structural-time-series-models.html#stan-implementation-basic-bsts-model"><i class="fa fa-check"></i><b>14.1.1</b> Stan Implementation: Basic BSTS Model</a></li>
<li class="chapter" data-level="14.1.2" data-path="bayesian-structural-time-series-models.html"><a href="bayesian-structural-time-series-models.html#stan-implementation-time-varying-trend-volatility"><i class="fa fa-check"></i><b>14.1.2</b> Stan Implementation: Time-Varying Trend Volatility</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="bayesian-structural-time-series-models.html"><a href="bayesian-structural-time-series-models.html#model-comparison-and-selection"><i class="fa fa-check"></i><b>14.2</b> Model Comparison and Selection</a></li>
<li class="chapter" data-level="14.3" data-path="bayesian-structural-time-series-models.html"><a href="bayesian-structural-time-series-models.html#conclusion-9"><i class="fa fa-check"></i><b>14.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="model-comparison-and-selection-in-bayesian-analysis.html"><a href="model-comparison-and-selection-in-bayesian-analysis.html"><i class="fa fa-check"></i><b>15</b> Model Comparison and Selection in Bayesian Analysis</a>
<ul>
<li class="chapter" data-level="15.1" data-path="model-comparison-and-selection-in-bayesian-analysis.html"><a href="model-comparison-and-selection-in-bayesian-analysis.html#introduction-5"><i class="fa fa-check"></i><b>15.1</b> Introduction</a></li>
<li class="chapter" data-level="15.2" data-path="model-comparison-and-selection-in-bayesian-analysis.html"><a href="model-comparison-and-selection-in-bayesian-analysis.html#posterior-predictive-checks"><i class="fa fa-check"></i><b>15.2</b> Posterior Predictive Checks</a></li>
<li class="chapter" data-level="15.3" data-path="model-comparison-and-selection-in-bayesian-analysis.html"><a href="model-comparison-and-selection-in-bayesian-analysis.html#information-criteria-waic-and-loo-cv"><i class="fa fa-check"></i><b>15.3</b> Information Criteria: WAIC and LOO-CV</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="model-comparison-and-selection-in-bayesian-analysis.html"><a href="model-comparison-and-selection-in-bayesian-analysis.html#waic-watanabe-akaike-information-criterion"><i class="fa fa-check"></i><b>15.3.1</b> WAIC (Watanabe-Akaike Information Criterion)</a></li>
<li class="chapter" data-level="15.3.2" data-path="model-comparison-and-selection-in-bayesian-analysis.html"><a href="model-comparison-and-selection-in-bayesian-analysis.html#loo-cv-leave-one-out-cross-validation"><i class="fa fa-check"></i><b>15.3.2</b> LOO-CV (Leave-One-Out Cross-Validation)</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="model-comparison-and-selection-in-bayesian-analysis.html"><a href="model-comparison-and-selection-in-bayesian-analysis.html#bayes-factors-vs.-information-criteria"><i class="fa fa-check"></i><b>15.4</b> Bayes Factors vs. Information Criteria</a></li>
<li class="chapter" data-level="15.5" data-path="model-comparison-and-selection-in-bayesian-analysis.html"><a href="model-comparison-and-selection-in-bayesian-analysis.html#summary-of-bayesian-model-comparison-methods"><i class="fa fa-check"></i><b>15.5</b> Summary of Bayesian Model Comparison Methods</a></li>
<li class="chapter" data-level="15.6" data-path="model-comparison-and-selection-in-bayesian-analysis.html"><a href="model-comparison-and-selection-in-bayesian-analysis.html#practical-example-comparing-models-in-r"><i class="fa fa-check"></i><b>15.6</b> Practical Example: Comparing Models in R</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>16</b> Appendix</a>
<ul>
<li class="chapter" data-level="16.1" data-path="appendix.html"><a href="appendix.html#main-stan-distributions-cheatsheet"><i class="fa fa-check"></i><b>16.1</b> Main Stan Distributions Cheatsheet</a></li>
<li class="chapter" data-level="16.2" data-path="appendix.html"><a href="appendix.html#main-stan-functions-cheatsheet"><i class="fa fa-check"></i><b>16.2</b> Main Stan Functions Cheatsheet</a></li>
<li class="chapter" data-level="16.3" data-path="appendix.html"><a href="appendix.html#why-non-distribution-functions"><i class="fa fa-check"></i><b>16.3</b> Why Non-Distribution Functions?</a></li>
<li class="chapter" data-level="16.4" data-path="appendix.html"><a href="appendix.html#stan-functions-cheatsheet"><i class="fa fa-check"></i><b>16.4</b> Stan Functions Cheatsheet</a>
<ul>
<li class="chapter" data-level="16.4.1" data-path="appendix.html"><a href="appendix.html#mathematical-functions"><i class="fa fa-check"></i><b>16.4.1</b> 1. Mathematical Functions</a></li>
<li class="chapter" data-level="16.4.2" data-path="appendix.html"><a href="appendix.html#transformation-functions"><i class="fa fa-check"></i><b>16.4.2</b> 2. Transformation Functions</a></li>
<li class="chapter" data-level="16.4.3" data-path="appendix.html"><a href="appendix.html#matrix-and-vector-operations"><i class="fa fa-check"></i><b>16.4.3</b> 3. Matrix and Vector Operations</a></li>
<li class="chapter" data-level="16.4.4" data-path="appendix.html"><a href="appendix.html#utility-functions"><i class="fa fa-check"></i><b>16.4.4</b> 4. Utility Functions</a></li>
<li class="chapter" data-level="16.4.5" data-path="appendix.html"><a href="appendix.html#specialized-solvers"><i class="fa fa-check"></i><b>16.4.5</b> 5. Specialized Solvers</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="appendix.html"><a href="appendix.html#example-hierarchical-linear-regression"><i class="fa fa-check"></i><b>16.5</b> Example: Hierarchical Linear Regression</a></li>
<li class="chapter" data-level="16.6" data-path="appendix.html"><a href="appendix.html#tips-for-using-stan-functions"><i class="fa fa-check"></i><b>16.6</b> Tips for Using Stan Functions</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Stan Bookdown</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian-gaussian-mixture-models" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> Bayesian Gaussian Mixture Models<a href="bayesian-gaussian-mixture-models.html#bayesian-gaussian-mixture-models" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In statistics, a mixture model is a probabilistic model used to represent the presence of subpopulations within an overall population without requiring that an individual belongs to a specific subpopulation. It is a flexible approach that can be used to model complex data containing multiple regions with high probability mass, such as multimodal distributions. A typical finite-dimensional mixture model consists of observed random variables, random latent variables specifying the identity of the mixture component of each observation, mixture weights, and parameters. Mixture models can be used to make statistical inferences about the properties of subpopulations without sub-population identity information. Mixture models are also referred to as latent class models if they assume that some of their parameters differ across unobserved subgroups or classes.</p>
<p>Bayesian mixture models can be implemented in Stan, a probabilistic programming language. Mixture models assume that a given measurement can be drawn from one of K data generating processes, each with their own set of parameters. Stan allows for the fitting of Bayesian mixture models using its Hamiltonian Monte Carlo sampler. The models can be parameterized in several ways (see below) and used directly for modeling data with multimodal distributions or as priors for other parameters. The implementation of mixture models in Stan involves defining the model, specifying the priors, and marginalizing out the discrete parameters. Several resources provide examples and tutorials on fitting Bayesian mixture models in Stan, demonstrating the practical implementation of these models.</p>
<p>In this post I will first introduce how mixture models are implemented in Bayesian inference. It is noteworthy to take into consideration non-identifiability inherent these models how the non-identifiability can be tempered with principled prior information. Michael Betancourt has a blogpost describing the problems often encountered with gaussian mixture models, specifically the estimation of parameters of a mixture model and identifiability i.e. the problem with labelling <a href="http://mc-stan.org/documentation/case-studies/identifying_mixture_models.html">mixtures</a>.</p>
<div id="single-varaible-example" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> Single varaible example<a href="bayesian-gaussian-mixture-models.html#single-varaible-example" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<pre><code>library(dplyr)
library(ggplot2)
library(ggthemes)

N &lt;- 500

#  three clusters
mu &lt;- c(1, 4, 9)
sigma &lt;- c(1.2, 1, 0.8)

# probability of each cluster
Theta &lt;- c(.3, .5, .3)

# Draw which model each belongs to
z &lt;- sample(1:3, size = N, prob = Theta, replace = T)

# white noise
epsilon &lt;- rnorm(N)

# Simulate the data using the fact that y ~ normal(mu, sigma) can be 
# expressed as y = mu + sigma*epsilon for epsilon ~ normal(0, 1)
y &lt;- mu[z] + sigma[z]*epsilon

data_frame(y= y, z = as.factor(z)) %&gt;% 
  ggplot(aes(x = y, fill = z)) +
  geom_density(alpha = 0.3) +
  ggtitle(&quot;Three clusters&quot;)</code></pre>
<p><img src="/images/gmm_1.png" /></p>
<pre><code>mixture_model&lt;-&#39;

// saved as finite_mixture_linear_regression.stan
data {
  int N;
  vector[N] y;
  int n_groups;
}
parameters {
  vector[n_groups] mu;
  vector&lt;lower = 0&gt;[n_groups] sigma;
  simplex[n_groups] Theta;
}
model {
  vector[n_groups] contributions;
  // priors
  mu ~ normal(0, 10);
  sigma ~ cauchy(0, 2);
  Theta ~ dirichlet(rep_vector(2.0, n_groups));
  
  
  // likelihood
  for(i in 1:N) {
    for(k in 1:n_groups) {
      contributions[k] = log(Theta[k]) + normal_lpdf(y[i] | mu[k], sigma[k]);
    }
    target += log_sum_exp(contributions);
  }
}&#39;</code></pre>
<p><strong>Data Block</strong>
- <code>N</code>: Number of observations.
- <code>y</code>: Vector of observed responses.
- <code>n_groups</code>: Number of mixture components or groups.</p>
<p><strong>Parameters Block</strong>
- <code>mu</code>: Vector of means for each mixture component.
- <code>sigma</code>: Vector of standard deviations for each mixture component.
- <code>Theta</code>: Vector of mixing proportions, representing the probability of each group.</p>
<p><strong>Model Block</strong>
- <strong>Priors</strong>: Normal priors are specified for the means <code>mu</code> with a mean of 0 and a standard deviation of 10. Cauchy priors are specified for the standard deviations <code>sigma</code> with a location of 0 and a scale of 2. Dirichlet priors are specified for the mixing proportions <code>Theta</code> with equal concentration parameters of 2.0 for each group.
- <strong>Likelihood</strong>: The likelihood is constructed within a nested loop. For each observation <code>i</code> and each group <code>k</code>, it calculates the log-likelihood of the observation given the mean and standard deviation of that group. These log-likelihoods are stored in the <code>contributions</code> vector.
- <strong>Log-Sum-Exp Trick</strong>: To avoid numerical instability when dealing with small probabilities, the log-sum-exp trick is used. The <code>log_sum_exp</code> function sums up the contributions after exponentiating them. This is done to compute the log-likelihood of the data given the mixture model.
- <strong>Target</strong>: The <code>target</code> is incremented by the log of the sum of exponentiated contributions for each observation. The <code>target</code> is essentially the log-posterior, and the goal of Stan is to maximize it during sampling.</p>
<pre><code>library(rstan)
options(mc.cores = parallel::detectCores())

fit=stan(model_code=mixture_model, data=list(N= N, y = y, n_groups = 3), iter=3000, warmup=500, chains=3)


print(fit)
params=extract(fit)
#density plots of the posteriors of the mixture means
par(mfrow=c(1,3))
plot(density(params$mu[,1]), ylab=&#39;&#39;, xlab=&#39;mu[1]&#39;, main=&#39;&#39;)
abline(v=c(8), lty=&#39;dotted&#39;, col=&#39;red&#39;,lwd=2)


plot(density(params$mu[,2]), ylab=&#39;&#39;, xlab=&#39;mu[1]&#39;, main=&#39;&#39;)
abline(v=c(0), lty=&#39;dotted&#39;, col=&#39;red&#39;,lwd=2)

plot(density(params$mu[,3]), ylab=&#39;&#39;, xlab=&#39;mu[1]&#39;, main=&#39;&#39;)
abline(v=c(4), lty=&#39;dotted&#39;, col=&#39;red&#39;,lwd=2)

</code></pre>
<pre><code>Inference for Stan model: 9c40393d28e90e2c335fff95de690860.
3 chains, each with iter=3000; warmup=500; thin=1; 
post-warmup draws per chain=2500, total post-warmup draws=7500.

             mean se_mean   sd     2.5%      25%      50%      75%    97.5% n_eff  Rhat
mu[1]        6.35    3.06 3.76     0.65     1.21     8.96     9.02     9.11     2 21.87
mu[2]        3.72    3.05 3.74     0.59     0.94     1.25     8.96     9.09     2 13.86
mu[3]        4.03    0.00 0.17     3.71     3.92     4.03     4.14     4.36  2575  1.00
sigma[1]     0.85    0.17 0.23     0.62     0.69     0.73     1.02     1.41     2  2.32
sigma[2]     1.01    0.19 0.27     0.64     0.73     1.03     1.19     1.60     2  1.76
sigma[3]     1.13    0.00 0.12     0.92     1.05     1.12     1.20     1.39  3232  1.00
Theta[1]     0.27    0.00 0.04     0.19     0.25     0.27     0.29     0.34  1186  1.02
Theta[2]     0.27    0.00 0.05     0.18     0.24     0.26     0.29     0.40  1553  1.00
Theta[3]     0.47    0.00 0.06     0.32     0.44     0.47     0.51     0.57  1702  1.00
lp__     -1161.00    0.05 2.18 -1166.34 -1162.15 -1160.64 -1159.40 -1157.91  2064  1.00

Samples were drawn using NUTS(diag_e) at Tue Feb  6 13:03:03 2024.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<p><img src="/images/gmm_2.png" /></p>
</div>
<div id="example-with-multiple-variable" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Example with multiple variable<a href="bayesian-gaussian-mixture-models.html#example-with-multiple-variable" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<pre><code>library(MASS)

#first cluster
mu1=c(0,0,0,0)
sigma1=matrix(c(0.2,0,0,0,0,0.2,0,0,0,0,0.1,0,0,0,0,0.1),ncol=4,nrow=4, byrow=TRUE)
norm1=mvrnorm(30, mu1, sigma1)

#second cluster
mu2=c(10,10,10,10)
sigma2=sigma1
norm2=mvrnorm(30, mu2, sigma2)

#third cluster
mu3=c(4,4,4,4)
sigma3=sigma1
norm3=mvrnorm(30, mu3, sigma3)

norms=rbind(norm1,norm2,norm3) #combine the 3 mixtures together
N=90 #total number of data points 
Dim=4 #number of dimensions
y=array(as.vector(norms), dim=c(N,Dim))
mixture_data=list(N=N, D=4, K=3, y=y)

as.data.frame(norms)  %&gt;%
  pivot_longer(colnames(as.data.frame(norms)), names_to = &quot;var&quot;, values_to = &quot;value&quot;)%&gt;%
  ggplot( aes(x=value, color=var)) + geom_density() +
  ggtitle(&quot;Three clusters on four variables&quot;)</code></pre>
<p><img src="/images/gmm_3.png" /></p>
<pre><code>mixture_model&lt;-&#39;
data {
 int D; //number of dimensions
 int K; //number of gaussians
 int N; //number of data
 vector[D] y[N]; //data
}

parameters {
 simplex[K] theta; //mixing proportions
 ordered[D] mu[K]; //mixture component means
 cholesky_factor_corr[D] L[K]; //cholesky factor of covariance
}

model {
 real ps[K];
 
 for(k in 1:K){
 mu[k] ~ normal(0,3);
 L[k] ~ lkj_corr_cholesky(4);
 }
 

 for (n in 1:N){
 for (k in 1:K){
 ps[k] = log(theta[k])+multi_normal_cholesky_lpdf(y[n] | mu[k], L[k]); 
 }
 target += log_sum_exp(ps);
 }

}&#39;</code></pre>
<p><strong>Data Block</strong>
- <code>D</code>: Number of dimensions.
- <code>K</code>: Number of Gaussian components.
- <code>N</code>: Number of data points.
- <code>y</code>: An array of vectors, each representing a data point in D dimensions.</p>
<p><strong>Parameters Block</strong>
- <code>theta</code>: Mixing proportions. It is a simplex, ensuring that the proportions sum to 1.
- <code>mu</code>: Mixture component means. These are ordered variables.
- <code>L</code>: Cholesky factors of the covariance matrices for each component.</p>
<p><strong>Model Block</strong>
- <strong>Priors</strong>: Priors are specified for the means <code>mu</code> and the Cholesky factors <code>L</code>. Each mean is drawn from a normal distribution with a mean of 0 and a standard deviation of 3. The Cholesky factor is drawn from a LKJ correlation distribution with shape parameter 4.</p>
<ul>
<li><p><strong>Log-Probability Calculation</strong>: For each data point <code>n</code> and each component <code>k</code>, the log-probability <code>ps[k]</code> is calculated. This log-probability is the logarithm of the product of the mixing proportion and the multivariate normal density of the data point under the k-th component.</p></li>
<li><p><strong>Target Increment</strong>: The <code>target</code> is incremented by the logarithm of the sum of exponentiated log-probabilities <code>ps</code>. This step ensures that the model assigns higher probability to data points that are well-explained by one of the Gaussian components.</p></li>
</ul>
<pre><code>fit=stan(model_code=mixture_model, data=mixture_data, iter=3000, warmup=1000, chains=1)
print(fit)

Inference for Stan model: f913dae683b9f29657b0863fec348d71.
1 chains, each with iter=3000; warmup=1000; thin=1; 
post-warmup draws per chain=2000, total post-warmup draws=2000.

            mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat
theta[1]    0.33    0.00 0.05    0.24    0.30    0.33    0.37    0.43  2120 1.00
theta[2]    0.33    0.00 0.05    0.24    0.30    0.33    0.36    0.44  1740 1.00
theta[3]    0.33    0.00 0.05    0.24    0.30    0.33    0.37    0.43  1854 1.00
mu[1,1]     3.80    0.00 0.11    3.55    3.74    3.80    3.86    3.99   632 1.01
mu[1,2]     3.91    0.01 0.12    3.67    3.84    3.90    3.98    4.15   236 1.00
mu[1,3]     4.02    0.01 0.13    3.78    3.92    4.02    4.11    4.29   344 1.00
mu[1,4]     4.09    0.01 0.15    3.82    3.99    4.09    4.20    4.39   259 1.00
mu[2,1]     9.77    0.01 0.12    9.51    9.70    9.79    9.86    9.98   405 1.00
mu[2,2]     9.96    0.00 0.11    9.74    9.90    9.96   10.03   10.20   794 1.00
mu[2,3]    10.09    0.00 0.11    9.91   10.01   10.07   10.15   10.33   518 1.00
mu[2,4]    10.18    0.01 0.12    9.97   10.09   10.17   10.25   10.43   498 1.00
mu[3,1]    -0.22    0.01 0.13   -0.49   -0.30   -0.21   -0.14    0.01   409 1.01
mu[3,2]    -0.10    0.01 0.13   -0.37   -0.17   -0.09   -0.02    0.17   218 1.00
mu[3,3]     0.07    0.01 0.13   -0.17    0.00    0.07    0.15    0.36    81 1.00
mu[3,4]     0.16    0.01 0.12   -0.03    0.08    0.15    0.23    0.42   541 1.00

For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
<pre><code>
params=extract(fit)
#density plots of the posteriors of the mixture means
par(mfrow=c(1,3))
plot(density(params$mu[,1,1]), ylab=&#39;&#39;, xlab=&#39;mu[1]&#39;, main=&#39;&#39;)
lines(density(params$mu[,1,2]), col=rgb(0,0,0,0.7))
lines(density(params$mu[,1,3]), col=rgb(0,0,0,0.4))
lines(density(params$mu[,1,4]), col=rgb(0,0,0,0.1))
abline(v=c(4), lty=&#39;dotted&#39;, col=&#39;red&#39;,lwd=2)

plot(density(params$mu[,2,1]), ylab=&#39;&#39;, xlab=&#39;mu[2]&#39;, main=&#39;&#39;)
lines(density(params$mu[,2,2]), col=rgb(0,0,0,0.7))
lines(density(params$mu[,2,3]), col=rgb(0,0,0,0.4))
lines(density(params$mu[,2,4]), col=rgb(0,0,0,0.1))
abline(v=c(10), lty=&#39;dotted&#39;, col=&#39;red&#39;,lwd=2)

plot(density(params$mu[,3,1]), ylab=&#39;&#39;, xlab=&#39;mu[3]&#39;, main=&#39;&#39;)
lines(density(params$mu[,3,2]), col=rgb(0,0,0,0.7))
lines(density(params$mu[,3,3]), col=rgb(0,0,0,0.4))
lines(density(params$mu[,3,4]), col=rgb(0,0,0,0.1))
abline(v=c(0), lty=&#39;dotted&#39;, col=&#39;red&#39;,lwd=2)
</code></pre>
<p><img src="/images/gmm_4.png" /></p>
</div>
<div id="conclusion-4" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> Conclusion<a href="bayesian-gaussian-mixture-models.html#conclusion-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bayesian mixture models offer several advantages in statistical modeling. Their inherent flexibility makes them well-suited for diverse tasks such as clustering, data compression, outlier detection, and generative classification. The Bayesian framework’s ability to incorporate prior knowledge enhances model accuracy, especially when informative prior information is available. Moreover, these models effectively handle unobserved heterogeneity by integrating multiple data generating processes, proving valuable when data alone may not fully identify underlying patterns. The stability provided by Bayesian estimation ensures reliable posterior distributions, reducing sensitivity to issues like singularities, over-fitting, and violated identification criteria. Bayesian mixture models also facilitate the examination of the posterior distribution of the number of classes, offering insights into the underlying class structure of the data. However, the use of Bayesian mixture models comes with certain limitations. Applying these models demands a high level of statistical expertise to appropriately specify priors and ensure correct model formulation, presenting a challenge for practitioners lacking a strong background in Bayesian statistics. The complexity of posterior inference is compounded by label switching, a phenomenon that complicates the interpretation of results. Bayesian nonparametric mixture models, in particular, may suffer from inconsistency in estimating the number of clusters, impacting their performance in clustering applications. Additionally, model fitting challenges arise, and careful evaluation of inaccuracies in predictions and comparison with alternative models are essential to address potential shortcomings.</p>
<p>In this post, we learned to fit mixture models using Stan. We saw how to evaluate model fit using the usual prior and posterior predictive checks, and to investigate parameter recovery. Such mixture models are notoriously difficult to fit, but they have a lot of potential in cognitive science applications, especially in developing computational models of different kinds of cognitive processes. The reader interested in a deeper understanding of potential challanges in the process can refer to Betancourt discussion of identification problems in Bayesian mixture models in a <a href="https://mc-stan.org/users/documentation/case-studies/identifying_mixture_models.html">case study</a>.</p>
</div>
<div id="references-7" class="section level2 hasAnchor" number="9.4">
<h2><span class="header-section-number">9.4</span> References<a href="bayesian-gaussian-mixture-models.html#references-7" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><a href="https://modernstatisticalworkflow.blogspot.com/2016/10/finite-mixture-models-in-stan.html">Finite mixture models in Stan</a></li>
<li><a href="https://maggielieu.com/2017/03/21/multivariate-gaussian-mixture-model-done-properly/">Multivariate Gaussian Mixture Model done properly</a></li>
<li><a href="https://mc-stan.org/docs/stan-users-guide/mixture-modeling.html">Finite Mixtures</a></li>
<li><a href="https://mc-stan.org/users/documentation/case-studies/identifying_mixture_models.html">Identifying Bayesian Mixture Models</a></li>
<li><a href="https://vasishth.github.io/bayescogsci/book/ch-mixture.html">Mixture models</a></li>
<li><a href="https://rpubs.com/kaz_yos/fmm2">Bayesian Density Estimation (Finite Mixture Model)</a></li>
<li><a href="https://hal.science/hal-03866434/document">Bayesian mixture models (in)consistency for the number of clusters</a></li>
<li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6459682/">Advantages of a Bayesian Approach for Examining Class Structure in Finite Mixture Models</a></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="gaussian-process-regression-gpr.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bayesian-canonical-correlation-analysis-in-stan.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_Stan/edit/main/09-GMM.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_Stan/blob/main/09-GMM.Rmd",
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
