<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Bayesian Regularized Regression | Stan Bookdown</title>
  <meta name="description" content="Chapter 5 Bayesian Regularized Regression | Stan Bookdown" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Bayesian Regularized Regression | Stan Bookdown" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Bayesian Regularized Regression | Stan Bookdown" />
  
  
  

<meta name="author" content="Kamran Afzali" />


<meta name="date" content="2025-10-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="robust-t-regression.html"/>
<link rel="next" href="bayesian-quantile-regression.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Stan</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Choosing the Right Bayesian Model</a></li>
<li class="chapter" data-level="2" data-path="bayesian-modeling-in-r-and-stan.html"><a href="bayesian-modeling-in-r-and-stan.html"><i class="fa fa-check"></i><b>2</b> Bayesian Modeling in R and Stan</a>
<ul>
<li class="chapter" data-level="2.1" data-path="bayesian-modeling-in-r-and-stan.html"><a href="bayesian-modeling-in-r-and-stan.html#what-is-stan"><i class="fa fa-check"></i><b>2.1</b> What is <code>stan</code>?</a></li>
<li class="chapter" data-level="2.2" data-path="bayesian-modeling-in-r-and-stan.html"><a href="bayesian-modeling-in-r-and-stan.html#model-file"><i class="fa fa-check"></i><b>2.2</b> Model file</a></li>
<li class="chapter" data-level="2.3" data-path="bayesian-modeling-in-r-and-stan.html"><a href="bayesian-modeling-in-r-and-stan.html#fit-the-model"><i class="fa fa-check"></i><b>2.3</b> Fit the model</a></li>
<li class="chapter" data-level="2.4" data-path="bayesian-modeling-in-r-and-stan.html"><a href="bayesian-modeling-in-r-and-stan.html#mcmc-diagnostics"><i class="fa fa-check"></i><b>2.4</b> MCMC diagnostics</a></li>
<li class="chapter" data-level="2.5" data-path="bayesian-modeling-in-r-and-stan.html"><a href="bayesian-modeling-in-r-and-stan.html#conclusions"><i class="fa fa-check"></i><b>2.5</b> Conclusions</a></li>
<li class="chapter" data-level="2.6" data-path="bayesian-modeling-in-r-and-stan.html"><a href="bayesian-modeling-in-r-and-stan.html#references"><i class="fa fa-check"></i><b>2.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayesian-regression-models-for-non-normal-data.html"><a href="bayesian-regression-models-for-non-normal-data.html"><i class="fa fa-check"></i><b>3</b> Bayesian Regression Models for Non-Normal Data</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bayesian-regression-models-for-non-normal-data.html"><a href="bayesian-regression-models-for-non-normal-data.html#logistic-regression"><i class="fa fa-check"></i><b>3.1</b> Logistic Regression</a></li>
<li class="chapter" data-level="3.2" data-path="bayesian-regression-models-for-non-normal-data.html"><a href="bayesian-regression-models-for-non-normal-data.html#negative-binomial"><i class="fa fa-check"></i><b>3.2</b> Negative Binomial</a></li>
<li class="chapter" data-level="3.3" data-path="bayesian-regression-models-for-non-normal-data.html"><a href="bayesian-regression-models-for-non-normal-data.html#conclusion"><i class="fa fa-check"></i><b>3.3</b> Conclusion</a></li>
<li class="chapter" data-level="3.4" data-path="bayesian-regression-models-for-non-normal-data.html"><a href="bayesian-regression-models-for-non-normal-data.html#references-1"><i class="fa fa-check"></i><b>3.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="robust-t-regression.html"><a href="robust-t-regression.html"><i class="fa fa-check"></i><b>4</b> Robust t-regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="robust-t-regression.html"><a href="robust-t-regression.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="robust-t-regression.html"><a href="robust-t-regression.html#concepts-and-code"><i class="fa fa-check"></i><b>4.2</b> Concepts and code</a></li>
<li class="chapter" data-level="4.3" data-path="robust-t-regression.html"><a href="robust-t-regression.html#conclusion-1"><i class="fa fa-check"></i><b>4.3</b> Conclusion</a></li>
<li class="chapter" data-level="4.4" data-path="robust-t-regression.html"><a href="robust-t-regression.html#references-2"><i class="fa fa-check"></i><b>4.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html"><i class="fa fa-check"></i><b>5</b> Bayesian Regularized Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#introduction-1"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#ridge-regression-and-gaussian-priors"><i class="fa fa-check"></i><b>5.2</b> Ridge Regression and Gaussian Priors</a></li>
<li class="chapter" data-level="5.3" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#lasso-regression-and-laplace-priors"><i class="fa fa-check"></i><b>5.3</b> LASSO Regression and Laplace Priors</a></li>
<li class="chapter" data-level="5.4" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#hierarchical-shrinkage-and-adaptive-priors"><i class="fa fa-check"></i><b>5.4</b> Hierarchical Shrinkage and Adaptive Priors</a></li>
<li class="chapter" data-level="5.5" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#implementation"><i class="fa fa-check"></i><b>5.5</b> Implementation</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#bayesian-ridge-regression"><i class="fa fa-check"></i><b>5.5.1</b> Bayesian Ridge Regression</a></li>
<li class="chapter" data-level="5.5.2" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#bayesian-lasso-regression"><i class="fa fa-check"></i><b>5.5.2</b> Bayesian LASSO Regression</a></li>
<li class="chapter" data-level="5.5.3" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#hierarchical-shrinkage-horseshoe-prior"><i class="fa fa-check"></i><b>5.5.3</b> Hierarchical Shrinkage (Horseshoe Prior)</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#empirical-comparison-and-model-evaluation"><i class="fa fa-check"></i><b>5.6</b> Empirical Comparison and Model Evaluation</a></li>
<li class="chapter" data-level="5.7" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#understanding-posterior-behavior"><i class="fa fa-check"></i><b>5.7</b> Understanding Posterior Behavior</a></li>
<li class="chapter" data-level="5.8" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#choosing-among-regularization-methods"><i class="fa fa-check"></i><b>5.8</b> Choosing Among Regularization Methods</a></li>
<li class="chapter" data-level="5.9" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#conclusion-2"><i class="fa fa-check"></i><b>5.9</b> Conclusion</a></li>
<li class="chapter" data-level="5.10" data-path="bayesian-regularized-regression.html"><a href="bayesian-regularized-regression.html#references-3"><i class="fa fa-check"></i><b>5.10</b> References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bayesian-quantile-regression.html"><a href="bayesian-quantile-regression.html"><i class="fa fa-check"></i><b>6</b> Bayesian Quantile Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="bayesian-quantile-regression.html"><a href="bayesian-quantile-regression.html#introduction-2"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="bayesian-quantile-regression.html"><a href="bayesian-quantile-regression.html#model-specification"><i class="fa fa-check"></i><b>6.2</b> Model Specification</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="bayesian-quantile-regression.html"><a href="bayesian-quantile-regression.html#using-bayesqr-package"><i class="fa fa-check"></i><b>6.2.1</b> Using bayesQR package</a></li>
<li class="chapter" data-level="6.2.2" data-path="bayesian-quantile-regression.html"><a href="bayesian-quantile-regression.html#using-brms-package"><i class="fa fa-check"></i><b>6.2.2</b> Using brms package</a></li>
<li class="chapter" data-level="6.2.3" data-path="bayesian-quantile-regression.html"><a href="bayesian-quantile-regression.html#backend-stan-model"><i class="fa fa-check"></i><b>6.2.3</b> Backend stan model</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="bayesian-quantile-regression.html"><a href="bayesian-quantile-regression.html#conclusion-3"><i class="fa fa-check"></i><b>6.3</b> Conclusion</a></li>
<li class="chapter" data-level="6.4" data-path="bayesian-quantile-regression.html"><a href="bayesian-quantile-regression.html#references-4"><i class="fa fa-check"></i><b>6.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian-multilevel-regression-in-stan.html"><a href="bayesian-multilevel-regression-in-stan.html"><i class="fa fa-check"></i><b>7</b> Bayesian Multilevel Regression in Stan</a>
<ul>
<li class="chapter" data-level="7.1" data-path="bayesian-multilevel-regression-in-stan.html"><a href="bayesian-multilevel-regression-in-stan.html#introduction-3"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="bayesian-multilevel-regression-in-stan.html"><a href="bayesian-multilevel-regression-in-stan.html#implementation-in-r"><i class="fa fa-check"></i><b>7.2</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="bayesian-multilevel-regression-in-stan.html"><a href="bayesian-multilevel-regression-in-stan.html#random-intercepts-model-modeling-baseline-differences"><i class="fa fa-check"></i><b>7.2.1</b> Random Intercepts Model: Modeling Baseline Differences</a></li>
<li class="chapter" data-level="7.2.2" data-path="bayesian-multilevel-regression-in-stan.html"><a href="bayesian-multilevel-regression-in-stan.html#random-slopes-model-capturing-varying-effects"><i class="fa fa-check"></i><b>7.2.2</b> Random Slopes Model: Capturing Varying Effects</a></li>
<li class="chapter" data-level="7.2.3" data-path="bayesian-multilevel-regression-in-stan.html"><a href="bayesian-multilevel-regression-in-stan.html#model-comparison-and-visualization"><i class="fa fa-check"></i><b>7.2.3</b> Model Comparison and Visualization</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="bayesian-multilevel-regression-in-stan.html"><a href="bayesian-multilevel-regression-in-stan.html#visualization-and-interpretation"><i class="fa fa-check"></i><b>7.3</b> Visualization and Interpretation</a></li>
<li class="chapter" data-level="7.4" data-path="bayesian-multilevel-regression-in-stan.html"><a href="bayesian-multilevel-regression-in-stan.html#conclusion-4"><i class="fa fa-check"></i><b>7.4</b> Conclusion</a></li>
<li class="chapter" data-level="7.5" data-path="bayesian-multilevel-regression-in-stan.html"><a href="bayesian-multilevel-regression-in-stan.html#references-5"><i class="fa fa-check"></i><b>7.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="gaussian-process-regression-gpr.html"><a href="gaussian-process-regression-gpr.html"><i class="fa fa-check"></i><b>8</b> Gaussian Process Regression (GPR)</a>
<ul>
<li class="chapter" data-level="8.1" data-path="gaussian-process-regression-gpr.html"><a href="gaussian-process-regression-gpr.html#introduction-4"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="gaussian-process-regression-gpr.html"><a href="gaussian-process-regression-gpr.html#challenges"><i class="fa fa-check"></i><b>8.2</b> Challenges</a></li>
<li class="chapter" data-level="8.3" data-path="gaussian-process-regression-gpr.html"><a href="gaussian-process-regression-gpr.html#gpfit-package"><i class="fa fa-check"></i><b>8.3</b> GPfit package</a></li>
<li class="chapter" data-level="8.4" data-path="gaussian-process-regression-gpr.html"><a href="gaussian-process-regression-gpr.html#bayesian-gpr"><i class="fa fa-check"></i><b>8.4</b> Bayesian GPR</a></li>
<li class="chapter" data-level="8.5" data-path="gaussian-process-regression-gpr.html"><a href="gaussian-process-regression-gpr.html#references-6"><i class="fa fa-check"></i><b>8.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="bayesian-gaussian-mixture-models.html"><a href="bayesian-gaussian-mixture-models.html"><i class="fa fa-check"></i><b>9</b> Bayesian Gaussian Mixture Models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="bayesian-gaussian-mixture-models.html"><a href="bayesian-gaussian-mixture-models.html#single-varaible-example"><i class="fa fa-check"></i><b>9.1</b> Single varaible example</a></li>
<li class="chapter" data-level="9.2" data-path="bayesian-gaussian-mixture-models.html"><a href="bayesian-gaussian-mixture-models.html#example-with-multiple-variable"><i class="fa fa-check"></i><b>9.2</b> Example with multiple variable</a></li>
<li class="chapter" data-level="9.3" data-path="bayesian-gaussian-mixture-models.html"><a href="bayesian-gaussian-mixture-models.html#conclusion-5"><i class="fa fa-check"></i><b>9.3</b> Conclusion</a></li>
<li class="chapter" data-level="9.4" data-path="bayesian-gaussian-mixture-models.html"><a href="bayesian-gaussian-mixture-models.html#references-7"><i class="fa fa-check"></i><b>9.4</b> References</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bayesian-canonical-correlation-analysis-in-stan.html"><a href="bayesian-canonical-correlation-analysis-in-stan.html"><i class="fa fa-check"></i><b>10</b> Bayesian Canonical Correlation Analysis in Stan</a>
<ul>
<li class="chapter" data-level="10.1" data-path="bayesian-canonical-correlation-analysis-in-stan.html"><a href="bayesian-canonical-correlation-analysis-in-stan.html#introduction-5"><i class="fa fa-check"></i><b>10.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="bayesian-canonical-correlation-analysis-in-stan.html"><a href="bayesian-canonical-correlation-analysis-in-stan.html#canonical-correlation-analysis"><i class="fa fa-check"></i><b>10.1.1</b> Canonical Correlation Analysis</a></li>
<li class="chapter" data-level="10.1.2" data-path="bayesian-canonical-correlation-analysis-in-stan.html"><a href="bayesian-canonical-correlation-analysis-in-stan.html#stan-model"><i class="fa fa-check"></i><b>10.1.2</b> Stan Model</a></li>
<li class="chapter" data-level="10.1.3" data-path="bayesian-canonical-correlation-analysis-in-stan.html"><a href="bayesian-canonical-correlation-analysis-in-stan.html#model-validation-on-test-set"><i class="fa fa-check"></i><b>10.1.3</b> Model validation on test set</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="bayesian-canonical-correlation-analysis-in-stan.html"><a href="bayesian-canonical-correlation-analysis-in-stan.html#conclusion-6"><i class="fa fa-check"></i><b>10.2</b> Conclusion</a></li>
<li class="chapter" data-level="10.3" data-path="bayesian-canonical-correlation-analysis-in-stan.html"><a href="bayesian-canonical-correlation-analysis-in-stan.html#references-8"><i class="fa fa-check"></i><b>10.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="bayesian-seasonal-decomposition-in-stan-and-rstan.html"><a href="bayesian-seasonal-decomposition-in-stan-and-rstan.html"><i class="fa fa-check"></i><b>11</b> Bayesian Seasonal Decomposition in Stan and RStan</a>
<ul>
<li class="chapter" data-level="11.1" data-path="bayesian-seasonal-decomposition-in-stan-and-rstan.html"><a href="bayesian-seasonal-decomposition-in-stan-and-rstan.html#simulating-seasonal-time-series-data"><i class="fa fa-check"></i><b>11.1</b> Simulating Seasonal Time Series Data*</a></li>
<li class="chapter" data-level="11.2" data-path="bayesian-seasonal-decomposition-in-stan-and-rstan.html"><a href="bayesian-seasonal-decomposition-in-stan-and-rstan.html#bayesian-seasonal-decomposition-model-in-stan"><i class="fa fa-check"></i><b>11.2</b> Bayesian Seasonal Decomposition Model in Stan</a></li>
<li class="chapter" data-level="11.3" data-path="bayesian-seasonal-decomposition-in-stan-and-rstan.html"><a href="bayesian-seasonal-decomposition-in-stan-and-rstan.html#fitting-the-model-in-r"><i class="fa fa-check"></i><b>11.3</b> Fitting the Model in R</a></li>
<li class="chapter" data-level="11.4" data-path="bayesian-seasonal-decomposition-in-stan-and-rstan.html"><a href="bayesian-seasonal-decomposition-in-stan-and-rstan.html#extracting-and-visualizing-components"><i class="fa fa-check"></i><b>11.4</b> Extracting and Visualizing Components</a></li>
<li class="chapter" data-level="11.5" data-path="bayesian-seasonal-decomposition-in-stan-and-rstan.html"><a href="bayesian-seasonal-decomposition-in-stan-and-rstan.html#extensions-and-applications"><i class="fa fa-check"></i><b>11.5</b> Extensions and Applications</a></li>
<li class="chapter" data-level="11.6" data-path="bayesian-seasonal-decomposition-in-stan-and-rstan.html"><a href="bayesian-seasonal-decomposition-in-stan-and-rstan.html#conclusion-7"><i class="fa fa-check"></i><b>11.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><i class="fa fa-check"></i><b>12</b> Bayesian Exponential Smoothing and Holt-Winters Models in Stan and RStan</a>
<ul>
<li class="chapter" data-level="12.1" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#bayesian-simple-exponential-smoothing-ses"><i class="fa fa-check"></i><b>12.1</b> Bayesian Simple Exponential Smoothing (SES)</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#stan-model-1"><i class="fa fa-check"></i><b>12.1.1</b> Stan Model</a></li>
<li class="chapter" data-level="12.1.2" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#fitting-in-r"><i class="fa fa-check"></i><b>12.1.2</b> Fitting in R</a></li>
<li class="chapter" data-level="12.1.3" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#visualization"><i class="fa fa-check"></i><b>12.1.3</b> Visualization</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#bayesian-holt-winters-models"><i class="fa fa-check"></i><b>12.2</b> Bayesian Holt-Winters Models</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#without-seasonality-additive-trend"><i class="fa fa-check"></i><b>12.2.1</b> Without Seasonality (Additive Trend)</a></li>
<li class="chapter" data-level="12.2.2" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#stan-model-2"><i class="fa fa-check"></i><b>12.2.2</b> Stan Model</a></li>
<li class="chapter" data-level="12.2.3" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#visualization-1"><i class="fa fa-check"></i><b>12.2.3</b> Visualization</a></li>
<li class="chapter" data-level="12.2.4" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#with-seasonality-additive-seasonal-component"><i class="fa fa-check"></i><b>12.2.4</b> With Seasonality (Additive Seasonal Component)</a></li>
<li class="chapter" data-level="12.2.5" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#fit"><i class="fa fa-check"></i><b>12.2.5</b> Fit</a></li>
<li class="chapter" data-level="12.2.6" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#visualization-2"><i class="fa fa-check"></i><b>12.2.6</b> Visualization</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html"><a href="bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html#conclusion-8"><i class="fa fa-check"></i><b>12.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="bayesian-ar-arma-and-arima-models-in-stan-and-rstan.html"><a href="bayesian-ar-arma-and-arima-models-in-stan-and-rstan.html"><i class="fa fa-check"></i><b>13</b> Bayesian AR, ARMA, and ARIMA Models in Stan and RStan</a>
<ul>
<li class="chapter" data-level="13.1" data-path="bayesian-ar-arma-and-arima-models-in-stan-and-rstan.html"><a href="bayesian-ar-arma-and-arima-models-in-stan-and-rstan.html#bayesian-ar1-model"><i class="fa fa-check"></i><b>13.1</b> Bayesian AR(1) Model</a></li>
<li class="chapter" data-level="13.2" data-path="bayesian-ar-arma-and-arima-models-in-stan-and-rstan.html"><a href="bayesian-ar-arma-and-arima-models-in-stan-and-rstan.html#bayesian-arma11-model"><i class="fa fa-check"></i><b>13.2</b> Bayesian ARMA(1,1) Model</a></li>
<li class="chapter" data-level="13.3" data-path="bayesian-ar-arma-and-arima-models-in-stan-and-rstan.html"><a href="bayesian-ar-arma-and-arima-models-in-stan-and-rstan.html#bayesian-arima111-model"><i class="fa fa-check"></i><b>13.3</b> Bayesian ARIMA(1,1,1) Model</a></li>
<li class="chapter" data-level="13.4" data-path="bayesian-ar-arma-and-arima-models-in-stan-and-rstan.html"><a href="bayesian-ar-arma-and-arima-models-in-stan-and-rstan.html#conclusion-9"><i class="fa fa-check"></i><b>13.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="bayesian-structural-time-series-models.html"><a href="bayesian-structural-time-series-models.html"><i class="fa fa-check"></i><b>14</b> 14 Bayesian Structural Time Series Models</a>
<ul>
<li class="chapter" data-level="14.1" data-path="bayesian-structural-time-series-models.html"><a href="bayesian-structural-time-series-models.html#introduction-6"><i class="fa fa-check"></i><b>14.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="bayesian-structural-time-series-models.html"><a href="bayesian-structural-time-series-models.html#stan-implementation-basic-bsts-model"><i class="fa fa-check"></i><b>14.1.1</b> Stan Implementation: Basic BSTS Model</a></li>
<li class="chapter" data-level="14.1.2" data-path="bayesian-structural-time-series-models.html"><a href="bayesian-structural-time-series-models.html#stan-implementation-time-varying-trend-volatility"><i class="fa fa-check"></i><b>14.1.2</b> Stan Implementation: Time-Varying Trend Volatility</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="bayesian-structural-time-series-models.html"><a href="bayesian-structural-time-series-models.html#model-comparison-and-selection"><i class="fa fa-check"></i><b>14.2</b> Model Comparison and Selection</a></li>
<li class="chapter" data-level="14.3" data-path="bayesian-structural-time-series-models.html"><a href="bayesian-structural-time-series-models.html#conclusion-10"><i class="fa fa-check"></i><b>14.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="model-comparison-and-selection-in-bayesian-analysis.html"><a href="model-comparison-and-selection-in-bayesian-analysis.html"><i class="fa fa-check"></i><b>15</b> Model Comparison and Selection in Bayesian Analysis</a>
<ul>
<li class="chapter" data-level="15.1" data-path="model-comparison-and-selection-in-bayesian-analysis.html"><a href="model-comparison-and-selection-in-bayesian-analysis.html#introduction-7"><i class="fa fa-check"></i><b>15.1</b> Introduction</a></li>
<li class="chapter" data-level="15.2" data-path="model-comparison-and-selection-in-bayesian-analysis.html"><a href="model-comparison-and-selection-in-bayesian-analysis.html#posterior-predictive-checks"><i class="fa fa-check"></i><b>15.2</b> Posterior Predictive Checks</a></li>
<li class="chapter" data-level="15.3" data-path="model-comparison-and-selection-in-bayesian-analysis.html"><a href="model-comparison-and-selection-in-bayesian-analysis.html#information-criteria-waic-and-loo-cv"><i class="fa fa-check"></i><b>15.3</b> Information Criteria: WAIC and LOO-CV</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="model-comparison-and-selection-in-bayesian-analysis.html"><a href="model-comparison-and-selection-in-bayesian-analysis.html#waic-watanabe-akaike-information-criterion"><i class="fa fa-check"></i><b>15.3.1</b> WAIC (Watanabe-Akaike Information Criterion)</a></li>
<li class="chapter" data-level="15.3.2" data-path="model-comparison-and-selection-in-bayesian-analysis.html"><a href="model-comparison-and-selection-in-bayesian-analysis.html#loo-cv-leave-one-out-cross-validation"><i class="fa fa-check"></i><b>15.3.2</b> LOO-CV (Leave-One-Out Cross-Validation)</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="model-comparison-and-selection-in-bayesian-analysis.html"><a href="model-comparison-and-selection-in-bayesian-analysis.html#bayes-factors-vs.-information-criteria"><i class="fa fa-check"></i><b>15.4</b> Bayes Factors vs. Information Criteria</a></li>
<li class="chapter" data-level="15.5" data-path="model-comparison-and-selection-in-bayesian-analysis.html"><a href="model-comparison-and-selection-in-bayesian-analysis.html#summary-of-bayesian-model-comparison-methods"><i class="fa fa-check"></i><b>15.5</b> Summary of Bayesian Model Comparison Methods</a></li>
<li class="chapter" data-level="15.6" data-path="model-comparison-and-selection-in-bayesian-analysis.html"><a href="model-comparison-and-selection-in-bayesian-analysis.html#practical-example-comparing-models-in-r"><i class="fa fa-check"></i><b>15.6</b> Practical Example: Comparing Models in R</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>16</b> Appendix</a>
<ul>
<li class="chapter" data-level="16.1" data-path="appendix.html"><a href="appendix.html#main-stan-distributions-cheatsheet"><i class="fa fa-check"></i><b>16.1</b> Main Stan Distributions Cheatsheet</a></li>
<li class="chapter" data-level="16.2" data-path="appendix.html"><a href="appendix.html#main-stan-functions-cheatsheet"><i class="fa fa-check"></i><b>16.2</b> Main Stan Functions Cheatsheet</a></li>
<li class="chapter" data-level="16.3" data-path="appendix.html"><a href="appendix.html#why-non-distribution-functions"><i class="fa fa-check"></i><b>16.3</b> Why Non-Distribution Functions?</a></li>
<li class="chapter" data-level="16.4" data-path="appendix.html"><a href="appendix.html#stan-functions-cheatsheet"><i class="fa fa-check"></i><b>16.4</b> Stan Functions Cheatsheet</a>
<ul>
<li class="chapter" data-level="16.4.1" data-path="appendix.html"><a href="appendix.html#mathematical-functions"><i class="fa fa-check"></i><b>16.4.1</b> 1. Mathematical Functions</a></li>
<li class="chapter" data-level="16.4.2" data-path="appendix.html"><a href="appendix.html#transformation-functions"><i class="fa fa-check"></i><b>16.4.2</b> 2. Transformation Functions</a></li>
<li class="chapter" data-level="16.4.3" data-path="appendix.html"><a href="appendix.html#matrix-and-vector-operations"><i class="fa fa-check"></i><b>16.4.3</b> 3. Matrix and Vector Operations</a></li>
<li class="chapter" data-level="16.4.4" data-path="appendix.html"><a href="appendix.html#utility-functions"><i class="fa fa-check"></i><b>16.4.4</b> 4. Utility Functions</a></li>
<li class="chapter" data-level="16.4.5" data-path="appendix.html"><a href="appendix.html#specialized-solvers"><i class="fa fa-check"></i><b>16.4.5</b> 5. Specialized Solvers</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="appendix.html"><a href="appendix.html#example-hierarchical-linear-regression"><i class="fa fa-check"></i><b>16.5</b> Example: Hierarchical Linear Regression</a></li>
<li class="chapter" data-level="16.6" data-path="appendix.html"><a href="appendix.html#tips-for-using-stan-functions"><i class="fa fa-check"></i><b>16.6</b> Tips for Using Stan Functions</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Stan Bookdown</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian-regularized-regression" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> Bayesian Regularized Regression<a href="bayesian-regularized-regression.html#bayesian-regularized-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction-1" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Introduction<a href="bayesian-regularized-regression.html#introduction-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this post, we will explore Bayesian analogues of regularized linear regression models such as LASSO and Ridge regression, which extend traditional linear regression to handle challenging modeling scenarios. These methods prove particularly valuable for improving prediction accuracy, estimating regression models with many variables, and providing alternatives to traditional model selection approaches.</p>
<p>The foundation of our discussion rests on the standard linear regression model <span class="math inline">\(\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}\)</span>, where <span class="math inline">\(\mathbf{y} \in \mathbb{R}^n\)</span> represents the response vector, <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times p}\)</span> is the design matrix, <span class="math inline">\(\boldsymbol{\beta} \in \mathbb{R}^p\)</span> contains the coefficients of interest, and <span class="math inline">\(\boldsymbol{\epsilon} \sim \mathcal{N}(0, \sigma^2\mathbf{I})\)</span> captures the random error. Traditional linear regression treats coefficients as independent, producing what statisticians call “unbiased” estimates. However, this independence assumption can lead to poor performance in settings with limited data, high-dimensional predictor spaces, or substantial noise.</p>
<p>Regularized linear regression models deliberately introduce bias into coefficient estimates by pooling information across parameters, causing them to shrink toward a common value, typically zero. This shrinkage represents a fundamental trade-off captured by the bias-variance decomposition: <span class="math inline">\(\text{MSE}(\hat{\boldsymbol{\beta}}) = \text{Bias}^2(\hat{\boldsymbol{\beta}}) + \text{Var}(\hat{\boldsymbol{\beta}})\)</span>. While regularization increases bias, it often reduces variance substantially enough that the overall mean squared error decreases, particularly in high-dimensional or noisy settings where traditional methods tend to overfit.</p>
<p>The Bayesian perspective reframes regularization as a problem of specifying appropriate prior distributions for model parameters. Rather than viewing penalty terms as arbitrary constraints on optimization, Bayesian regularization emerges naturally from the choice of prior distribution over coefficients. This connection provides both theoretical insight into why regularization works and practical advantages through full posterior distributions for all parameters.</p>
</div>
<div id="ridge-regression-and-gaussian-priors" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Ridge Regression and Gaussian Priors<a href="bayesian-regularized-regression.html#ridge-regression-and-gaussian-priors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Ridge regression modifies the ordinary least squares objective by adding a penalty proportional to the sum of squared coefficients. In the frequentist framework, ridge regression minimizes <span class="math inline">\(\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|_2^2 + \lambda \|\boldsymbol{\beta}\|_2^2\)</span>, yielding the closed-form solution <span class="math inline">\(\hat{\boldsymbol{\beta}}_{\text{ridge}} = (\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\)</span>. The penalty parameter <span class="math inline">\(\lambda\)</span> controls the strength of regularization, with larger values producing more shrinkage toward zero.</p>
<p>The Bayesian equivalent emerges by placing independent Gaussian priors on each coefficient: <span class="math inline">\(\beta_j \sim \mathcal{N}(0, \tau^2)\)</span> for <span class="math inline">\(j = 1, \ldots, p\)</span>. Under this prior specification, the posterior distribution becomes <span class="math inline">\(p(\boldsymbol{\beta}|\mathbf{y}, \mathbf{X}, \sigma^2, \tau^2) \propto \exp\left(-\frac{1}{2\sigma^2}\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|_2^2 - \frac{1}{2\tau^2}\|\boldsymbol{\beta}\|_2^2\right)\)</span>. The mathematical equivalence becomes clear when we recognize that <span class="math inline">\(\lambda = \sigma^2/\tau^2\)</span>, establishing a direct correspondence between the frequentist penalty parameter and the ratio of error variance to prior variance.</p>
<p>This Bayesian formulation offers several advantages over its frequentist counterpart. Most importantly, it provides full posterior distributions for all parameters rather than just point estimates, enabling principled uncertainty quantification without requiring bootstrap procedures. The prior variance <span class="math inline">\(\tau^2\)</span> also admits a natural hierarchical extension by treating it as an unknown parameter with its own prior distribution, allowing the data to inform the appropriate level of shrinkage.</p>
</div>
<div id="lasso-regression-and-laplace-priors" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> LASSO Regression and Laplace Priors<a href="bayesian-regularized-regression.html#lasso-regression-and-laplace-priors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>LASSO regression replaces the ridge penalty with an L1 penalty on the coefficient magnitudes, minimizing <span class="math inline">\(\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|_2^2 + \lambda \|\boldsymbol{\beta}\|_1\)</span>. This seemingly minor modification produces dramatically different behavior: while ridge regression shrinks coefficients toward zero, LASSO can set them exactly to zero, performing automatic variable selection. The geometric intuition reveals that the L1 penalty creates a diamond-shaped constraint region whose corners lie on the coordinate axes, making exact zeros more likely when the unconstrained optimum lies in particular regions of the parameter space.</p>
<p>The Bayesian LASSO emerges by specifying independent Laplace priors for each coefficient: <span class="math inline">\(\beta_j \sim \text{Laplace}(0, b)\)</span> for <span class="math inline">\(j = 1, \ldots, p\)</span>. The Laplace density <span class="math inline">\(p(\beta_j|b) = \frac{1}{2b}\exp\left(-\frac{|\beta_j|}{b}\right)\)</span> places much more probability mass at zero compared to a Gaussian distribution with the same variance, naturally encouraging sparsity. The relationship between the frequentist penalty and Bayesian scale parameter follows <span class="math inline">\(\lambda = \sigma^2/b\)</span>.</p>
<p>While the Bayesian LASSO provides the same point estimates as its frequentist counterpart through the posterior mode, its real advantage lies in proper uncertainty quantification for the selected variables. The full posterior distribution accounts for both parameter uncertainty and model uncertainty, providing more honest assessments of prediction intervals and coefficient significance than frequentist methods that condition on the selected model.</p>
</div>
<div id="hierarchical-shrinkage-and-adaptive-priors" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Hierarchical Shrinkage and Adaptive Priors<a href="bayesian-regularized-regression.html#hierarchical-shrinkage-and-adaptive-priors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Both ridge and LASSO impose uniform shrinkage across all coefficients, but real data often contains a mixture of large, moderate, and negligible effects that would benefit from adaptive shrinkage. Hierarchical shrinkage priors address this limitation by allowing different coefficients to experience different amounts of shrinkage based on their apparent signal strength in the data.</p>
<p>The general framework specifies <span class="math inline">\(\beta_j \sim \mathcal{N}(0, \tau^2 \lambda_j^2)\)</span> for <span class="math inline">\(j = 1, \ldots, p\)</span>, where <span class="math inline">\(\tau\)</span> represents a global shrinkage parameter affecting all coefficients and <span class="math inline">\(\lambda_j\)</span> represents local shrinkage parameters that can vary across coefficients. This hierarchical structure enables the model to shrink small coefficients aggressively while leaving large coefficients relatively unshrunk.</p>
<p>The horseshoe prior represents one particularly successful implementation of this idea, specifying <span class="math inline">\(\lambda_j \sim \text{Cauchy}^+(0, 1)\)</span> and <span class="math inline">\(\tau \sim \text{Cauchy}^+(0, 1)\)</span>. The Cauchy distribution’s heavy tails ensure that large coefficients can escape shrinkage while still encouraging small coefficients toward zero. The horseshoe prior gets its name from the shape of its shrinkage function, which resembles an inverted horseshoe when plotted against the signal-to-noise ratio.</p>
</div>
<div id="implementation" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Implementation<a href="bayesian-regularized-regression.html#implementation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="bayesian-regularized-regression.html#cb71-1" tabindex="-1"></a><span class="fu">library</span>(tidymodels)</span></code></pre></div>
<pre><code>## ── Attaching packages ───────────────────────────────────────────────────────────────────────────────────────────────────────── tidymodels 1.3.0 ──</code></pre>
<pre><code>## ✔ broom        1.0.8     ✔ recipes      1.3.1
## ✔ dials        1.4.0     ✔ rsample      1.3.1
## ✔ dplyr        1.1.4     ✔ tibble       3.2.1
## ✔ ggplot2      3.5.2     ✔ tidyr        1.3.1
## ✔ infer        1.0.9     ✔ tune         1.3.0
## ✔ modeldata    1.4.0     ✔ workflows    1.2.0
## ✔ parsnip      1.3.2     ✔ workflowsets 1.1.1
## ✔ purrr        1.0.4     ✔ yardstick    1.3.2</code></pre>
<pre><code>## Warning: package &#39;infer&#39; was built under R version 4.5.1</code></pre>
<pre><code>## Warning: package &#39;rsample&#39; was built under R version 4.5.1</code></pre>
<pre><code>## ── Conflicts ──────────────────────────────────────────────────────────────────────────────────────────────────────────── tidymodels_conflicts() ──
## ✖ purrr::discard() masks scales::discard()
## ✖ dplyr::filter()  masks stats::filter()
## ✖ dplyr::lag()     masks stats::lag()
## ✖ recipes::step()  masks stats::step()
## • Dig deeper into tidy modeling with R at https://www.tmwr.org</code></pre>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="bayesian-regularized-regression.html#cb77-1" tabindex="-1"></a><span class="fu">library</span>(rstan)</span></code></pre></div>
<pre><code>## Loading required package: StanHeaders</code></pre>
<pre><code>## 
## rstan version 2.32.7 (Stan version 2.32.2)</code></pre>
<pre><code>## For execution on a local, multicore CPU with excess RAM we recommend calling
## options(mc.cores = parallel::detectCores()).
## To avoid recompilation of unchanged Stan programs, we recommend calling
## rstan_options(auto_write = TRUE)
## For within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,
## change `threads_per_chain` option:
## rstan_options(threads_per_chain = 1)</code></pre>
<pre><code>## 
## Attaching package: &#39;rstan&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:tidyr&#39;:
## 
##     extract</code></pre>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="bayesian-regularized-regression.html#cb83-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code></pre></div>
<pre><code>## ── Attaching core tidyverse packages ─────────────────────────────────────────────────────────────────────────────────────────── tidyverse 2.0.0 ──
## ✔ forcats   1.0.0     ✔ readr     2.1.5
## ✔ lubridate 1.9.4     ✔ stringr   1.5.1</code></pre>
<pre><code>## ── Conflicts ───────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ readr::col_factor() masks scales::col_factor()
## ✖ purrr::discard()    masks scales::discard()
## ✖ rstan::extract()    masks tidyr::extract()
## ✖ dplyr::filter()     masks stats::filter()
## ✖ stringr::fixed()    masks recipes::fixed()
## ✖ dplyr::lag()        masks stats::lag()
## ✖ readr::spec()       masks yardstick::spec()
## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors</code></pre>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="bayesian-regularized-regression.html#cb86-1" tabindex="-1"></a><span class="co"># Data simulation</span></span>
<span id="cb86-2"><a href="bayesian-regularized-regression.html#cb86-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb86-3"><a href="bayesian-regularized-regression.html#cb86-3" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb86-4"><a href="bayesian-regularized-regression.html#cb86-4" tabindex="-1"></a>n_train <span class="ot">&lt;-</span> <span class="dv">700</span></span>
<span id="cb86-5"><a href="bayesian-regularized-regression.html#cb86-5" tabindex="-1"></a>n_test <span class="ot">&lt;-</span> <span class="dv">300</span></span>
<span id="cb86-6"><a href="bayesian-regularized-regression.html#cb86-6" tabindex="-1"></a>alpha_true <span class="ot">&lt;-</span> <span class="dv">40</span>  <span class="co"># intercept</span></span>
<span id="cb86-7"><a href="bayesian-regularized-regression.html#cb86-7" tabindex="-1"></a>beta_true <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">1</span>, <span class="fl">0.25</span>)  <span class="co"># slopes</span></span>
<span id="cb86-8"><a href="bayesian-regularized-regression.html#cb86-8" tabindex="-1"></a>sigma_true <span class="ot">&lt;-</span> <span class="dv">5</span>  <span class="co"># residual standard deviation</span></span>
<span id="cb86-9"><a href="bayesian-regularized-regression.html#cb86-9" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">length</span>(beta_true)</span>
<span id="cb86-10"><a href="bayesian-regularized-regression.html#cb86-10" tabindex="-1"></a></span>
<span id="cb86-11"><a href="bayesian-regularized-regression.html#cb86-11" tabindex="-1"></a><span class="co"># Generate design matrix and response</span></span>
<span id="cb86-12"><a href="bayesian-regularized-regression.html#cb86-12" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> p), n, p)</span>
<span id="cb86-13"><a href="bayesian-regularized-regression.html#cb86-13" tabindex="-1"></a>epsilon <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> sigma_true)</span>
<span id="cb86-14"><a href="bayesian-regularized-regression.html#cb86-14" tabindex="-1"></a>y <span class="ot">&lt;-</span> alpha_true <span class="sc">+</span> X <span class="sc">%*%</span> beta_true <span class="sc">+</span> epsilon</span>
<span id="cb86-15"><a href="bayesian-regularized-regression.html#cb86-15" tabindex="-1"></a></span>
<span id="cb86-16"><a href="bayesian-regularized-regression.html#cb86-16" tabindex="-1"></a><span class="co"># Create dataset</span></span>
<span id="cb86-17"><a href="bayesian-regularized-regression.html#cb86-17" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">y =</span> y, X)</span>
<span id="cb86-18"><a href="bayesian-regularized-regression.html#cb86-18" tabindex="-1"></a><span class="fu">colnames</span>(data) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;y&quot;</span>, <span class="fu">paste0</span>(<span class="st">&quot;X&quot;</span>, <span class="dv">1</span><span class="sc">:</span>p))</span>
<span id="cb86-19"><a href="bayesian-regularized-regression.html#cb86-19" tabindex="-1"></a></span>
<span id="cb86-20"><a href="bayesian-regularized-regression.html#cb86-20" tabindex="-1"></a><span class="co"># Train-test split</span></span>
<span id="cb86-21"><a href="bayesian-regularized-regression.html#cb86-21" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb86-22"><a href="bayesian-regularized-regression.html#cb86-22" tabindex="-1"></a>data_split <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(data, <span class="at">prop =</span> <span class="fl">0.7</span>)</span>
<span id="cb86-23"><a href="bayesian-regularized-regression.html#cb86-23" tabindex="-1"></a>train_data <span class="ot">&lt;-</span> <span class="fu">training</span>(data_split)</span>
<span id="cb86-24"><a href="bayesian-regularized-regression.html#cb86-24" tabindex="-1"></a>test_data <span class="ot">&lt;-</span> <span class="fu">testing</span>(data_split)</span>
<span id="cb86-25"><a href="bayesian-regularized-regression.html#cb86-25" tabindex="-1"></a></span>
<span id="cb86-26"><a href="bayesian-regularized-regression.html#cb86-26" tabindex="-1"></a><span class="co"># Prepare data for Stan</span></span>
<span id="cb86-27"><a href="bayesian-regularized-regression.html#cb86-27" tabindex="-1"></a>X_train <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(train_data[, <span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb86-28"><a href="bayesian-regularized-regression.html#cb86-28" tabindex="-1"></a>X_test <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(test_data[, <span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb86-29"><a href="bayesian-regularized-regression.html#cb86-29" tabindex="-1"></a>y_train <span class="ot">&lt;-</span> train_data<span class="sc">$</span>y</span>
<span id="cb86-30"><a href="bayesian-regularized-regression.html#cb86-30" tabindex="-1"></a>y_test <span class="ot">&lt;-</span> test_data<span class="sc">$</span>y</span></code></pre></div>
<div id="bayesian-ridge-regression" class="section level3 hasAnchor" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> Bayesian Ridge Regression<a href="bayesian-regularized-regression.html#bayesian-ridge-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="bayesian-regularized-regression.html#cb87-1" tabindex="-1"></a><span class="co"># Stan model for Bayesian Ridge Regression</span></span>
<span id="cb87-2"><a href="bayesian-regularized-regression.html#cb87-2" tabindex="-1"></a>stan_ridge <span class="ot">&lt;-</span> <span class="st">&quot;</span></span>
<span id="cb87-3"><a href="bayesian-regularized-regression.html#cb87-3" tabindex="-1"></a><span class="st">data {</span></span>
<span id="cb87-4"><a href="bayesian-regularized-regression.html#cb87-4" tabindex="-1"></a><span class="st">  int&lt;lower=0&gt; N_train;</span></span>
<span id="cb87-5"><a href="bayesian-regularized-regression.html#cb87-5" tabindex="-1"></a><span class="st">  int&lt;lower=0&gt; N_test;</span></span>
<span id="cb87-6"><a href="bayesian-regularized-regression.html#cb87-6" tabindex="-1"></a><span class="st">  int&lt;lower=0&gt; p;</span></span>
<span id="cb87-7"><a href="bayesian-regularized-regression.html#cb87-7" tabindex="-1"></a><span class="st">  matrix[N_train, p] X_train;</span></span>
<span id="cb87-8"><a href="bayesian-regularized-regression.html#cb87-8" tabindex="-1"></a><span class="st">  matrix[N_test, p] X_test;</span></span>
<span id="cb87-9"><a href="bayesian-regularized-regression.html#cb87-9" tabindex="-1"></a><span class="st">  vector[N_train] y_train;</span></span>
<span id="cb87-10"><a href="bayesian-regularized-regression.html#cb87-10" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb87-11"><a href="bayesian-regularized-regression.html#cb87-11" tabindex="-1"></a><span class="st">parameters {</span></span>
<span id="cb87-12"><a href="bayesian-regularized-regression.html#cb87-12" tabindex="-1"></a><span class="st">  real alpha;</span></span>
<span id="cb87-13"><a href="bayesian-regularized-regression.html#cb87-13" tabindex="-1"></a><span class="st">  vector[p] beta;</span></span>
<span id="cb87-14"><a href="bayesian-regularized-regression.html#cb87-14" tabindex="-1"></a><span class="st">  real&lt;lower=0&gt; sigma;</span></span>
<span id="cb87-15"><a href="bayesian-regularized-regression.html#cb87-15" tabindex="-1"></a><span class="st">  real&lt;lower=0&gt; tau;  // prior standard deviation for beta</span></span>
<span id="cb87-16"><a href="bayesian-regularized-regression.html#cb87-16" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb87-17"><a href="bayesian-regularized-regression.html#cb87-17" tabindex="-1"></a><span class="st">model {</span></span>
<span id="cb87-18"><a href="bayesian-regularized-regression.html#cb87-18" tabindex="-1"></a><span class="st">  // Priors</span></span>
<span id="cb87-19"><a href="bayesian-regularized-regression.html#cb87-19" tabindex="-1"></a><span class="st">  alpha ~ normal(0, 10);</span></span>
<span id="cb87-20"><a href="bayesian-regularized-regression.html#cb87-20" tabindex="-1"></a><span class="st">  tau ~ cauchy(0, 1);</span></span>
<span id="cb87-21"><a href="bayesian-regularized-regression.html#cb87-21" tabindex="-1"></a><span class="st">  sigma ~ cauchy(0, 5);</span></span>
<span id="cb87-22"><a href="bayesian-regularized-regression.html#cb87-22" tabindex="-1"></a><span class="st">  beta ~ normal(0, tau);  // Ridge prior</span></span>
<span id="cb87-23"><a href="bayesian-regularized-regression.html#cb87-23" tabindex="-1"></a><span class="st">  </span></span>
<span id="cb87-24"><a href="bayesian-regularized-regression.html#cb87-24" tabindex="-1"></a><span class="st">  // Likelihood</span></span>
<span id="cb87-25"><a href="bayesian-regularized-regression.html#cb87-25" tabindex="-1"></a><span class="st">  y_train ~ normal(alpha + X_train * beta, sigma);</span></span>
<span id="cb87-26"><a href="bayesian-regularized-regression.html#cb87-26" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb87-27"><a href="bayesian-regularized-regression.html#cb87-27" tabindex="-1"></a><span class="st">generated quantities {</span></span>
<span id="cb87-28"><a href="bayesian-regularized-regression.html#cb87-28" tabindex="-1"></a><span class="st">  vector[N_test] y_pred;</span></span>
<span id="cb87-29"><a href="bayesian-regularized-regression.html#cb87-29" tabindex="-1"></a><span class="st">  </span></span>
<span id="cb87-30"><a href="bayesian-regularized-regression.html#cb87-30" tabindex="-1"></a><span class="st">  for (i in 1:N_test) {</span></span>
<span id="cb87-31"><a href="bayesian-regularized-regression.html#cb87-31" tabindex="-1"></a><span class="st">    y_pred[i] = normal_rng(alpha + X_test[i] * beta, sigma);</span></span>
<span id="cb87-32"><a href="bayesian-regularized-regression.html#cb87-32" tabindex="-1"></a><span class="st">  }</span></span>
<span id="cb87-33"><a href="bayesian-regularized-regression.html#cb87-33" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb87-34"><a href="bayesian-regularized-regression.html#cb87-34" tabindex="-1"></a><span class="st">&quot;</span></span>
<span id="cb87-35"><a href="bayesian-regularized-regression.html#cb87-35" tabindex="-1"></a></span>
<span id="cb87-36"><a href="bayesian-regularized-regression.html#cb87-36" tabindex="-1"></a><span class="co"># Prepare data</span></span>
<span id="cb87-37"><a href="bayesian-regularized-regression.html#cb87-37" tabindex="-1"></a>stan_data_ridge <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb87-38"><a href="bayesian-regularized-regression.html#cb87-38" tabindex="-1"></a>  <span class="at">N_train =</span> n_train,</span>
<span id="cb87-39"><a href="bayesian-regularized-regression.html#cb87-39" tabindex="-1"></a>  <span class="at">N_test =</span> n_test,</span>
<span id="cb87-40"><a href="bayesian-regularized-regression.html#cb87-40" tabindex="-1"></a>  <span class="at">p =</span> p,</span>
<span id="cb87-41"><a href="bayesian-regularized-regression.html#cb87-41" tabindex="-1"></a>  <span class="at">X_train =</span> X_train,</span>
<span id="cb87-42"><a href="bayesian-regularized-regression.html#cb87-42" tabindex="-1"></a>  <span class="at">X_test =</span> X_test,</span>
<span id="cb87-43"><a href="bayesian-regularized-regression.html#cb87-43" tabindex="-1"></a>  <span class="at">y_train =</span> y_train</span>
<span id="cb87-44"><a href="bayesian-regularized-regression.html#cb87-44" tabindex="-1"></a>)</span>
<span id="cb87-45"><a href="bayesian-regularized-regression.html#cb87-45" tabindex="-1"></a></span>
<span id="cb87-46"><a href="bayesian-regularized-regression.html#cb87-46" tabindex="-1"></a><span class="co"># Fit model</span></span>
<span id="cb87-47"><a href="bayesian-regularized-regression.html#cb87-47" tabindex="-1"></a>fit_ridge <span class="ot">&lt;-</span> <span class="fu">stan</span>(<span class="at">model_code =</span> stan_ridge, </span>
<span id="cb87-48"><a href="bayesian-regularized-regression.html#cb87-48" tabindex="-1"></a>                  <span class="at">data =</span> stan_data_ridge,</span>
<span id="cb87-49"><a href="bayesian-regularized-regression.html#cb87-49" tabindex="-1"></a>                  <span class="at">chains =</span> <span class="dv">4</span>, <span class="at">iter =</span> <span class="dv">2000</span>,</span>
<span id="cb87-50"><a href="bayesian-regularized-regression.html#cb87-50" tabindex="-1"></a>                  <span class="at">cores =</span> parallel<span class="sc">::</span><span class="fu">detectCores</span>())</span>
<span id="cb87-51"><a href="bayesian-regularized-regression.html#cb87-51" tabindex="-1"></a></span>
<span id="cb87-52"><a href="bayesian-regularized-regression.html#cb87-52" tabindex="-1"></a><span class="fu">print</span>(fit_ridge, <span class="at">pars =</span> <span class="fu">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;tau&quot;</span>))</span></code></pre></div>
<pre><code>## Inference for Stan model: anon_model.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##          mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat
## alpha   40.32    0.00 0.18 39.96 40.19 40.31 40.44 40.67  7148    1
## beta[1] -1.86    0.00 0.19 -2.24 -1.99 -1.86 -1.73 -1.48  6679    1
## beta[2]  2.64    0.00 0.18  2.28  2.52  2.64  2.76  2.99  5238    1
## beta[3]  3.91    0.00 0.20  3.53  3.77  3.91  4.04  4.28  6158    1
## beta[4]  1.09    0.00 0.19  0.70  0.97  1.10  1.22  1.47  6568    1
## beta[5]  0.32    0.00 0.19 -0.04  0.20  0.32  0.45  0.70  6110    1
## sigma    4.97    0.00 0.13  4.72  4.88  4.97  5.06  5.24  7300    1
## tau      2.53    0.02 0.94  1.42  1.92  2.32  2.88  4.87  2591    1
## 
## Samples were drawn using NUTS(diag_e) at Thu Oct 16 08:50:41 2025.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
</div>
<div id="bayesian-lasso-regression" class="section level3 hasAnchor" number="5.5.2">
<h3><span class="header-section-number">5.5.2</span> Bayesian LASSO Regression<a href="bayesian-regularized-regression.html#bayesian-lasso-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="bayesian-regularized-regression.html#cb89-1" tabindex="-1"></a><span class="co"># Stan model for Bayesian LASSO</span></span>
<span id="cb89-2"><a href="bayesian-regularized-regression.html#cb89-2" tabindex="-1"></a>stan_lasso <span class="ot">&lt;-</span> <span class="st">&quot;</span></span>
<span id="cb89-3"><a href="bayesian-regularized-regression.html#cb89-3" tabindex="-1"></a><span class="st">data {</span></span>
<span id="cb89-4"><a href="bayesian-regularized-regression.html#cb89-4" tabindex="-1"></a><span class="st">  int&lt;lower=0&gt; N_train;</span></span>
<span id="cb89-5"><a href="bayesian-regularized-regression.html#cb89-5" tabindex="-1"></a><span class="st">  int&lt;lower=0&gt; N_test;</span></span>
<span id="cb89-6"><a href="bayesian-regularized-regression.html#cb89-6" tabindex="-1"></a><span class="st">  int&lt;lower=0&gt; p;</span></span>
<span id="cb89-7"><a href="bayesian-regularized-regression.html#cb89-7" tabindex="-1"></a><span class="st">  matrix[N_train, p] X_train;</span></span>
<span id="cb89-8"><a href="bayesian-regularized-regression.html#cb89-8" tabindex="-1"></a><span class="st">  matrix[N_test, p] X_test;</span></span>
<span id="cb89-9"><a href="bayesian-regularized-regression.html#cb89-9" tabindex="-1"></a><span class="st">  vector[N_train] y_train;</span></span>
<span id="cb89-10"><a href="bayesian-regularized-regression.html#cb89-10" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb89-11"><a href="bayesian-regularized-regression.html#cb89-11" tabindex="-1"></a><span class="st">parameters {</span></span>
<span id="cb89-12"><a href="bayesian-regularized-regression.html#cb89-12" tabindex="-1"></a><span class="st">  real alpha;</span></span>
<span id="cb89-13"><a href="bayesian-regularized-regression.html#cb89-13" tabindex="-1"></a><span class="st">  vector[p] beta;</span></span>
<span id="cb89-14"><a href="bayesian-regularized-regression.html#cb89-14" tabindex="-1"></a><span class="st">  real&lt;lower=0&gt; sigma;</span></span>
<span id="cb89-15"><a href="bayesian-regularized-regression.html#cb89-15" tabindex="-1"></a><span class="st">  real&lt;lower=0&gt; b;  // Laplace scale parameter</span></span>
<span id="cb89-16"><a href="bayesian-regularized-regression.html#cb89-16" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb89-17"><a href="bayesian-regularized-regression.html#cb89-17" tabindex="-1"></a><span class="st">model {</span></span>
<span id="cb89-18"><a href="bayesian-regularized-regression.html#cb89-18" tabindex="-1"></a><span class="st">  // Priors</span></span>
<span id="cb89-19"><a href="bayesian-regularized-regression.html#cb89-19" tabindex="-1"></a><span class="st">  alpha ~ normal(0, 10);</span></span>
<span id="cb89-20"><a href="bayesian-regularized-regression.html#cb89-20" tabindex="-1"></a><span class="st">  b ~ cauchy(0, 1);</span></span>
<span id="cb89-21"><a href="bayesian-regularized-regression.html#cb89-21" tabindex="-1"></a><span class="st">  sigma ~ cauchy(0, 5);</span></span>
<span id="cb89-22"><a href="bayesian-regularized-regression.html#cb89-22" tabindex="-1"></a><span class="st">  beta ~ double_exponential(0, b);  // LASSO prior</span></span>
<span id="cb89-23"><a href="bayesian-regularized-regression.html#cb89-23" tabindex="-1"></a><span class="st">  </span></span>
<span id="cb89-24"><a href="bayesian-regularized-regression.html#cb89-24" tabindex="-1"></a><span class="st">  // Likelihood</span></span>
<span id="cb89-25"><a href="bayesian-regularized-regression.html#cb89-25" tabindex="-1"></a><span class="st">  y_train ~ normal(alpha + X_train * beta, sigma);</span></span>
<span id="cb89-26"><a href="bayesian-regularized-regression.html#cb89-26" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb89-27"><a href="bayesian-regularized-regression.html#cb89-27" tabindex="-1"></a><span class="st">generated quantities {</span></span>
<span id="cb89-28"><a href="bayesian-regularized-regression.html#cb89-28" tabindex="-1"></a><span class="st">  vector[N_test] y_pred;</span></span>
<span id="cb89-29"><a href="bayesian-regularized-regression.html#cb89-29" tabindex="-1"></a><span class="st">  </span></span>
<span id="cb89-30"><a href="bayesian-regularized-regression.html#cb89-30" tabindex="-1"></a><span class="st">  for (i in 1:N_test) {</span></span>
<span id="cb89-31"><a href="bayesian-regularized-regression.html#cb89-31" tabindex="-1"></a><span class="st">    y_pred[i] = normal_rng(alpha + X_test[i] * beta, sigma);</span></span>
<span id="cb89-32"><a href="bayesian-regularized-regression.html#cb89-32" tabindex="-1"></a><span class="st">  }</span></span>
<span id="cb89-33"><a href="bayesian-regularized-regression.html#cb89-33" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb89-34"><a href="bayesian-regularized-regression.html#cb89-34" tabindex="-1"></a><span class="st">&quot;</span></span>
<span id="cb89-35"><a href="bayesian-regularized-regression.html#cb89-35" tabindex="-1"></a></span>
<span id="cb89-36"><a href="bayesian-regularized-regression.html#cb89-36" tabindex="-1"></a><span class="co"># Prepare data (same as Ridge)</span></span>
<span id="cb89-37"><a href="bayesian-regularized-regression.html#cb89-37" tabindex="-1"></a>stan_data_lasso <span class="ot">&lt;-</span> stan_data_ridge</span>
<span id="cb89-38"><a href="bayesian-regularized-regression.html#cb89-38" tabindex="-1"></a></span>
<span id="cb89-39"><a href="bayesian-regularized-regression.html#cb89-39" tabindex="-1"></a><span class="co"># Fit model</span></span>
<span id="cb89-40"><a href="bayesian-regularized-regression.html#cb89-40" tabindex="-1"></a>fit_lasso <span class="ot">&lt;-</span> <span class="fu">stan</span>(<span class="at">model_code =</span> stan_lasso,</span>
<span id="cb89-41"><a href="bayesian-regularized-regression.html#cb89-41" tabindex="-1"></a>                  <span class="at">data =</span> stan_data_lasso,</span>
<span id="cb89-42"><a href="bayesian-regularized-regression.html#cb89-42" tabindex="-1"></a>                  <span class="at">chains =</span> <span class="dv">4</span>, <span class="at">iter =</span> <span class="dv">2000</span>,</span>
<span id="cb89-43"><a href="bayesian-regularized-regression.html#cb89-43" tabindex="-1"></a>                  <span class="at">cores =</span> parallel<span class="sc">::</span><span class="fu">detectCores</span>())</span>
<span id="cb89-44"><a href="bayesian-regularized-regression.html#cb89-44" tabindex="-1"></a></span>
<span id="cb89-45"><a href="bayesian-regularized-regression.html#cb89-45" tabindex="-1"></a><span class="fu">print</span>(fit_lasso, <span class="at">pars =</span> <span class="fu">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;b&quot;</span>))</span></code></pre></div>
<pre><code>## Inference for Stan model: anon_model.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##          mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat
## alpha   40.32    0.00 0.19 39.95 40.19 40.32 40.44 40.68  7136    1
## beta[1] -1.85    0.00 0.19 -2.21 -1.98 -1.85 -1.72 -1.48  7288    1
## beta[2]  2.64    0.00 0.19  2.27  2.52  2.64  2.77  3.01  7301    1
## beta[3]  3.92    0.00 0.20  3.53  3.78  3.92  4.06  4.29  7501    1
## beta[4]  1.08    0.00 0.19  0.71  0.95  1.08  1.22  1.46  7458    1
## beta[5]  0.31    0.00 0.18 -0.06  0.19  0.31  0.43  0.67  6465    1
## sigma    4.97    0.00 0.13  4.72  4.88  4.97  5.06  5.24  6770    1
## b        2.12    0.02 1.04  0.90  1.41  1.88  2.53  4.78  4190    1
## 
## Samples were drawn using NUTS(diag_e) at Thu Oct 16 08:51:46 2025.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
</div>
<div id="hierarchical-shrinkage-horseshoe-prior" class="section level3 hasAnchor" number="5.5.3">
<h3><span class="header-section-number">5.5.3</span> Hierarchical Shrinkage (Horseshoe Prior)<a href="bayesian-regularized-regression.html#hierarchical-shrinkage-horseshoe-prior" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="bayesian-regularized-regression.html#cb91-1" tabindex="-1"></a><span class="co"># Stan model for Horseshoe prior</span></span>
<span id="cb91-2"><a href="bayesian-regularized-regression.html#cb91-2" tabindex="-1"></a>stan_horseshoe <span class="ot">&lt;-</span> <span class="st">&quot;</span></span>
<span id="cb91-3"><a href="bayesian-regularized-regression.html#cb91-3" tabindex="-1"></a><span class="st">data {</span></span>
<span id="cb91-4"><a href="bayesian-regularized-regression.html#cb91-4" tabindex="-1"></a><span class="st">  int&lt;lower=0&gt; N_train;</span></span>
<span id="cb91-5"><a href="bayesian-regularized-regression.html#cb91-5" tabindex="-1"></a><span class="st">  int&lt;lower=0&gt; N_test;</span></span>
<span id="cb91-6"><a href="bayesian-regularized-regression.html#cb91-6" tabindex="-1"></a><span class="st">  int&lt;lower=0&gt; p;</span></span>
<span id="cb91-7"><a href="bayesian-regularized-regression.html#cb91-7" tabindex="-1"></a><span class="st">  matrix[N_train, p] X_train;</span></span>
<span id="cb91-8"><a href="bayesian-regularized-regression.html#cb91-8" tabindex="-1"></a><span class="st">  matrix[N_test, p] X_test;</span></span>
<span id="cb91-9"><a href="bayesian-regularized-regression.html#cb91-9" tabindex="-1"></a><span class="st">  vector[N_train] y_train;</span></span>
<span id="cb91-10"><a href="bayesian-regularized-regression.html#cb91-10" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb91-11"><a href="bayesian-regularized-regression.html#cb91-11" tabindex="-1"></a><span class="st">parameters {</span></span>
<span id="cb91-12"><a href="bayesian-regularized-regression.html#cb91-12" tabindex="-1"></a><span class="st">  real alpha;</span></span>
<span id="cb91-13"><a href="bayesian-regularized-regression.html#cb91-13" tabindex="-1"></a><span class="st">  vector[p] beta_raw;</span></span>
<span id="cb91-14"><a href="bayesian-regularized-regression.html#cb91-14" tabindex="-1"></a><span class="st">  real&lt;lower=0&gt; sigma;</span></span>
<span id="cb91-15"><a href="bayesian-regularized-regression.html#cb91-15" tabindex="-1"></a><span class="st">  real&lt;lower=0&gt; tau;  // global shrinkage</span></span>
<span id="cb91-16"><a href="bayesian-regularized-regression.html#cb91-16" tabindex="-1"></a><span class="st">  vector&lt;lower=0&gt;[p] lambda;  // local shrinkage</span></span>
<span id="cb91-17"><a href="bayesian-regularized-regression.html#cb91-17" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb91-18"><a href="bayesian-regularized-regression.html#cb91-18" tabindex="-1"></a><span class="st">transformed parameters {</span></span>
<span id="cb91-19"><a href="bayesian-regularized-regression.html#cb91-19" tabindex="-1"></a><span class="st">  vector[p] beta;</span></span>
<span id="cb91-20"><a href="bayesian-regularized-regression.html#cb91-20" tabindex="-1"></a><span class="st">  beta = beta_raw .* lambda * tau;  // hierarchical shrinkage</span></span>
<span id="cb91-21"><a href="bayesian-regularized-regression.html#cb91-21" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb91-22"><a href="bayesian-regularized-regression.html#cb91-22" tabindex="-1"></a><span class="st">model {</span></span>
<span id="cb91-23"><a href="bayesian-regularized-regression.html#cb91-23" tabindex="-1"></a><span class="st">  // Priors</span></span>
<span id="cb91-24"><a href="bayesian-regularized-regression.html#cb91-24" tabindex="-1"></a><span class="st">  alpha ~ normal(0, 10);</span></span>
<span id="cb91-25"><a href="bayesian-regularized-regression.html#cb91-25" tabindex="-1"></a><span class="st">  tau ~ cauchy(0, 1);  // global shrinkage</span></span>
<span id="cb91-26"><a href="bayesian-regularized-regression.html#cb91-26" tabindex="-1"></a><span class="st">  lambda ~ cauchy(0, 1);  // local shrinkage</span></span>
<span id="cb91-27"><a href="bayesian-regularized-regression.html#cb91-27" tabindex="-1"></a><span class="st">  beta_raw ~ normal(0, 1);</span></span>
<span id="cb91-28"><a href="bayesian-regularized-regression.html#cb91-28" tabindex="-1"></a><span class="st">  sigma ~ cauchy(0, 5);</span></span>
<span id="cb91-29"><a href="bayesian-regularized-regression.html#cb91-29" tabindex="-1"></a><span class="st">  </span></span>
<span id="cb91-30"><a href="bayesian-regularized-regression.html#cb91-30" tabindex="-1"></a><span class="st">  // Likelihood</span></span>
<span id="cb91-31"><a href="bayesian-regularized-regression.html#cb91-31" tabindex="-1"></a><span class="st">  y_train ~ normal(alpha + X_train * beta, sigma);</span></span>
<span id="cb91-32"><a href="bayesian-regularized-regression.html#cb91-32" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb91-33"><a href="bayesian-regularized-regression.html#cb91-33" tabindex="-1"></a><span class="st">generated quantities {</span></span>
<span id="cb91-34"><a href="bayesian-regularized-regression.html#cb91-34" tabindex="-1"></a><span class="st">  vector[N_test] y_pred;</span></span>
<span id="cb91-35"><a href="bayesian-regularized-regression.html#cb91-35" tabindex="-1"></a><span class="st">  </span></span>
<span id="cb91-36"><a href="bayesian-regularized-regression.html#cb91-36" tabindex="-1"></a><span class="st">  for (i in 1:N_test) {</span></span>
<span id="cb91-37"><a href="bayesian-regularized-regression.html#cb91-37" tabindex="-1"></a><span class="st">    y_pred[i] = normal_rng(alpha + X_test[i] * beta, sigma);</span></span>
<span id="cb91-38"><a href="bayesian-regularized-regression.html#cb91-38" tabindex="-1"></a><span class="st">  }</span></span>
<span id="cb91-39"><a href="bayesian-regularized-regression.html#cb91-39" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb91-40"><a href="bayesian-regularized-regression.html#cb91-40" tabindex="-1"></a><span class="st">&quot;</span></span>
<span id="cb91-41"><a href="bayesian-regularized-regression.html#cb91-41" tabindex="-1"></a></span>
<span id="cb91-42"><a href="bayesian-regularized-regression.html#cb91-42" tabindex="-1"></a><span class="co"># Prepare data (same as before)</span></span>
<span id="cb91-43"><a href="bayesian-regularized-regression.html#cb91-43" tabindex="-1"></a>stan_data_horseshoe <span class="ot">&lt;-</span> stan_data_ridge</span>
<span id="cb91-44"><a href="bayesian-regularized-regression.html#cb91-44" tabindex="-1"></a></span>
<span id="cb91-45"><a href="bayesian-regularized-regression.html#cb91-45" tabindex="-1"></a><span class="co"># Fit model</span></span>
<span id="cb91-46"><a href="bayesian-regularized-regression.html#cb91-46" tabindex="-1"></a>fit_horseshoe <span class="ot">&lt;-</span> <span class="fu">stan</span>(<span class="at">model_code =</span> stan_horseshoe,</span>
<span id="cb91-47"><a href="bayesian-regularized-regression.html#cb91-47" tabindex="-1"></a>                      <span class="at">data =</span> stan_data_horseshoe,</span>
<span id="cb91-48"><a href="bayesian-regularized-regression.html#cb91-48" tabindex="-1"></a>                      <span class="at">chains =</span> <span class="dv">4</span>, <span class="at">iter =</span> <span class="dv">2000</span>,</span>
<span id="cb91-49"><a href="bayesian-regularized-regression.html#cb91-49" tabindex="-1"></a>                      <span class="at">cores =</span> parallel<span class="sc">::</span><span class="fu">detectCores</span>())</span></code></pre></div>
<pre><code>## Warning: There were 106 divergent transitions after warmup. See
## https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
## to find out why this is a problem and how to eliminate them.</code></pre>
<pre><code>## Warning: Examine the pairs() plot to diagnose sampling problems</code></pre>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="bayesian-regularized-regression.html#cb94-1" tabindex="-1"></a><span class="fu">print</span>(fit_horseshoe, <span class="at">pars =</span> <span class="fu">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;tau&quot;</span>))</span></code></pre></div>
<pre><code>## Inference for Stan model: anon_model.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##          mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat
## alpha   40.32    0.00 0.19 39.95 40.19 40.32 40.45 40.70  3660    1
## beta[1] -1.84    0.00 0.20 -2.23 -1.98 -1.84 -1.71 -1.46  3905    1
## beta[2]  2.65    0.00 0.18  2.30  2.52  2.65  2.77  3.01  3678    1
## beta[3]  3.93    0.00 0.20  3.54  3.80  3.93  4.06  4.31  3803    1
## beta[4]  1.07    0.00 0.19  0.69  0.94  1.07  1.20  1.44  4014    1
## beta[5]  0.26    0.00 0.18 -0.05  0.13  0.26  0.39  0.64  2376    1
## sigma    4.98    0.00 0.14  4.72  4.88  4.98  5.07  5.24  3966    1
## tau      2.21    0.04 1.50  0.56  1.21  1.83  2.75  6.06  1429    1
## 
## Samples were drawn using NUTS(diag_e) at Thu Oct 16 08:52:58 2025.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
</div>
</div>
<div id="empirical-comparison-and-model-evaluation" class="section level2 hasAnchor" number="5.6">
<h2><span class="header-section-number">5.6</span> Empirical Comparison and Model Evaluation<a href="bayesian-regularized-regression.html#empirical-comparison-and-model-evaluation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Understanding the theoretical properties of different regularization approaches provides essential intuition, but empirical evaluation reveals how these methods perform in practice. The code below demonstrates how to compare ridge regression, LASSO, and horseshoe priors using both predictive accuracy and coefficient recovery metrics.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="bayesian-regularized-regression.html#cb96-1" tabindex="-1"></a><span class="co"># Extract predictions and compute metrics</span></span>
<span id="cb96-2"><a href="bayesian-regularized-regression.html#cb96-2" tabindex="-1"></a>extract_metrics <span class="ot">&lt;-</span> <span class="cf">function</span>(fit, true_values) {</span>
<span id="cb96-3"><a href="bayesian-regularized-regression.html#cb96-3" tabindex="-1"></a>  y_pred <span class="ot">&lt;-</span> <span class="fu">extract</span>(fit)<span class="sc">$</span>y_pred</span>
<span id="cb96-4"><a href="bayesian-regularized-regression.html#cb96-4" tabindex="-1"></a>  y_pred_mean <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(y_pred)</span>
<span id="cb96-5"><a href="bayesian-regularized-regression.html#cb96-5" tabindex="-1"></a>  </span>
<span id="cb96-6"><a href="bayesian-regularized-regression.html#cb96-6" tabindex="-1"></a>  <span class="co"># RMSE</span></span>
<span id="cb96-7"><a href="bayesian-regularized-regression.html#cb96-7" tabindex="-1"></a>  rmse <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((y_pred_mean <span class="sc">-</span> true_values)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb96-8"><a href="bayesian-regularized-regression.html#cb96-8" tabindex="-1"></a>  </span>
<span id="cb96-9"><a href="bayesian-regularized-regression.html#cb96-9" tabindex="-1"></a>  <span class="co"># Coverage (95% credible intervals)</span></span>
<span id="cb96-10"><a href="bayesian-regularized-regression.html#cb96-10" tabindex="-1"></a>  y_pred_lower <span class="ot">&lt;-</span> <span class="fu">apply</span>(y_pred, <span class="dv">2</span>, quantile, <span class="fl">0.025</span>)</span>
<span id="cb96-11"><a href="bayesian-regularized-regression.html#cb96-11" tabindex="-1"></a>  y_pred_upper <span class="ot">&lt;-</span> <span class="fu">apply</span>(y_pred, <span class="dv">2</span>, quantile, <span class="fl">0.975</span>)</span>
<span id="cb96-12"><a href="bayesian-regularized-regression.html#cb96-12" tabindex="-1"></a>  coverage <span class="ot">&lt;-</span> <span class="fu">mean</span>(true_values <span class="sc">&gt;=</span> y_pred_lower <span class="sc">&amp;</span> true_values <span class="sc">&lt;=</span> y_pred_upper)</span>
<span id="cb96-13"><a href="bayesian-regularized-regression.html#cb96-13" tabindex="-1"></a>  </span>
<span id="cb96-14"><a href="bayesian-regularized-regression.html#cb96-14" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">rmse =</span> rmse, <span class="at">coverage =</span> coverage))</span>
<span id="cb96-15"><a href="bayesian-regularized-regression.html#cb96-15" tabindex="-1"></a>}</span>
<span id="cb96-16"><a href="bayesian-regularized-regression.html#cb96-16" tabindex="-1"></a></span>
<span id="cb96-17"><a href="bayesian-regularized-regression.html#cb96-17" tabindex="-1"></a><span class="co"># True test values (computed in R, not passed to Stan)</span></span>
<span id="cb96-18"><a href="bayesian-regularized-regression.html#cb96-18" tabindex="-1"></a>y_test_true <span class="ot">&lt;-</span> alpha_true <span class="sc">+</span> X_test <span class="sc">%*%</span> beta_true</span>
<span id="cb96-19"><a href="bayesian-regularized-regression.html#cb96-19" tabindex="-1"></a></span>
<span id="cb96-20"><a href="bayesian-regularized-regression.html#cb96-20" tabindex="-1"></a><span class="co"># Compare models</span></span>
<span id="cb96-21"><a href="bayesian-regularized-regression.html#cb96-21" tabindex="-1"></a>metrics_ridge <span class="ot">&lt;-</span> <span class="fu">extract_metrics</span>(fit_ridge, y_test_true)</span>
<span id="cb96-22"><a href="bayesian-regularized-regression.html#cb96-22" tabindex="-1"></a>metrics_lasso <span class="ot">&lt;-</span> <span class="fu">extract_metrics</span>(fit_lasso, y_test_true)</span>
<span id="cb96-23"><a href="bayesian-regularized-regression.html#cb96-23" tabindex="-1"></a>metrics_horseshoe <span class="ot">&lt;-</span> <span class="fu">extract_metrics</span>(fit_horseshoe, y_test_true)</span>
<span id="cb96-24"><a href="bayesian-regularized-regression.html#cb96-24" tabindex="-1"></a></span>
<span id="cb96-25"><a href="bayesian-regularized-regression.html#cb96-25" tabindex="-1"></a></span>
<span id="cb96-26"><a href="bayesian-regularized-regression.html#cb96-26" tabindex="-1"></a><span class="co"># Print comparison</span></span>
<span id="cb96-27"><a href="bayesian-regularized-regression.html#cb96-27" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Model Comparison (Test Set):</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb96-28"><a href="bayesian-regularized-regression.html#cb96-28" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Ridge - RMSE:&quot;</span>, <span class="fu">round</span>(metrics_ridge<span class="sc">$</span>rmse, <span class="dv">3</span>), </span>
<span id="cb96-29"><a href="bayesian-regularized-regression.html#cb96-29" tabindex="-1"></a>    <span class="st">&quot;Coverage:&quot;</span>, <span class="fu">round</span>(metrics_ridge<span class="sc">$</span>coverage, <span class="dv">3</span>), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb96-30"><a href="bayesian-regularized-regression.html#cb96-30" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;LASSO - RMSE:&quot;</span>, <span class="fu">round</span>(metrics_lasso<span class="sc">$</span>rmse, <span class="dv">3</span>), </span>
<span id="cb96-31"><a href="bayesian-regularized-regression.html#cb96-31" tabindex="-1"></a>    <span class="st">&quot;Coverage:&quot;</span>, <span class="fu">round</span>(metrics_lasso<span class="sc">$</span>coverage, <span class="dv">3</span>), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb96-32"><a href="bayesian-regularized-regression.html#cb96-32" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Horseshoe - RMSE:&quot;</span>, <span class="fu">round</span>(metrics_horseshoe<span class="sc">$</span>rmse, <span class="dv">3</span>), </span>
<span id="cb96-33"><a href="bayesian-regularized-regression.html#cb96-33" tabindex="-1"></a>    <span class="st">&quot;Coverage:&quot;</span>, <span class="fu">round</span>(metrics_horseshoe<span class="sc">$</span>coverage, <span class="dv">3</span>), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb96-34"><a href="bayesian-regularized-regression.html#cb96-34" tabindex="-1"></a></span>
<span id="cb96-35"><a href="bayesian-regularized-regression.html#cb96-35" tabindex="-1"></a><span class="co"># Plot coefficient estimates</span></span>
<span id="cb96-36"><a href="bayesian-regularized-regression.html#cb96-36" tabindex="-1"></a><span class="fu">library</span>(bayesplot)</span>
<span id="cb96-37"><a href="bayesian-regularized-regression.html#cb96-37" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb96-38"><a href="bayesian-regularized-regression.html#cb96-38" tabindex="-1"></a></span>
<span id="cb96-39"><a href="bayesian-regularized-regression.html#cb96-39" tabindex="-1"></a><span class="co"># Extract posterior samples</span></span>
<span id="cb96-40"><a href="bayesian-regularized-regression.html#cb96-40" tabindex="-1"></a>beta_ridge <span class="ot">&lt;-</span> <span class="fu">extract</span>(fit_ridge)<span class="sc">$</span>beta</span>
<span id="cb96-41"><a href="bayesian-regularized-regression.html#cb96-41" tabindex="-1"></a>beta_lasso <span class="ot">&lt;-</span> <span class="fu">extract</span>(fit_lasso)<span class="sc">$</span>beta</span>
<span id="cb96-42"><a href="bayesian-regularized-regression.html#cb96-42" tabindex="-1"></a>beta_horseshoe <span class="ot">&lt;-</span> <span class="fu">extract</span>(fit_horseshoe)<span class="sc">$</span>beta</span>
<span id="cb96-43"><a href="bayesian-regularized-regression.html#cb96-43" tabindex="-1"></a></span>
<span id="cb96-44"><a href="bayesian-regularized-regression.html#cb96-44" tabindex="-1"></a><span class="co"># Create comparison plot</span></span>
<span id="cb96-45"><a href="bayesian-regularized-regression.html#cb96-45" tabindex="-1"></a>beta_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb96-46"><a href="bayesian-regularized-regression.html#cb96-46" tabindex="-1"></a>  <span class="at">coefficient =</span> <span class="fu">rep</span>(<span class="fu">paste0</span>(<span class="st">&quot;beta&quot;</span>, <span class="dv">1</span><span class="sc">:</span>p), <span class="dv">3</span>),</span>
<span id="cb96-47"><a href="bayesian-regularized-regression.html#cb96-47" tabindex="-1"></a>  <span class="at">method =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;Ridge&quot;</span>, <span class="st">&quot;LASSO&quot;</span>, <span class="st">&quot;Horseshoe&quot;</span>), <span class="at">each =</span> p),</span>
<span id="cb96-48"><a href="bayesian-regularized-regression.html#cb96-48" tabindex="-1"></a>  <span class="at">mean =</span> <span class="fu">c</span>(<span class="fu">colMeans</span>(beta_ridge), <span class="fu">colMeans</span>(beta_lasso), <span class="fu">colMeans</span>(beta_horseshoe)),</span>
<span id="cb96-49"><a href="bayesian-regularized-regression.html#cb96-49" tabindex="-1"></a>  <span class="at">lower =</span> <span class="fu">c</span>(<span class="fu">apply</span>(beta_ridge, <span class="dv">2</span>, quantile, <span class="fl">0.025</span>),</span>
<span id="cb96-50"><a href="bayesian-regularized-regression.html#cb96-50" tabindex="-1"></a>            <span class="fu">apply</span>(beta_lasso, <span class="dv">2</span>, quantile, <span class="fl">0.025</span>),</span>
<span id="cb96-51"><a href="bayesian-regularized-regression.html#cb96-51" tabindex="-1"></a>            <span class="fu">apply</span>(beta_horseshoe, <span class="dv">2</span>, quantile, <span class="fl">0.025</span>)),</span>
<span id="cb96-52"><a href="bayesian-regularized-regression.html#cb96-52" tabindex="-1"></a>  <span class="at">upper =</span> <span class="fu">c</span>(<span class="fu">apply</span>(beta_ridge, <span class="dv">2</span>, quantile, <span class="fl">0.975</span>),</span>
<span id="cb96-53"><a href="bayesian-regularized-regression.html#cb96-53" tabindex="-1"></a>            <span class="fu">apply</span>(beta_lasso, <span class="dv">2</span>, quantile, <span class="fl">0.975</span>),</span>
<span id="cb96-54"><a href="bayesian-regularized-regression.html#cb96-54" tabindex="-1"></a>            <span class="fu">apply</span>(beta_horseshoe, <span class="dv">2</span>, quantile, <span class="fl">0.975</span>)),</span>
<span id="cb96-55"><a href="bayesian-regularized-regression.html#cb96-55" tabindex="-1"></a>  <span class="at">true_value =</span> <span class="fu">rep</span>(beta_true, <span class="dv">3</span>)</span>
<span id="cb96-56"><a href="bayesian-regularized-regression.html#cb96-56" tabindex="-1"></a>)</span>
<span id="cb96-57"><a href="bayesian-regularized-regression.html#cb96-57" tabindex="-1"></a></span>
<span id="cb96-58"><a href="bayesian-regularized-regression.html#cb96-58" tabindex="-1"></a><span class="fu">ggplot</span>(beta_df, <span class="fu">aes</span>(<span class="at">x =</span> coefficient, <span class="at">y =</span> mean, <span class="at">color =</span> method)) <span class="sc">+</span></span>
<span id="cb96-59"><a href="bayesian-regularized-regression.html#cb96-59" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">position =</span> <span class="fu">position_dodge</span>(<span class="fl">0.3</span>)) <span class="sc">+</span></span>
<span id="cb96-60"><a href="bayesian-regularized-regression.html#cb96-60" tabindex="-1"></a>  <span class="fu">geom_errorbar</span>(<span class="fu">aes</span>(<span class="at">ymin =</span> lower, <span class="at">ymax =</span> upper), </span>
<span id="cb96-61"><a href="bayesian-regularized-regression.html#cb96-61" tabindex="-1"></a>                <span class="at">position =</span> <span class="fu">position_dodge</span>(<span class="fl">0.3</span>), <span class="at">width =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb96-62"><a href="bayesian-regularized-regression.html#cb96-62" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y =</span> true_value), <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">shape =</span> <span class="dv">4</span>, <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb96-63"><a href="bayesian-regularized-regression.html#cb96-63" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Coefficient Estimates Comparison&quot;</span>,</span>
<span id="cb96-64"><a href="bayesian-regularized-regression.html#cb96-64" tabindex="-1"></a>       <span class="at">subtitle =</span> <span class="st">&quot;Black crosses show true values&quot;</span>,</span>
<span id="cb96-65"><a href="bayesian-regularized-regression.html#cb96-65" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Coefficient&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Estimate&quot;</span>) <span class="sc">+</span></span>
<span id="cb96-66"><a href="bayesian-regularized-regression.html#cb96-66" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb96-67"><a href="bayesian-regularized-regression.html#cb96-67" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">45</span>, <span class="at">hjust =</span> <span class="dv">1</span>))</span></code></pre></div>
<p>The empirical comparison typically reveals distinct patterns in how each method handles different types of signals. Ridge regression tends to shrink all coefficients proportionally, maintaining the relative magnitudes while reducing overall scale. This behavior works well when most predictors contribute meaningfully to the response, even if their individual effects are modest. LASSO regression exhibits more dramatic behavior, often setting smaller coefficients to exactly zero while leaving larger ones relatively unshrunk. This creates sparse solutions that can be easier to interpret but may sacrifice some predictive accuracy when the true model is dense.</p>
<p>Hierarchical shrinkage methods like the horseshoe prior attempt to get the best of both worlds by adapting the amount of shrinkage to each coefficient individually. Large coefficients experience minimal shrinkage, allowing them to maintain their full predictive power, while small coefficients get shrunk aggressively toward zero, reducing noise in the model. This adaptivity often leads to superior predictive performance, especially in settings where the true coefficient vector contains a mixture of large, moderate, and negligible effects.</p>
</div>
<div id="understanding-posterior-behavior" class="section level2 hasAnchor" number="5.7">
<h2><span class="header-section-number">5.7</span> Understanding Posterior Behavior<a href="bayesian-regularized-regression.html#understanding-posterior-behavior" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The mathematical structure of each prior determines not just point estimates but the entire posterior distribution. Ridge regression produces a posterior mean with the closed form <span class="math inline">\(E[\boldsymbol{\beta}|\mathbf{y}] = \left(\frac{1}{\sigma^2}\mathbf{X}^T\mathbf{X} + \frac{1}{\tau^2}\mathbf{I}\right)^{-1}\frac{1}{\sigma^2}\mathbf{X}^T\mathbf{y}\)</span>, which clearly shows how the prior variance <span class="math inline">\(\tau^2\)</span> balances against the data-driven term <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span>. When <span class="math inline">\(\tau^2\)</span> is small relative to <span class="math inline">\(\sigma^2\)</span>, the prior dominates and coefficients shrink strongly toward zero. When <span class="math inline">\(\tau^2\)</span> is large, the data term dominates and estimates approach ordinary least squares.</p>
<p>LASSO regression does not admit a closed-form posterior mean due to the non-conjugate Laplace prior, but the posterior mode corresponds exactly to the frequentist LASSO solution. The full posterior distribution, obtained through MCMC sampling, provides uncertainty intervals that properly account for both the sparsity-inducing prior and the inherent variability in coefficient estimates. This represents a substantial improvement over frequentist LASSO inference, which requires complex procedures to obtain valid confidence intervals for selected variables.</p>
<p>The horseshoe prior creates particularly interesting posterior behavior through its adaptive shrinkage mechanism. Each coefficient experiences shrinkage according to <span class="math inline">\(E[\beta_j|\text{data}] \approx (1 - \kappa_j) \hat{\beta}_j^{\text{OLS}}\)</span>, where <span class="math inline">\(\kappa_j \in (0,1)\)</span> represents a data-dependent shrinkage factor. Coefficients with strong signals in the data have shrinkage factors close to zero, leaving them essentially unshrunk, while coefficients with weak signals have shrinkage factors approaching one, causing aggressive shrinkage toward zero.</p>
</div>
<div id="choosing-among-regularization-methods" class="section level2 hasAnchor" number="5.8">
<h2><span class="header-section-number">5.8</span> Choosing Among Regularization Methods<a href="bayesian-regularized-regression.html#choosing-among-regularization-methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The choice between ridge regression, LASSO, and hierarchical shrinkage depends critically on the structure expected in the true coefficient vector and the goals of the analysis. Ridge regression works best when most predictors contribute meaningfully to the response, even if some effects are small. This commonly occurs in settings where the predictors represent different aspects of the same underlying phenomenon or when domain knowledge suggests that excluding variables entirely would be inappropriate.</p>
<p>LASSO regression excels when the true model is sparse, meaning that only a subset of predictors have non-zero effects. The automatic variable selection property makes LASSO particularly attractive for exploratory analyses where identifying the most important predictors is as important as achieving good predictive performance. However, LASSO can struggle when several predictors are highly correlated, as it tends to select one arbitrarily and zero out the others, potentially missing important relationships.</p>
<p>Hierarchical shrinkage methods like the horseshoe prior offer the most flexibility by allowing the data to determine the appropriate level of sparsity. These methods work well across a wide range of scenarios, from dense models where most predictors matter to sparse models where only a few predictors have substantial effects. The primary cost of this flexibility is computational complexity, as hierarchical models typically require more sophisticated MCMC sampling schemes and longer chains to achieve convergence.</p>
</div>
<div id="conclusion-2" class="section level2 hasAnchor" number="5.9">
<h2><span class="header-section-number">5.9</span> Conclusion<a href="bayesian-regularized-regression.html#conclusion-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bayesian regularized regression represents a natural and principled approach to handling the challenges that arise in modern statistical modeling. By reformulating penalty-based methods in terms of prior distributions, the Bayesian framework provides not only point estimates equivalent to their frequentist counterparts but also full posterior distributions that enable proper uncertainty quantification and model comparison. The connection between prior specifications and regularization behavior offers valuable insight into why different methods work well in different contexts and how to choose appropriately among them.</p>
<p>The three approaches examined here span a useful range of assumptions about coefficient behavior. Ridge regression assumes all coefficients are a priori similar and should be shrunk proportionally, LASSO assumes that many coefficients are exactly zero and should be eliminated entirely, and hierarchical shrinkage assumes that coefficients vary in importance and should be shrunk adaptively. Modern computing makes all three approaches feasible for most practical applications, shifting the primary challenge from computational limitations to thoughtful model selection based on domain knowledge and data characteristics.</p>
<p>These Bayesian methods also integrate naturally into larger modeling workflows that require uncertainty quantification, model averaging, or decision-making under uncertainty. The posterior distributions provide the building blocks for more complex analyses while maintaining the computational efficiency and theoretical elegance that make regularized regression so appealing in the first place.</p>
</div>
<div id="references-3" class="section level2 hasAnchor" number="5.10">
<h2><span class="header-section-number">5.10</span> References<a href="bayesian-regularized-regression.html#references-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Carvalho, C. M., Polson, N. G., &amp; Scott, J. G. (2010). The horseshoe estimator for sparse signals. <em>Biometrika</em>, 97(2), 465-480.</li>
<li>Park, T., &amp; Casella, G. (2008). The Bayesian lasso. <em>Journal of the American Statistical Association</em>, 103(482), 681-686.</li>
<li>Piironen, J., &amp; Vehtari, A. (2017). Sparsity information and regularization in the horseshoe and other shrinkage priors. <em>Electronic Journal of Statistics</em>, 11(2), 5018-5051.</li>
<li>Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. <em>Journal of the Royal Statistical Society</em>, 58(1), 267-288.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="robust-t-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bayesian-quantile-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_Stan/edit/main/05-Regularization.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_Stan/blob/main/05-Regularization.Rmd",
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
