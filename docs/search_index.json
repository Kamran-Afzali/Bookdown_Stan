[["index.html", "Stan Bookdown Chapter 1 Choosing the Right Bayesian Model", " Stan Bookdown Kamran Afzali 2025-06-20 Chapter 1 Choosing the Right Bayesian Model Introduction Bayesian modeling has become a central approach in modern data analysis, providing a coherent framework for incorporating prior knowledge and quantifying uncertainty. With the advent of powerful tools such as Stan and user-friendly interfaces like the R package brms, practitioners can now implement a wide array of Bayesian models with relative ease. However, the flexibility of the Bayesian framework also introduces a new challenge: selecting the most appropriate model for a given dataset and research question. The landscape of Bayesian models is vast, encompassing linear and generalized linear models, robust and regularized regressions, hierarchical models, and more sophisticated approaches such as Gaussian processes and mixture models. This guide aims to offer a structured approach to model selection within the Bayesian paradigm, focusing on practical considerations, data characteristics, and modeling objectives. Modeling Objectives and Data Characteristics The choice of a Bayesian model should begin with a clear understanding of the research objective. In broad terms, the aim of modeling can be categorized into two primary goals: inference and prediction. Inference focuses on understanding the relationships between variables, quantifying uncertainty in parameter estimates, and testing theoretical hypotheses. Prediction, on the other hand, emphasizes the accuracy of forecasting outcomes for new observations. While the two goals are not mutually exclusive, they can lead to different modeling choices, particularly in terms of model complexity and regularization. Another factor influencing model selection is the nature of the data. Key aspects include the type of response variable (continuous, binary, count, categorical), the presence of outliers or heavy-tailed distributions, the structure of the data (e.g., hierarchical or longitudinal), and the dimensionality of the predictor space. A careful examination of these characteristics provides essential guidance for selecting an appropriate Bayesian model. Prior Specification and Computational Considerations An essential feature of Bayesian modeling is the specification of prior distributions. Priors can be informative, weakly informative, or non-informative, depending on the amount of domain knowledge available. Informative priors are grounded in expert knowledge or historical data, while weakly informative priors help stabilize estimates without unduly influencing the posterior. Prior predictive checks can assess the implications of the priors before seeing the data, ensuring they encode plausible assumptions. Modelers should also perform sensitivity analyses to understand how different priors affect inferences. Computational feasibility is another practical concern. Some Bayesian models—especially nonparametric or high-dimensional ones—can be computationally intensive, requiring advanced MCMC algorithms or variational inference. Diagnostics such as the Gelman-Rubin R-hat statistic, effective sample size (ESS), and checks for divergent transitions should be used to ensure reliable inference (Gelman et al., 2013). Stan and brms provide tools to assess convergence and evaluate sampling efficiency. Bayesian Linear Regression Bayesian linear regression serves as the foundational model in the Bayesian framework. It assumes a linear relationship between predictors and a continuous response variable, with normally distributed residuals. This model is particularly useful for its simplicity and interpretability. When the assumptions of linearity and normality hold reasonably well, Bayesian linear regression provides reliable parameter estimates and predictive intervals. It also serves as a baseline model against which more complex models can be compared. In practice, Bayesian linear regression can be implemented in Stan with straightforward model code, specifying priors for the regression coefficients and residual variance. The flexibility of Bayesian inference allows for the incorporation of prior knowledge, which can be particularly valuable in small-sample contexts or when strong domain expertise is available. Robust Regression for Non-Normal Residuals Real-world data often deviate from the assumption of normally distributed residuals. Outliers or heavy-tailed distributions can exert undue influence on parameter estimates, leading to biased or unstable results. Bayesian robust regression addresses this issue by modeling the residuals using a t-distribution, which has heavier tails than the normal distribution. This approach reduces the influence of outliers, leading to more robust and reliable inferences. The implementation of robust regression in Stan involves specifying a likelihood based on the t-distribution and including an additional parameter for the degrees of freedom. This parameter controls the heaviness of the tails and can itself be estimated from the data. The robust regression model is particularly recommended when residual diagnostics from a standard linear model indicate non-normality or the presence of extreme observations. Regularized Regression for High-Dimensional Data When dealing with a large number of predictors or multicollinearity, regularization becomes essential to prevent overfitting and to enhance predictive performance. Bayesian regularized regression models incorporate shrinkage priors, such as the Laplace prior for Bayesian LASSO or the Gaussian prior for Bayesian ridge regression. These priors shrink the regression coefficients toward zero, effectively performing variable selection and regularization. In the Bayesian framework, regularization is naturally integrated through the prior distribution. For example, the Bayesian LASSO uses a double-exponential prior that induces sparsity by assigning higher probability mass near zero. These models are particularly useful in settings with more predictors than observations or when there is a need to identify the most influential variables. Models for Non-Normal Data In many applications, the response variable does not follow a normal distribution. Binary outcomes, count data, and categorical responses require specialized models. Bayesian generalized linear models (GLMs) extend the linear model framework to accommodate different types of response variables through appropriate link functions and likelihood distributions. For binary outcomes, the logistic regression model with a logit link is commonly used. For count data, Poisson and negative binomial models are appropriate, with the latter providing a flexible alternative in the presence of overdispersion. Multinomial and ordinal regression models are used for categorical outcomes, with the choice depending on whether the categories are ordered. These models are readily implemented in Stan and brms, allowing users to specify the appropriate family and link function. Model selection in this context should be guided by the distributional characteristics of the response variable and the research question at hand. Multilevel and Hierarchical Models Hierarchical data structures are common in social sciences, education, and biomedical research. In such settings, observations are nested within higher-level units, such as students within schools or patients within hospitals. Ignoring this structure can lead to biased inferences and underestimated uncertainty. Bayesian multilevel models explicitly account for the hierarchical structure by including group-level effects. These models allow for partial pooling of information across groups, balancing between complete pooling (ignoring group differences) and no pooling (treating each group separately). The brms package offers a user-friendly interface for fitting multilevel models, handling complex random effects structures with ease. The flexibility of Bayesian multilevel modeling also facilitates the inclusion of varying slopes, cross-level interactions, and non-linear effects. When the data structure suggests hierarchical dependencies, multilevel modeling should be the default approach. Nonlinear and Nonparametric Models In some applications, the relationship between predictors and the response variable is inherently nonlinear or unknown. Bayesian nonparametric models, such as Gaussian process regression, offer a flexible solution by modeling the function space directly. Gaussian processes define a prior over functions and use observed data to update this prior, resulting in a posterior distribution over functions. Gaussian process regression is particularly powerful when the form of the relationship is unknown or when modeling smooth, nonlinear trends is important. However, it comes at a higher computational cost and may not scale well with large datasets. Nevertheless, for problems involving spatial data, temporal trends, or complex functional relationships, Gaussian processes provide a valuable modeling tool. Mixture Models and Latent Structure Data arising from heterogeneous populations may be better modeled using mixture models. Bayesian Gaussian mixture models, for instance, assume that the data are generated from a mixture of several Gaussian distributions, each representing a subpopulation. These models can uncover latent structure in the data, such as clusters or subtypes. Mixture models introduce additional complexity due to the need to estimate both the component parameters and the mixing proportions. Bayesian inference provides a principled framework for dealing with this uncertainty, often using techniques such as latent variable augmentation and label switching adjustments. When there is reason to believe that the data comprise distinct subgroups with different underlying characteristics, mixture models offer an effective approach to modeling such heterogeneity. Comparative Summary Table Model Type Use Case Key Assumptions Priors Limitations Linear Regression Continuous outcome, low noise Linearity, normal errors Normal, Inverse-Gamma Poor with outliers Robust Regression Heavy-tailed residuals t-distributed residuals Prior on ν Increased complexity Regularized Regression High-dimensional predictors Sparsity Laplace, Gaussian Shrinkage may hide effects GLMs Binary/count/categorical outcomes Appropriate link function Varied Can overfit without strong priors Hierarchical Models Nested/grouped data Partial pooling Hierarchical priors Sensitive to group size Gaussian Processes Unknown nonlinear function Smoothness in kernel GP prior Poor scaling (O(n³)) Mixture Models Latent structure/clustering Finite components Dirichlet, etc. Label switching, identifiability Model Diagnostics and Comparison Choosing the right model also involves evaluating its performance and comparing it to alternative specifications. Bayesian model diagnostics include posterior predictive checks, which assess how well the model reproduces the observed data. Graphical comparisons between observed and replicated data can reveal model misfit or systematic discrepancies. Information criteria such as the Widely Applicable Information Criterion (WAIC) and Leave-One-Out Cross-Validation (LOO-CV) provide tools for model comparison, balancing fit and complexity. These criteria estimate the expected out-of-sample predictive performance and are particularly useful for selecting among nested or non-nested models (Vehtari, Gelman, &amp; Gabry, 2017). Bayes factors offer another method for model comparison, based on the ratio of marginal likelihoods. However, they are sensitive to prior specification and can be computationally intensive. In practice, WAIC and LOO-CV are often preferred for their robustness and ease of computation. A Decision Framework for Model Selection To aid practitioners in selecting the appropriate Bayesian model, a structured decision framework can be employed. This framework begins with identifying the type of response variable: continuous, binary, count, or categorical. Next, the data should be assessed for features such as outliers, overdispersion, hierarchical structure, and nonlinearity. Based on these characteristics, the modeler can then choose among linear models, robust regressions, generalized linear models, multilevel models, or nonparametric approaches. This decision process is iterative and should incorporate model diagnostics and domain knowledge. Starting with a simple model and progressively introducing complexity allows for a more transparent understanding of the data and the modeling assumptions. Each modeling choice should be justified in terms of its contribution to answering the research question and improving model fit. Conclusion Bayesian modeling offers unparalleled flexibility and rigor in statistical inference, but this power comes with the responsibility of thoughtful model selection. This guide has outlined the key considerations for choosing among the diverse array of Bayesian models available in tools like Stan and brms. By grounding model selection in the objectives of the analysis, the characteristics of the data, and robust diagnostic procedures, practitioners can make informed choices that enhance both the interpretability and predictive performance of their models. As with all statistical modeling, the process is iterative and benefits from a combination of statistical insight, computational tools, and substantive expertise. With this guide, researchers are better equipped to navigate the Bayesian modeling landscape and apply the appropriate models to their specific challenges. "],["bayesian-modeling-in-r-and-stan.html", "Chapter 2 Bayesian Modeling in R and Stan 2.1 References", " Chapter 2 Bayesian Modeling in R and Stan The aim of this post is to provide a quick overview and introduction to fitting Bayesian models using STAN and R. For this, I strongly recommend installing Rstudio, an integrated development environment that allows a “user-friendly” interaction with R. 2.0.1 What is STAN? STAN is a tool for analysing Bayesian models using Markov Chain Monte Carlo (MCMC) methods. MCMC is a sampling method for estimating a probability distribution without knowing all of the features of the distribution. STAN is a probabilistic programming language and free software for specifying statistical models utilising Hamiltonian Monte Carlo methods (HMC), a type of MCMC algorithm. Stan works with the most widely used data-analysis languages including R and Python. In this quick overview, we’ll focus on the rstan package and demonstrate how to fit STAN models with it. For an example dataset, here we simulate our own data in R. We firsty create a continuous outcome variable y as a function of one predictor x and a disturbance term ϵ. I simulate a dataset with 100 observations. Create the error term, the predictor and the outcome using a linear form with an intercept β0, slope β1, etc. coefficients. set.seed(123) n &lt;- 1000 a &lt;- 40 #intercept b &lt;- c(-2, 3, 4, 1 , 0.25) #slopes sigma2 &lt;- 25 #residual variance (sd=5) x &lt;- matrix(rnorm(5000),1000,5) eps &lt;- rnorm(n, mean = 0, sd = sqrt(sigma2)) #residuals y &lt;- a +x%*%b+ eps #response variable data &lt;- data.frame(y, x) #dataset head(data)%&gt;%kableExtra::kable() y X1 X2 X3 X4 X5 33.51510 -0.5604756 -0.9957987 -0.5116037 -0.1503075 0.1965498 43.76098 -0.2301775 -1.0399550 0.2369379 -0.3277571 0.6501132 27.64712 1.5587083 -0.0179802 -0.5415892 -1.4481653 0.6710042 50.72614 0.0705084 -0.1321751 1.2192276 -0.6972846 -1.2841578 39.46286 0.1292877 -2.5493428 0.1741359 2.5984902 -2.0261096 39.42009 1.7150650 1.0405735 -0.6152683 -0.0374150 2.2053261 2.0.2 Model file STAN models are written in an imperative programming language, which means the order in which you write the elements in your model file matters, i.e. you must first define your variables (e.g. integers, vectors, matrices, etc.), then the constraints that define the range of values your variable can take (e.g. only positive values for standard deviations), and finally the relationship between the variables. A Stan model is defined by different blocks including: Data (required): The data block reads information from the outside world, such as data vectors, matrices, integers, and so on. We also need to define the lengths and dimensions of objects, which may appear unusual to those who are used to R or Python. The number of observations is first declared as an integer variable N: int N; (note the use of semicolon to denote the end of a line). The number of predictors in our model, which is K. The intercept is included in this count, so we end up with two predictors (2 columns in the model matrix). Transformed Data (optional): The converted data block enables data preprocessing, such as data transformation or rescaling. Parameters (required): The parameters block specifies the parameters that must be assigned to prior distributions. Transformed parameters (optional): Before computing the posterior, the changed parameters block provides for parameter processing, such as transformation or rescaling of the parameters. model_stan = &quot; data { // declare the input data / parameters } transformed data { // optional - for transforming/scaling input data } parameters { // define model parameters } transformed parameters { // optional - for deriving additional non-model parameters // note however, as they are part of the sampling chain // transformed parameters slow sampling down. } model { // specifying priors and likelihood as well as the linear predictor } generated quantities { // optional - derivatives (posteriors) of the samples } &quot; cat(model_stan) ## ## data { ## // declare the input data / parameters ## } ## transformed data { ## // optional - for transforming/scaling input data ## } ## parameters { ## // define model parameters ## } ## transformed parameters { ## // optional - for deriving additional non-model parameters ## // note however, as they are part of the sampling chain ## // transformed parameters slow sampling down. ## } ## model { ## // specifying priors and likelihood as well as the linear predictor ## } ## generated quantities { ## // optional - derivatives (posteriors) of the samples ## } ## For this introduction, I’ll use a very simple model in the STAN model that only involves the specification of three blocks. In the data block, I declare the variables y and x as reals (or vectors) with length equal to N and declare the sample size n sim as a positive integer number using the phrase int n sim. I define the coefficients for the linear regression alpha and beta (as real values) and the standard deviation parameter sigma in the parameters block (as a positive real number). Finally, in the model block, I give the regression coefficients and standard deviation parameters weakly informative priors, and I use a normal distribution indexed by the conditional mean mu and standard deviation sigma parameters to model the outcome data y. I give full recognition to McElreath’s outstanding Statistical Rethinking (2020) book for this section. stan_mod = &quot;data { int&lt;lower=0&gt; N; // number of observations int&lt;lower=0&gt; K; // number of predictors matrix[N, K] X; // predictor matrix vector[N] y; // outcome vector } parameters { real alpha; // intercept vector[K] beta; // coefficients for predictors real&lt;lower=0&gt; sigma; // error scale } model { y ~ normal(alpha + X * beta, sigma); // target density }&quot; writeLines(stan_mod, con = &quot;stan_mod.stan&quot;) cat(stan_mod) 2.0.3 Fit the model We’ll need to organise the information into a list for Stan. This list should contain everything we defined in the data block of our Stan code. Then it is possible to run the model with the stan function. library(tidyverse) predictors &lt;- data %&gt;% select(-y) stan_data &lt;- list( N = 1000, K = 5, X = predictors, y = data$y ) fit_rstan &lt;- rstan::stan( file = &quot;stan_mod.stan&quot;, data = stan_data ) fit_rstan ## Inference for Stan model: anon_model. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% ## alpha 40.19 0.00 0.15 39.89 40.08 40.19 40.29 40.48 ## beta[1] -2.07 0.00 0.16 -2.39 -2.18 -2.07 -1.97 -1.76 ## beta[2] 2.73 0.00 0.16 2.42 2.62 2.73 2.84 3.04 ## beta[3] 3.83 0.00 0.16 3.51 3.72 3.83 3.93 4.13 ## beta[4] 1.23 0.00 0.16 0.92 1.11 1.23 1.34 1.54 ## beta[5] 0.34 0.00 0.16 0.04 0.23 0.34 0.45 0.65 ## sigma 4.96 0.00 0.11 4.74 4.88 4.95 5.03 5.17 ## lp__ -2098.32 0.04 1.88 -2102.75 -2099.39 -2097.99 -2096.92 -2095.67 ## n_eff Rhat ## alpha 5280 1 ## beta[1] 5552 1 ## beta[2] 6251 1 ## beta[3] 5988 1 ## beta[4] 6172 1 ## beta[5] 6343 1 ## sigma 5687 1 ## lp__ 1916 1 ## ## Samples were drawn using NUTS(diag_e) at Wed May 4 13:23:35 2022. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 2.0.4 MCMC diagnostics For Bayesian analysis, it is required to investigate the properties of the MCMC chains and the sampler in general, in addition to the standard model diagnostic checks (such as residual plots). Remember that the goal of MCMC sampling is to reproduce the posterior distribution of the model likelihood and priors by formulating a probability distribution. This method reliable, only if the MCMC samples accurately reflect the posterior. For each parameter, traceplots show the MCMC sample values after each iteration along the chain. Bad chain mixing (any pattern) indicates that the MCMC sample chains may not have spanned all aspects of the posterior distribution and that further iterations are needed to ensure that the distribution is accurately represented. Each parameter’s autocorrelation graphic shows the degree of correlation between MCMC samples separated by different lags. The degree of correlation between each MCMC sample and itself, for example, is represented by a lag of (obviously this will be a correlation of ). The MCMC samples should be independent in order to obtain unbiased parameter estimations (uncorrelated). For each parameter, the potential scale reduction factor (Rhat) statistic offers a measure of sampling efficiency/effectiveness. All values should, in theory, be less than 1, if the sampler has values of or greater than 1, it is likely that it was not particularly efficient or effective. A misspecified model or extremely unclear priors that lead to misspecified parameter space might lead to this. We should have looked at the convergence diagnostics before looking at the summaries. For the effects model, we utilise the package mcmcplots to generate density and trace graphs. It’s crucial to evaluate if the chains have converged when using MCMC to fit a model. To visually inspect MCMC diagnostics, we propose the bayesplot software. The bayesplot package includes routines for displaying MCMC diagnostics and supports model objects from both rstan and rstanarm. We’ll show how to make a trace plot with the mcmc_trace() function and a plot of Rhat values with the mcmc rhat() function. By printing the model fit, we can examine the parameters in the console. For each parameter, we derive posterior means, standard errors, and quantiles. n eff and Rhat are two other terms we use. These are the results of Stan’s engine’s exploration of the parameter space. For the time being, it’s enough to know that when Rhat is 1, everything is well. fit_rstan %&gt;% mcmc_trace() fit_rstan %&gt;% rhat() %&gt;% mcmc_rhat() + yaxis_text() 2.0.5 Parting thoughts To finish this post, I’d like to point out that the rstanarm package makes it possible to fit STAN models without having to write them down and instead using standard R syntax, such as that found in a glm(). So, what’s the point of learning all this STAN jargon? It depends: if you’re merely fitting “classical” models to your data with no fanfare, just use rstanarm; it’ll save you time and the models in this package are unquestionably better parametrized (i.e. faster) than the one I presented here. Learning STAN, on the other hand, is a good approach to get into a very flexible and strong language that will continue to evolve if you believe you will need to fit your own models one day. rstanarm is a package that acts as a user interface for Stan on the front end, and enables R users to create Bayesian models without needing to learn how to code in Stan. Using the standard formula and data, you can fit a model in rstanarm. If you want to use rstan to fit a different model type, you’ll have to code it yourself. The prefix stan_ precedes the model fitting functions and is followed by the model type. stan glm() and stan glmer() are two examples. A complete list of rstanarm functions can be found on Cran’s package guide. 2.0.6 Conclusions Summary This tutorial provided only a quick overview of how to fit simple linear regression models with the Bayesian software STAN and the rstan library and how to get a collection of useful summaries from the models. This is just a taste of the many models that STAN can fit; in future postings, we’ll look at generalised linear models, as well as non-normal models with various link functions and hierarchical models. The STAN model as provided here is quite adaptable and able to accommodate datasets of various sizes. Although this may appear to be a more difficult technique than just fitting a linear model in a frequentist framework, the real benefits of Bayesian methods become apparent as the analysis becomes more sophisticated (which is often the case in real applications), the flexibility of Bayesian modelling makes it very simple to account for increasingly complicated models. 2.1 References Stan Functions Reference Stan Users Guide Some things i’ve learned about stan Stan (+R) Workshop# The pool of tears "],["a-part-3.html", "Chapter 3 A Part 3", " Chapter 3 A Part 3 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
