[["index.html", "Stan Bookdown Chapter 1 Choosing the Right Bayesian Model", " Stan Bookdown Kamran Afzali 2025-06-20 Chapter 1 Choosing the Right Bayesian Model Introduction Bayesian modeling has become a central approach in modern data analysis, providing a coherent framework for incorporating prior knowledge and quantifying uncertainty. With the advent of powerful tools such as Stan and user-friendly interfaces like the R package brms, practitioners can now implement a wide array of Bayesian models with relative ease. However, the flexibility of the Bayesian framework also introduces a new challenge: selecting the most appropriate model for a given dataset and research question. The landscape of Bayesian models is vast, encompassing linear and generalized linear models, robust and regularized regressions, hierarchical models, and more sophisticated approaches such as Gaussian processes and mixture models. This guide aims to offer a structured approach to model selection within the Bayesian paradigm, focusing on practical considerations, data characteristics, and modeling objectives. Modeling Objectives and Data Characteristics The choice of a Bayesian model should begin with a clear understanding of the research objective. In broad terms, the aim of modeling can be categorized into two primary goals: inference and prediction. Inference focuses on understanding the relationships between variables, quantifying uncertainty in parameter estimates, and testing theoretical hypotheses. Prediction, on the other hand, emphasizes the accuracy of forecasting outcomes for new observations. While the two goals are not mutually exclusive, they can lead to different modeling choices, particularly in terms of model complexity and regularization. Another factor influencing model selection is the nature of the data. Key aspects include the type of response variable (continuous, binary, count, categorical), the presence of outliers or heavy-tailed distributions, the structure of the data (e.g., hierarchical or longitudinal), and the dimensionality of the predictor space. A careful examination of these characteristics provides essential guidance for selecting an appropriate Bayesian model. Prior Specification and Computational Considerations An essential feature of Bayesian modeling is the specification of prior distributions. Priors can be informative, weakly informative, or non-informative, depending on the amount of domain knowledge available. Informative priors are grounded in expert knowledge or historical data, while weakly informative priors help stabilize estimates without unduly influencing the posterior. Prior predictive checks can assess the implications of the priors before seeing the data, ensuring they encode plausible assumptions. Modelers should also perform sensitivity analyses to understand how different priors affect inferences. Computational feasibility is another practical concern. Some Bayesian models—especially nonparametric or high-dimensional ones—can be computationally intensive, requiring advanced MCMC algorithms or variational inference. Diagnostics such as the Gelman-Rubin R-hat statistic, effective sample size (ESS), and checks for divergent transitions should be used to ensure reliable inference (Gelman et al., 2013). Stan and brms provide tools to assess convergence and evaluate sampling efficiency. Bayesian Linear Regression Bayesian linear regression serves as the foundational model in the Bayesian framework. It assumes a linear relationship between predictors and a continuous response variable, with normally distributed residuals. This model is particularly useful for its simplicity and interpretability. When the assumptions of linearity and normality hold reasonably well, Bayesian linear regression provides reliable parameter estimates and predictive intervals. It also serves as a baseline model against which more complex models can be compared. In practice, Bayesian linear regression can be implemented in Stan with straightforward model code, specifying priors for the regression coefficients and residual variance. The flexibility of Bayesian inference allows for the incorporation of prior knowledge, which can be particularly valuable in small-sample contexts or when strong domain expertise is available. Robust Regression for Non-Normal Residuals Real-world data often deviate from the assumption of normally distributed residuals. Outliers or heavy-tailed distributions can exert undue influence on parameter estimates, leading to biased or unstable results. Bayesian robust regression addresses this issue by modeling the residuals using a t-distribution, which has heavier tails than the normal distribution. This approach reduces the influence of outliers, leading to more robust and reliable inferences. The implementation of robust regression in Stan involves specifying a likelihood based on the t-distribution and including an additional parameter for the degrees of freedom. This parameter controls the heaviness of the tails and can itself be estimated from the data. The robust regression model is particularly recommended when residual diagnostics from a standard linear model indicate non-normality or the presence of extreme observations. Regularized Regression for High-Dimensional Data When dealing with a large number of predictors or multicollinearity, regularization becomes essential to prevent overfitting and to enhance predictive performance. Bayesian regularized regression models incorporate shrinkage priors, such as the Laplace prior for Bayesian LASSO or the Gaussian prior for Bayesian ridge regression. These priors shrink the regression coefficients toward zero, effectively performing variable selection and regularization. In the Bayesian framework, regularization is naturally integrated through the prior distribution. For example, the Bayesian LASSO uses a double-exponential prior that induces sparsity by assigning higher probability mass near zero. These models are particularly useful in settings with more predictors than observations or when there is a need to identify the most influential variables. Models for Non-Normal Data In many applications, the response variable does not follow a normal distribution. Binary outcomes, count data, and categorical responses require specialized models. Bayesian generalized linear models (GLMs) extend the linear model framework to accommodate different types of response variables through appropriate link functions and likelihood distributions. For binary outcomes, the logistic regression model with a logit link is commonly used. For count data, Poisson and negative binomial models are appropriate, with the latter providing a flexible alternative in the presence of overdispersion. Multinomial and ordinal regression models are used for categorical outcomes, with the choice depending on whether the categories are ordered. These models are readily implemented in Stan and brms, allowing users to specify the appropriate family and link function. Model selection in this context should be guided by the distributional characteristics of the response variable and the research question at hand. Multilevel and Hierarchical Models Hierarchical data structures are common in social sciences, education, and biomedical research. In such settings, observations are nested within higher-level units, such as students within schools or patients within hospitals. Ignoring this structure can lead to biased inferences and underestimated uncertainty. Bayesian multilevel models explicitly account for the hierarchical structure by including group-level effects. These models allow for partial pooling of information across groups, balancing between complete pooling (ignoring group differences) and no pooling (treating each group separately). The brms package offers a user-friendly interface for fitting multilevel models, handling complex random effects structures with ease. The flexibility of Bayesian multilevel modeling also facilitates the inclusion of varying slopes, cross-level interactions, and non-linear effects. When the data structure suggests hierarchical dependencies, multilevel modeling should be the default approach. Nonlinear and Nonparametric Models In some applications, the relationship between predictors and the response variable is inherently nonlinear or unknown. Bayesian nonparametric models, such as Gaussian process regression, offer a flexible solution by modeling the function space directly. Gaussian processes define a prior over functions and use observed data to update this prior, resulting in a posterior distribution over functions. Gaussian process regression is particularly powerful when the form of the relationship is unknown or when modeling smooth, nonlinear trends is important. However, it comes at a higher computational cost and may not scale well with large datasets. Nevertheless, for problems involving spatial data, temporal trends, or complex functional relationships, Gaussian processes provide a valuable modeling tool. Mixture Models and Latent Structure Data arising from heterogeneous populations may be better modeled using mixture models. Bayesian Gaussian mixture models, for instance, assume that the data are generated from a mixture of several Gaussian distributions, each representing a subpopulation. These models can uncover latent structure in the data, such as clusters or subtypes. Mixture models introduce additional complexity due to the need to estimate both the component parameters and the mixing proportions. Bayesian inference provides a principled framework for dealing with this uncertainty, often using techniques such as latent variable augmentation and label switching adjustments. When there is reason to believe that the data comprise distinct subgroups with different underlying characteristics, mixture models offer an effective approach to modeling such heterogeneity. Comparative Summary Table Model Type Use Case Key Assumptions Priors Limitations Linear Regression Continuous outcome, low noise Linearity, normal errors Normal, Inverse-Gamma Poor with outliers Robust Regression Heavy-tailed residuals t-distributed residuals Prior on ν Increased complexity Regularized Regression High-dimensional predictors Sparsity Laplace, Gaussian Shrinkage may hide effects GLMs Binary/count/categorical outcomes Appropriate link function Varied Can overfit without strong priors Hierarchical Models Nested/grouped data Partial pooling Hierarchical priors Sensitive to group size Gaussian Processes Unknown nonlinear function Smoothness in kernel GP prior Poor scaling (O(n³)) Mixture Models Latent structure/clustering Finite components Dirichlet, etc. Label switching, identifiability Model Diagnostics and Comparison Choosing the right model also involves evaluating its performance and comparing it to alternative specifications. Bayesian model diagnostics include posterior predictive checks, which assess how well the model reproduces the observed data. Graphical comparisons between observed and replicated data can reveal model misfit or systematic discrepancies. Information criteria such as the Widely Applicable Information Criterion (WAIC) and Leave-One-Out Cross-Validation (LOO-CV) provide tools for model comparison, balancing fit and complexity. These criteria estimate the expected out-of-sample predictive performance and are particularly useful for selecting among nested or non-nested models (Vehtari, Gelman, &amp; Gabry, 2017). Bayes factors offer another method for model comparison, based on the ratio of marginal likelihoods. However, they are sensitive to prior specification and can be computationally intensive. In practice, WAIC and LOO-CV are often preferred for their robustness and ease of computation. A Decision Framework for Model Selection To aid practitioners in selecting the appropriate Bayesian model, a structured decision framework can be employed. This framework begins with identifying the type of response variable: continuous, binary, count, or categorical. Next, the data should be assessed for features such as outliers, overdispersion, hierarchical structure, and nonlinearity. Based on these characteristics, the modeler can then choose among linear models, robust regressions, generalized linear models, multilevel models, or nonparametric approaches. This decision process is iterative and should incorporate model diagnostics and domain knowledge. Starting with a simple model and progressively introducing complexity allows for a more transparent understanding of the data and the modeling assumptions. Each modeling choice should be justified in terms of its contribution to answering the research question and improving model fit. Conclusion Bayesian modeling offers unparalleled flexibility and rigor in statistical inference, but this power comes with the responsibility of thoughtful model selection. This guide has outlined the key considerations for choosing among the diverse array of Bayesian models available in tools like Stan and brms. By grounding model selection in the objectives of the analysis, the characteristics of the data, and robust diagnostic procedures, practitioners can make informed choices that enhance both the interpretability and predictive performance of their models. As with all statistical modeling, the process is iterative and benefits from a combination of statistical insight, computational tools, and substantive expertise. With this guide, researchers are better equipped to navigate the Bayesian modeling landscape and apply the appropriate models to their specific challenges. "],["bayesian-modeling-in-r-and-stan.html", "Chapter 2 Bayesian Modeling in R and Stan 2.1 What is STAN? 2.2 Model file 2.3 Fit the model 2.4 MCMC diagnostics 2.5 Parting thoughts 2.6 Conclusions 2.7 References", " Chapter 2 Bayesian Modeling in R and Stan The aim of this post is to provide a quick overview and introduction to fitting Bayesian models using STAN and R. For this, I strongly recommend installing Rstudio, an integrated development environment that allows a “user-friendly” interaction with R. 2.1 What is STAN? STAN is a tool for analysing Bayesian models using Markov Chain Monte Carlo (MCMC) methods. MCMC is a sampling method for estimating a probability distribution without knowing all of the features of the distribution. STAN is a probabilistic programming language and free software for specifying statistical models utilising Hamiltonian Monte Carlo methods (HMC), a type of MCMC algorithm. Stan works with the most widely used data-analysis languages including R and Python. In this quick overview, we’ll focus on the rstan package and demonstrate how to fit STAN models with it. For an example dataset, here we simulate our own data in R. We firsty create a continuous outcome variable y as a function of one predictor x and a disturbance term ϵ. I simulate a dataset with 100 observations. Create the error term, the predictor and the outcome using a linear form with an intercept β0, slope β1, etc. coefficients. set.seed(123) n &lt;- 1000 a &lt;- 40 #intercept b &lt;- c(-2, 3, 4, 1 , 0.25) #slopes sigma2 &lt;- 25 #residual variance (sd=5) x &lt;- matrix(rnorm(5000),1000,5) eps &lt;- rnorm(n, mean = 0, sd = sqrt(sigma2)) #residuals y &lt;- a +x%*%b+ eps #response variable data &lt;- data.frame(y, x) #dataset head(data)%&gt;%kableExtra::kable() y X1 X2 X3 X4 X5 33.51510 -0.5604756 -0.9957987 -0.5116037 -0.1503075 0.1965498 43.76098 -0.2301775 -1.0399550 0.2369379 -0.3277571 0.6501132 27.64712 1.5587083 -0.0179802 -0.5415892 -1.4481653 0.6710042 50.72614 0.0705084 -0.1321751 1.2192276 -0.6972846 -1.2841578 39.46286 0.1292877 -2.5493428 0.1741359 2.5984902 -2.0261096 39.42009 1.7150650 1.0405735 -0.6152683 -0.0374150 2.2053261 2.2 Model file STAN models are written in an imperative programming language, which means the order in which you write the elements in your model file matters, i.e. you must first define your variables (e.g. integers, vectors, matrices, etc.), then the constraints that define the range of values your variable can take (e.g. only positive values for standard deviations), and finally the relationship between the variables. A Stan model is defined by different blocks including: Data (required): The data block reads information from the outside world, such as data vectors, matrices, integers, and so on. We also need to define the lengths and dimensions of objects, which may appear unusual to those who are used to R or Python. The number of observations is first declared as an integer variable N: int N; (note the use of semicolon to denote the end of a line). The number of predictors in our model, which is K. The intercept is included in this count, so we end up with two predictors (2 columns in the model matrix). Transformed Data (optional): The converted data block enables data preprocessing, such as data transformation or rescaling. Parameters (required): The parameters block specifies the parameters that must be assigned to prior distributions. Transformed parameters (optional): Before computing the posterior, the changed parameters block provides for parameter processing, such as transformation or rescaling of the parameters. model_stan = &quot; data { // declare the input data / parameters } transformed data { // optional - for transforming/scaling input data } parameters { // define model parameters } transformed parameters { // optional - for deriving additional non-model parameters // note however, as they are part of the sampling chain // transformed parameters slow sampling down. } model { // specifying priors and likelihood as well as the linear predictor } generated quantities { // optional - derivatives (posteriors) of the samples } &quot; cat(model_stan) ## ## data { ## // declare the input data / parameters ## } ## transformed data { ## // optional - for transforming/scaling input data ## } ## parameters { ## // define model parameters ## } ## transformed parameters { ## // optional - for deriving additional non-model parameters ## // note however, as they are part of the sampling chain ## // transformed parameters slow sampling down. ## } ## model { ## // specifying priors and likelihood as well as the linear predictor ## } ## generated quantities { ## // optional - derivatives (posteriors) of the samples ## } ## For this introduction, I’ll use a very simple model in the STAN model that only involves the specification of three blocks. In the data block, I declare the variables y and x as reals (or vectors) with length equal to N and declare the sample size n sim as a positive integer number using the phrase int n sim. I define the coefficients for the linear regression alpha and beta (as real values) and the standard deviation parameter sigma in the parameters block (as a positive real number). Finally, in the model block, I give the regression coefficients and standard deviation parameters weakly informative priors, and I use a normal distribution indexed by the conditional mean mu and standard deviation sigma parameters to model the outcome data y. I give full recognition to McElreath’s outstanding Statistical Rethinking (2020) book for this section. stan_mod = &quot;data { int&lt;lower=0&gt; N; // number of observations int&lt;lower=0&gt; K; // number of predictors matrix[N, K] X; // predictor matrix vector[N] y; // outcome vector } parameters { real alpha; // intercept vector[K] beta; // coefficients for predictors real&lt;lower=0&gt; sigma; // error scale } model { y ~ normal(alpha + X * beta, sigma); // target density }&quot; writeLines(stan_mod, con = &quot;stan_mod.stan&quot;) cat(stan_mod) 2.3 Fit the model We’ll need to organise the information into a list for Stan. This list should contain everything we defined in the data block of our Stan code. Then it is possible to run the model with the stan function. library(tidyverse) predictors &lt;- data %&gt;% select(-y) stan_data &lt;- list( N = 1000, K = 5, X = predictors, y = data$y ) fit_rstan &lt;- rstan::stan( file = &quot;stan_mod.stan&quot;, data = stan_data ) fit_rstan ## Inference for Stan model: anon_model. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% ## alpha 40.19 0.00 0.15 39.89 40.08 40.19 40.29 40.48 ## beta[1] -2.07 0.00 0.16 -2.39 -2.18 -2.07 -1.97 -1.76 ## beta[2] 2.73 0.00 0.16 2.42 2.62 2.73 2.84 3.04 ## beta[3] 3.83 0.00 0.16 3.51 3.72 3.83 3.93 4.13 ## beta[4] 1.23 0.00 0.16 0.92 1.11 1.23 1.34 1.54 ## beta[5] 0.34 0.00 0.16 0.04 0.23 0.34 0.45 0.65 ## sigma 4.96 0.00 0.11 4.74 4.88 4.95 5.03 5.17 ## lp__ -2098.32 0.04 1.88 -2102.75 -2099.39 -2097.99 -2096.92 -2095.67 ## n_eff Rhat ## alpha 5280 1 ## beta[1] 5552 1 ## beta[2] 6251 1 ## beta[3] 5988 1 ## beta[4] 6172 1 ## beta[5] 6343 1 ## sigma 5687 1 ## lp__ 1916 1 ## ## Samples were drawn using NUTS(diag_e) at Wed May 4 13:23:35 2022. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 2.4 MCMC diagnostics For Bayesian analysis, it is required to investigate the properties of the MCMC chains and the sampler in general, in addition to the standard model diagnostic checks (such as residual plots). Remember that the goal of MCMC sampling is to reproduce the posterior distribution of the model likelihood and priors by formulating a probability distribution. This method reliable, only if the MCMC samples accurately reflect the posterior. For each parameter, traceplots show the MCMC sample values after each iteration along the chain. Bad chain mixing (any pattern) indicates that the MCMC sample chains may not have spanned all aspects of the posterior distribution and that further iterations are needed to ensure that the distribution is accurately represented. Each parameter’s autocorrelation graphic shows the degree of correlation between MCMC samples separated by different lags. The degree of correlation between each MCMC sample and itself, for example, is represented by a lag of (obviously this will be a correlation of ). The MCMC samples should be independent in order to obtain unbiased parameter estimations (uncorrelated). For each parameter, the potential scale reduction factor (Rhat) statistic offers a measure of sampling efficiency/effectiveness. All values should, in theory, be less than 1, if the sampler has values of or greater than 1, it is likely that it was not particularly efficient or effective. A misspecified model or extremely unclear priors that lead to misspecified parameter space might lead to this. We should have looked at the convergence diagnostics before looking at the summaries. For the effects model, we utilise the package mcmcplots to generate density and trace graphs. It’s crucial to evaluate if the chains have converged when using MCMC to fit a model. To visually inspect MCMC diagnostics, we propose the bayesplot software. The bayesplot package includes routines for displaying MCMC diagnostics and supports model objects from both rstan and rstanarm. We’ll show how to make a trace plot with the mcmc_trace() function and a plot of Rhat values with the mcmc rhat() function. By printing the model fit, we can examine the parameters in the console. For each parameter, we derive posterior means, standard errors, and quantiles. n eff and Rhat are two other terms we use. These are the results of Stan’s engine’s exploration of the parameter space. For the time being, it’s enough to know that when Rhat is 1, everything is well. fit_rstan %&gt;% mcmc_trace() fit_rstan %&gt;% rhat() %&gt;% mcmc_rhat() + yaxis_text() 2.5 Parting thoughts To finish this post, I’d like to point out that the rstanarm package makes it possible to fit STAN models without having to write them down and instead using standard R syntax, such as that found in a glm(). So, what’s the point of learning all this STAN jargon? It depends: if you’re merely fitting “classical” models to your data with no fanfare, just use rstanarm; it’ll save you time and the models in this package are unquestionably better parametrized (i.e. faster) than the one I presented here. Learning STAN, on the other hand, is a good approach to get into a very flexible and strong language that will continue to evolve if you believe you will need to fit your own models one day. rstanarm is a package that acts as a user interface for Stan on the front end, and enables R users to create Bayesian models without needing to learn how to code in Stan. Using the standard formula and data, you can fit a model in rstanarm. If you want to use rstan to fit a different model type, you’ll have to code it yourself. The prefix stan_ precedes the model fitting functions and is followed by the model type. stan glm() and stan glmer() are two examples. A complete list of rstanarm functions can be found on Cran’s package guide. 2.6 Conclusions This tutorial provided only a quick overview of how to fit simple linear regression models with the Bayesian software STAN and the rstan library and how to get a collection of useful summaries from the models. This is just a taste of the many models that STAN can fit; in future postings, we’ll look at generalised linear models, as well as non-normal models with various link functions and hierarchical models. The STAN model as provided here is quite adaptable and able to accommodate datasets of various sizes. Although this may appear to be a more difficult technique than just fitting a linear model in a frequentist framework, the real benefits of Bayesian methods become apparent as the analysis becomes more sophisticated (which is often the case in real applications), the flexibility of Bayesian modelling makes it very simple to account for increasingly complicated models. 2.7 References Stan Functions Reference Stan Users Guide Some things i’ve learned about stan Stan (+R) Workshop# The pool of tears "],["bayesian-regression-models-for-non-normal-data.html", "Chapter 3 Bayesian Regression Models for Non-Normal Data 3.1 Logistic Regression 3.2 Negative Binomial 3.3 Conclusion 3.4 References", " Chapter 3 Bayesian Regression Models for Non-Normal Data Our last post covered how to do bayesian regression for normally distributed data in R using STAN. In this post, we’ll take a look at how to fit a regression model adapted to non-normal model in STAN using two common distributions seen in empirical data, namely, binomial and negative-binomial. As mentioned before in Bayesian modelling we use a set of sampling methods known as Markov Chain Monte Carlo (MCMC), we define a statistical model and find probabilistic estimates for the parameters. 3.1 Logistic Regression The likelihood of a binary outcome, such as pass or fail, is estimated using logistic models (but this model can be extended to include more than two outcomes). This is accomplished by using the logit function to convert a standard regression. The main parameter that we focus on here describes odds of the outcome derived from probabilities and transformed using a logit function. The following is the code for a logistic regression model with one predictor and an intercept. library(tidyverse) library(kableExtra) library(arm) library(emdbook) library(rstan) library(rstanarm) set.seed(1234) x1 = rnorm(10000) z = 1 + 2*x1 pr = 1/(1+exp(-z)) y = rbinom(10000,1,pr) df = data.frame(y=y,x1=x1) head(df)%&gt;%kableExtra::kable() y x1 0 -1.2070657 0 0.2774292 1 1.0844412 0 -2.3456977 1 0.4291247 1 0.5060559 glm( y~x1,data=df,family=&quot;binomial&quot;)%&gt;%summary()%&gt;%pluck(coefficients)%&gt;%kableExtra::kable() Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.036922 0.0294863 35.16621 0 x1 1.979352 0.0417219 47.44162 0 model_stan = &quot; data { int&lt;lower=0&gt; N; vector[N] x; int&lt;lower=0,upper=1&gt; y[N]; } parameters { real alpha; real beta; } model { y ~ bernoulli_logit(alpha + beta * x); } &quot; writeLines(model_stan, con = &quot;model_stan.stan&quot;) cat(model_stan) ## ## data { ## int&lt;lower=0&gt; N; ## vector[N] x; ## int&lt;lower=0,upper=1&gt; y[N]; ## } ## parameters { ## real alpha; ## real beta; ## } ## model { ## y ~ bernoulli_logit(alpha + beta * x); ## } ## stan_data &lt;- list( N = 10000, x = df$x1, y = df$y ) fit_rstan &lt;- rstan::stan( file = &quot;model_stan.stan&quot;, data = stan_data ) fit_rstan ## Inference for Stan model: anon_model. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff ## alpha 1.04 0.00 0.03 0.98 1.02 1.04 1.06 1.10 1936 ## beta 1.98 0.00 0.04 1.90 1.95 1.98 2.01 2.06 1804 ## lp__ -4339.45 0.03 1.05 -4342.29 -4339.80 -4339.12 -4338.74 -4338.49 1479 ## Rhat ## alpha 1 ## beta 1 ## lp__ 1 ## ## Samples were drawn using NUTS(diag_e) at Tue Jun 21 14:03:59 2022. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). This also can be expanded to several predictors. Below you can find the code as well as some recommendations for making sense of priors. set.seed(1234) x1 = rnorm(10000) x2 = rnorm(10000) x3 = rnorm(10000) z = 1 + 2*x1 + 3*x2 - 1*x3 pr = 1/(1+exp(-z)) y = rbinom(10000,1,pr) df2 = data.frame(y=y,x1=x1,x2=x2,x3=x3) head(df2)%&gt;%kableExtra::kable() y x1 x2 x3 0 -1.2070657 -1.8168975 -1.6878627 1 0.2774292 0.6271668 -0.9552011 1 1.0844412 0.5180921 -0.6480572 0 -2.3456977 0.1409218 0.2610342 1 0.4291247 1.4572719 -1.2196940 1 0.5060559 -0.4935965 -1.5501888 glm( y~x1+x2+x3,data=df2,family=&quot;binomial&quot;)%&gt;%summary()%&gt;%pluck(coefficients)%&gt;%kableExtra::kable() Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.0108545 0.0367817 27.48253 0 x1 1.9471836 0.0494298 39.39289 0 x2 2.9598129 0.0641890 46.11088 0 x3 -0.9448316 0.0372918 -25.33619 0 model_stan2 =&quot; data { int&lt;lower=0&gt; N; // number of observations int&lt;lower=0&gt; K; // number of predictors matrix[N, K] X; // predictor matrix int&lt;lower=0,upper=1&gt; y[N]; // outcome vector } parameters { real alpha; // intercept vector[K] beta; // coefficients for predictors } model { y ~ bernoulli_logit(alpha + X * beta); } &quot; writeLines(model_stan2, con = &quot;model_stan2.stan&quot;) cat(model_stan2) ## ## ## ## data { ## int&lt;lower=0&gt; N; // number of observations ## int&lt;lower=0&gt; K; // number of predictors ## matrix[N, K] X; // predictor matrix ## int&lt;lower=0,upper=1&gt; y[N]; // outcome vector ## } ## parameters { ## real alpha; // intercept ## vector[K] beta; // coefficients for predictors ## } ## model { ## y ~ bernoulli_logit(alpha + X * beta); ## } predictors &lt;- df2[,2:4] stan_data2 &lt;- list( N = 10000, K = 3, X = predictors, y = df2$y ) fit_rstan2 &lt;- rstan::stan( file = &quot;model_stan2.stan&quot;, data = stan_data2 ) ## Trying to compile a simple C file fit_rstan2 ## Inference for Stan model: anon_model. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% ## alpha 1.01 0.00 0.04 0.94 0.99 1.01 1.04 1.08 ## beta[1] 1.95 0.00 0.05 1.85 1.92 1.95 1.98 2.05 ## beta[2] 2.96 0.00 0.06 2.84 2.92 2.96 3.00 3.09 ## beta[3] -0.95 0.00 0.04 -1.02 -0.97 -0.95 -0.92 -0.87 ## lp__ -3006.39 0.03 1.38 -3009.82 -3007.07 -3006.08 -3005.37 -3004.68 ## n_eff Rhat ## alpha 2845 1 ## beta[1] 2338 1 ## beta[2] 2221 1 ## beta[3] 2160 1 ## lp__ 1788 1 ## ## Samples were drawn using NUTS(diag_e) at Tue Jun 21 14:05:38 2022. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 3.2 Negative Binomial The Poisson distribution, which assumes that the variance is equal to the mean, is a popular choice for modelling count data. The data are said to be overdispersed when the variance exceeds the mean, the Negative Binomial distribution can be used in this case. Note that the negative binomial distribution is a probability distribution of discrete random variables. Let’s say we have a response variable y that has a negative binomial distribution and is influenced by a collection of k explanatory variables X. The negative binomial distribution to model this data has two parameters: The μ or the expected value that need to be positive so a log link function can be used to map the linear predictor (the explanatory variables times the regression parameters) and ϕ which is the overdispersion parameter, where a small value means a large deviation from a Poisson distribution, as ϕ gets larger the negative binomial looks more and more like a Poisson distribution. Let’s simulate some data and fit a STAN model to them: N&lt;-100000 df3 &lt;-data.frame(x1=runif(N,-2,2),x2=runif(N,-2,2)) #the model X&lt;-model.matrix(~x1*x2,df3) K&lt;-dim(X)[2] #number of regression params #the regression slopes betas&lt;-runif(K,-1,1) #the overdispersion for the simulated data phi&lt;-5 #simulate the response y_nb&lt;-rnbinom(N,size=phi,mu=exp(X%*%betas)) hist(y_nb) MASS::glm.nb(y_nb ~ X[,2:K])%&gt;%summary()%&gt;%pluck(coefficients)%&gt;%kableExtra::kable() Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.0788902 0.0039192 20.12938 0 X[, 2:K]x1 -0.0815968 0.0032703 -24.95109 0 X[, 2:K]x2 0.7941334 0.0031740 250.20341 0 X[, 2:K]x1:x2 0.4301282 0.0026264 163.77308 0 data { int N; //the number of observations int K; //the number of columns in the model matrix int y[N]; //the response matrix[N,K] X; //the model matrix } parameters { vector[K] beta; //the regression parameters real phi; //the overdispersion parameters } transformed parameters { vector[N] mu;//the linear predictor mu &lt;- exp(X*beta); //using the log link } model { beta[1] ~ cauchy(0,10); //prior for the intercept following Gelman 2008 for(i in 2:K) beta[i] ~ cauchy(0,2.5);//prior for the slopes following Gelman 2008 y ~ neg_binomial_2(mu,phi); } generated quantities { vector[N] y_rep; for(n in 1:N){ y_rep[n] &lt;- neg_binomial_2_rng(mu[n],phi); //posterior draws to get posterior predictive checks } } stan_mod = &quot;data { int N; //the number of observations int K; //the number of columns in the model matrix int y[N]; //the response matrix[N,K] X; //the model matrix } parameters { vector[K] beta; //the regression parameters real phi; //the overdispersion parameters } transformed parameters { vector[N] mu;//the linear predictor mu &lt;- exp(X*beta); //using the log link } model { beta[1] ~ cauchy(0,10); //prior for the intercept following Gelman 2008 for(i in 2:K) beta[i] ~ cauchy(0,2.5);//prior for the slopes following Gelman 2008 y ~ neg_binomial_2(mu,phi); } &quot; writeLines(stan_mod, con = &quot;stan_mod.stan&quot;) cat(stan_mod) ## data { ## int N; //the number of observations ## int K; //the number of columns in the model matrix ## int y[N]; //the response ## matrix[N,K] X; //the model matrix ## } ## parameters { ## vector[K] beta; //the regression parameters ## real phi; //the overdispersion parameters ## } ## transformed parameters { ## vector[N] mu;//the linear predictor ## mu &lt;- exp(X*beta); //using the log link ## } ## model { ## beta[1] ~ cauchy(0,10); //prior for the intercept following Gelman 2008 ## ## for(i in 2:K) ## beta[i] ~ cauchy(0,2.5);//prior for the slopes following Gelman 2008 ## ## y ~ neg_binomial_2(mu,phi); ## } stan_data3 &lt;- list( N = N, K = K, X = X, y = y_nb ) fit_rstan3 &lt;- rstan::stan( file = &quot;stan_mod.stan&quot;, data = stan_data3 ) fit_rstan3%&gt;%summary() ## $summary ## mean se_mean sd 2.5% 25% ## beta[1] 7.899069e-02 6.825217e-05 0.0039010484 7.162531e-02 7.635923e-02 ## beta[2] -8.157657e-02 5.689044e-05 0.0032236243 -8.786688e-02 -8.376046e-02 ## beta[3] 7.941039e-01 5.618629e-05 0.0031476117 7.880475e-01 7.919520e-01 ## beta[4] 4.301036e-01 4.552321e-05 0.0026265744 4.248578e-01 4.283603e-01 ## phi 4.908958e+00 2.039012e-03 0.0831883094 4.745765e+00 4.854542e+00 3.3 Conclusion This tutorial provided only a quick overview of how to fit logistic and negative binomial regression models with the Bayesian software STAN using the rstan library/API and to extract a collection of useful summaries from the models. Future postings will address the question of outliers and the use of robust linear models. 3.4 References Stan Functions Reference Stan Users Guide Some things i’ve learned about stan Stan (+R) Workshop "],["appendix.html", "Chapter 4 Appendix 4.1 Main Stan Distributions Cheatsheet 4.2 Main Stan Functions Cheatsheet 4.3 Why Non-Distribution Functions? 4.4 Stan Functions Cheatsheet 4.5 Example: Hierarchical Linear Regression 4.6 Tips for Using Stan Functions", " Chapter 4 Appendix 4.1 Main Stan Distributions Cheatsheet Statistical modeling in Stan is powered by a flexible and expressive probabilistic language grounded in log-density functions. While the modeling blocks (model, data, parameters, etc.) help structure a model, the core statistical logic is defined through distributions. This cheatsheet offers a practical summary of the most important distributions used in Stan, their syntax, required parameters, typical use cases, and examples of where they show up in statistical modeling. Distribution Function Parameters Use Case Model Type(s) Bernoulli `bernoulli_lpmf(y θ)` θ ∈ (0, 1) Binary outcome (0/1) Logistic regression, classification Binomial `binomial_lpmf(y n, θ)` n ∈ ℕ⁺, θ ∈ (0, 1) # of successes in n trials Logistic GLMs, grouped binomial models Categorical `categorical_lpmf(y θ)` θ: simplex vector (length K) Single draw from K categories Multinomial regression Multinomial `multinomial_lpmf(y θ)` y: int vector of counts, θ: simplex Category count data Count models with category splits Normal `normal_lpdf(y μ, σ)` μ ∈ ℝ, σ &gt; 0 Gaussian noise, residuals Linear regression, priors for real parameters Student’s t `student_t_lpdf(y ν, μ, σ)` ν &gt; 0, μ ∈ ℝ, σ &gt; 0 Heavy-tailed data, robust models Robust regression, hierarchical priors Cauchy `cauchy_lpdf(y μ, σ)` μ ∈ ℝ, σ &gt; 0 Weakly informative, heavy-tailed prior Priors on scale parameters (e.g., τ ~ cauchy(0, 2.5)) Exponential `exponential_lpdf(y λ)` λ &gt; 0 Time to event, memoryless processes Survival models, Poisson process modeling Gamma `gamma_lpdf(y α, β)` α &gt; 0, β &gt; 0 Positive skewed data Priors on rates or shape parameters Inverse Gamma `inv_gamma_lpdf(y α, β)` α &gt; 0, β &gt; 0 Prior for variances Priors on σ², τ², especially in hierarchies Lognormal `lognormal_lpdf(y μ, σ)` μ ∈ ℝ, σ &gt; 0 Positive, right-skewed data Income, durations, reliability Beta `beta_lpdf(y α, β)` α &gt; 0, β &gt; 0 Probabilities or proportions Priors on probabilities (θ ∈ (0, 1)) Dirichlet `dirichlet_lpdf(θ α)` θ: simplex, α &gt; 0 vector Probabilities summing to 1 Priors for category proportions, LDA Poisson `poisson_lpmf(y λ)` λ &gt; 0 Count data, rare event modeling GLMs for count data Negative Binomial `neg_binomial_2_lpmf(y μ, φ)` μ &gt; 0, φ &gt; 0 Overdispersed count data GLMs with extra-Poisson variation Ordered Logistic `ordered_logistic_lpmf(y η, c)` η ∈ ℝ, c: ordered cut-points Ordinal outcomes Ordinal regression Uniform `uniform_lpdf(y a, b)` a &lt; b Flat prior within range Non-informative priors Pareto `pareto_lpdf(y y_min, α)` y_min &gt; 0, α &gt; 0 Heavy-tail data, power-law phenomena Extremes, outlier modeling Von Mises `von_mises_lpdf(y μ, κ)` μ ∈ [0, 2π), κ ≥ 0 Circular data (angles, wind direction) Directional models Weibull `weibull_lpdf(y α, σ)` α, σ &gt; 0 Survival times, failure rates Survival models, reliability analysis LKJ Correlation `lkj_corr_cholesky_lpdf(L η)` η &gt; 0, L: Cholesky factor Prior for correlation matrices Hierarchical models with random slopes Wishart `wishart_lpdf(S ν, Σ)` ν &gt; dim-1, Σ: scale matrix Prior on covariance matrices Multivariate Gaussian models (rarely used) 4.2 Main Stan Functions Cheatsheet Stan is a robust platform for Bayesian statistical modeling, renowned for its Hamiltonian Monte Carlo (HMC) engine and flexible modeling language. While probability distributions like normal_lpdf or poisson_lpmf define priors and likelihoods, Stan’s non-distribution functions—spanning mathematical operations, matrix algebra, utility tools, and specialized solvers—are equally critical for building efficient and expressive models. These functions enable data transformations, efficient computations, and post-processing in the generated quantities block. This cheatsheet organizes Stan’s most commonly used non-distribution functions into categories, providing their purpose, example usage, and the model types where they’re most applicable. Whether you’re crafting linear regressions, hierarchical models, or dynamic systems, this guide will help you leverage Stan’s toolkit effectively. We’ll wrap up with an example model to bring these functions to life. 4.3 Why Non-Distribution Functions? Stan’s non-distribution functions serve several key purposes: - Transformations: Functions like log, exp, and inv_logit map parameters to constrained spaces or perform nonlinear calculations. - Matrix Operations: Functions like dot_product and cholesky_decompose enable efficient linear algebra for multivariate models. - Utilities: Functions like to_vector and mean simplify data manipulation and posterior summaries. - Specialized Tools: Solvers like ode_rk45 and integrate_1d tackle complex systems, such as differential equations or custom likelihoods. - Posterior Processing: Functions in the generated quantities block, like sum or sd, compute diagnostics or predictions. This cheatsheet focuses on these functions to help you streamline model specification and analysis. 4.4 Stan Functions Cheatsheet 4.4.1 1. Mathematical Functions These functions perform scalar operations, often used in transformed parameters or model blocks. Function Purpose Example Usage Model Type(s) abs(x) Absolute value real z = abs(x); General computations, robust stats exp(x) Exponential (e^x) lambda = exp(alpha); Rate models, transformations log(x) Natural logarithm real l = log(y); Log-likelihoods, transformations sqrt(x) Square root sigma = sqrt(variance); Variance computations, scaling lgamma(x) Log gamma function lp += lgamma(alpha); Mixture models, custom likelihoods log_sum_exp(x) Log-sum-exp for numerical stability lp = log_sum_exp(log_theta); Mixture models, marginal likelihoods 4.4.2 2. Transformation Functions These map parameters to constrained spaces, often in transformed parameters. Function Purpose Example Usage Model Type(s) inv_logit(x) Logistic sigmoid (ℝ → (0,1)) theta = inv_logit(alpha + beta*x); Logistic regression, probability models logit(p) Log-odds ((0,1) → ℝ) eta = logit(p); Logistic regression, probit models softmax(x) Normalize vector to simplex theta = softmax(alpha); Multinomial regression, LDA inv(x) Reciprocal (1/x) inv_sigma = inv(sigma); Variance transformations 4.4.3 3. Matrix and Vector Operations These enable efficient linear algebra, critical for multivariate and hierarchical models. Function Purpose Example Usage Model Type(s) dot_product(a, b) Inner product of two vectors real z = dot_product(a, b); Linear regression, similarity measures matrix_times_vector(A, v) Matrix-vector multiplication eta = matrix_times_vector(X, beta); Multivariate regression, GLMs cholesky_decompose(S) Cholesky factorization L = cholesky_decompose(Sigma); Hierarchical models, multivariate normals multiply_lower_tri_self_transpose(L) Covariance from Cholesky factor Sigma = multiply_lower_tri_self_transpose(L); Multivariate normals, hierarchical models diag_matrix(v) Diagonal matrix from vector M = diag_matrix(v); Covariance priors, scaling determinant(A) Matrix determinant det = determinant(Sigma); Model diagnostics, multivariate priors 4.4.4 4. Utility Functions These simplify data manipulation and posterior summaries, often in generated quantities. Function Purpose Example Usage Model Type(s) to_vector(x) Convert matrix/array to vector vec = to_vector(matrix); Posterior summaries, data reshaping to_array_1d(x) Convert to 1D array arr = to_array_1d(matrix); Data preprocessing, summaries sum(x) Sum of elements total = sum(y); Aggregations, diagnostics mean(x) Mean of elements avg = mean(y_rep); Posterior summaries, diagnostics sd(x) Standard deviation std = sd(y_rep); Posterior summaries, diagnostics int_step(x) Indicator (x ≥ 0 → 1, else 0) flag = int_step(x - 1); Conditional logic, model diagnostics 4.4.5 5. Specialized Solvers These handle advanced computations like differential equations or parallel processing. Function Purpose Example Usage Model Type(s) ode_rk45(fun, y0, t0, ts, ...) Solve ODEs (Runge-Kutta 45) y = ode_rk45(ode_sys, y0, t0, ts, params); Dynamic systems, pharmacokinetics integrate_1d(f, a, b, ...) Numerical integration val = integrate_1d(f, a, b, params); Custom likelihoods, marginalization map_rect(f, phi, ...) Parallel computation over data shards results = map_rect(f, phi, theta, data); Large-scale hierarchical models 4.5 Example: Hierarchical Linear Regression Here’s a Stan model for a hierarchical linear regression, using matrix_times_vector, to_vector, and mean to demonstrate practical function usage: data { int&lt;lower=0&gt; N; // Number of observations int&lt;lower=0&gt; J; // Number of groups array[N] int&lt;lower=1,upper=J&gt; group; // Group indicators matrix[N, 2] X; // Design matrix (intercept + predictor) vector[N] y; // Outcome } parameters { vector[2] beta; // Fixed effects vector[J] alpha; // Group-level intercepts real&lt;lower=0&gt; sigma; // Residual standard deviation real&lt;lower=0&gt; tau; // Standard deviation of group intercepts } model { beta ~ normal(0, 5); // Prior on fixed effects tau ~ cauchy(0, 2.5); // Prior on group SD alpha ~ normal(0, tau); // Group-level priors sigma ~ cauchy(0, 2.5); // Prior on residual SD vector[N] mu = matrix_times_vector(X, beta) + to_vector(alpha[group]); y ~ normal(mu, sigma); // Likelihood } generated quantities { vector[N] y_rep; // Posterior predictive real mean_y_rep; // Mean of predictions for (n in 1:N) { y_rep[n] = normal_rng(matrix_times_vector(X[n], beta) + alpha[group[n]], sigma); } mean_y_rep = mean(to_vector(y_rep)); // Summary statistic } This model: - Uses matrix_times_vector to compute the linear predictor efficiently. - Employs to_vector to align group-level intercepts with observations. - Computes mean_y_rep in generated quantities using mean and to_vector for posterior diagnostics. - Generates predictions with normal_rng for posterior predictive checks. 4.6 Tips for Using Stan Functions Efficiency: Prefer vectorized operations like matrix_times_vector over loops for speed. Numerical Stability: Use log_sum_exp for summing exponentials to avoid overflow. Posterior Analysis: Leverage mean, sd, and to_vector in generated quantities for summaries and diagnostics. Constraints: Ensure inputs meet requirements (e.g., x &gt; 0 for log, positive-definite matrices for cholesky_decompose). Advanced Modeling: Use ode_rk45 for dynamic systems or map_rect for parallelized large-scale models. Documentation: The Stan Reference Manual (e.g., version 2.33) and Stan’s GitHub examples provide detailed guidance. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
