[["index.html", "Stan Bookdown Chapter 1 Choosing the Right Bayesian Model", " Stan Bookdown Kamran Afzali 2025-10-16 Chapter 1 Choosing the Right Bayesian Model Introduction Bayesian modeling has become a central approach in modern data analysis, providing a coherent framework for incorporating prior knowledge and quantifying uncertainty. With the advent of powerful tools such as Stan and user-friendly interfaces like the R package brms, practitioners can now implement a wide array of Bayesian models with relative ease. However, the flexibility of the Bayesian framework also introduces a new challenge: selecting the most appropriate model for a given dataset and research question. The landscape of Bayesian models is vast, encompassing linear and generalized linear models, robust and regularized regressions, hierarchical models, time series models, multivariate approaches, and more sophisticated methods such as Gaussian processes and mixture models. This guide aims to offer a structured approach to model selection within the Bayesian paradigm, focusing on practical considerations, data characteristics, and modeling objectives. Modeling Objectives and Data Characteristics The choice of a Bayesian model should begin with a clear understanding of the research objective. In broad terms, the aim of modeling can be categorized into two primary goals: inference and prediction. Inference focuses on understanding the relationships between variables, quantifying uncertainty in parameter estimates, and testing theoretical hypotheses. Prediction, on the other hand, emphasizes the accuracy of forecasting outcomes for new observations. While the two goals are not mutually exclusive, they can lead to different modeling choices, particularly in terms of model complexity and regularization. Another factor influencing model selection is the nature of the data. Key aspects include the type of response variable (continuous, binary, count, categorical), the temporal structure of observations (time series, longitudinal data), the presence of outliers or heavy-tailed distributions, the structure of the data (e.g., hierarchical or longitudinal), and the dimensionality of the predictor space. A careful examination of these characteristics provides essential guidance for selecting an appropriate Bayesian model. Prior Specification and Computational Considerations An essential feature of Bayesian modeling is the specification of prior distributions. Priors can be informative, weakly informative, or non-informative, depending on the amount of domain knowledge available. Informative priors are grounded in expert knowledge or historical data, while weakly informative priors help stabilize estimates without unduly influencing the posterior. Prior predictive checks can assess the implications of the priors before seeing the data, ensuring they encode plausible assumptions. Modelers should also perform sensitivity analyses to understand how different priors affect inferences. Computational feasibility is another practical concern. Some Bayesian models—especially nonparametric or high-dimensional ones—can be computationally intensive, requiring advanced MCMC algorithms or variational inference. Diagnostics such as the Gelman-Rubin R-hat statistic, effective sample size (ESS), and checks for divergent transitions should be used to ensure reliable inference (Gelman et al., 2013). Stan and brms provide tools to assess convergence and evaluate sampling efficiency. Bayesian Linear Regression Bayesian linear regression serves as the foundational model in the Bayesian framework. It assumes a linear relationship between predictors and a continuous response variable, with normally distributed residuals. This model is particularly useful for its simplicity and interpretability. When the assumptions of linearity and normality hold reasonably well, Bayesian linear regression provides reliable parameter estimates and predictive intervals. It also serves as a baseline model against which more complex models can be compared. In practice, Bayesian linear regression can be implemented in Stan with straightforward model code, specifying priors for the regression coefficients and residual variance. The flexibility of Bayesian inference allows for the incorporation of prior knowledge, which can be particularly valuable in small-sample contexts or when strong domain expertise is available. Robust Regression for Non-Normal Residuals Real-world data often deviate from the assumption of normally distributed residuals. Outliers or heavy-tailed distributions can exert undue influence on parameter estimates, leading to biased or unstable results. Bayesian robust regression addresses this issue by modeling the residuals using a t-distribution, which has heavier tails than the normal distribution. This approach reduces the influence of outliers, leading to more robust and reliable inferences. The implementation of robust regression in Stan involves specifying a likelihood based on the t-distribution and including an additional parameter for the degrees of freedom. This parameter controls the heaviness of the tails and can itself be estimated from the data. The robust regression model is particularly recommended when residual diagnostics from a standard linear model indicate non-normality or the presence of extreme observations. Regularized Regression for High-Dimensional Data When dealing with a large number of predictors or multicollinearity, regularization becomes essential to prevent overfitting and to enhance predictive performance. Bayesian regularized regression models incorporate shrinkage priors, such as the Laplace prior for Bayesian LASSO or the Gaussian prior for Bayesian ridge regression. These priors shrink the regression coefficients toward zero, effectively performing variable selection and regularization. In the Bayesian framework, regularization is naturally integrated through the prior distribution. For example, the Bayesian LASSO uses a double-exponential prior that induces sparsity by assigning higher probability mass near zero. These models are particularly useful in settings with more predictors than observations or when there is a need to identify the most influential variables. Models for Non-Normal Data In many applications, the response variable does not follow a normal distribution. Binary outcomes, count data, and categorical responses require specialized models. Bayesian generalized linear models (GLMs) extend the linear model framework to accommodate different types of response variables through appropriate link functions and likelihood distributions. For binary outcomes, the logistic regression model with a logit link is commonly used. For count data, Poisson and negative binomial models are appropriate, with the latter providing a flexible alternative in the presence of overdispersion. Multinomial and ordinal regression models are used for categorical outcomes, with the choice depending on whether the categories are ordered. These models are readily implemented in Stan and brms, allowing users to specify the appropriate family and link function. Model selection in this context should be guided by the distributional characteristics of the response variable and the research question at hand. Multilevel and Hierarchical Models Hierarchical data structures are common in social sciences, education, and biomedical research. In such settings, observations are nested within higher-level units, such as students within schools or patients within hospitals. Ignoring this structure can lead to biased inferences and underestimated uncertainty. Bayesian multilevel models explicitly account for the hierarchical structure by including group-level effects. These models allow for partial pooling of information across groups, balancing between complete pooling (ignoring group differences) and no pooling (treating each group separately). The brms package offers a user-friendly interface for fitting multilevel models, handling complex random effects structures with ease. The flexibility of Bayesian multilevel modeling also facilitates the inclusion of varying slopes, cross-level interactions, and non-linear effects. When the data structure suggests hierarchical dependencies, multilevel modeling should be the default approach. Canonical Correlation Analysis When the research objective involves understanding the relationship between two sets of multivariate observations, Bayesian canonical correlation analysis provides a principled framework for multivariate association. This approach identifies linear combinations of variables from each set that are maximally correlated, revealing the underlying structure of relationships between variable groups. Bayesian canonical correlation extends traditional methods by providing uncertainty quantification for canonical correlations and loadings, allowing for more robust inference about multivariate relationships. The Bayesian formulation typically employs matrix-variate priors on the canonical vectors and incorporates shrinkage to prevent overfitting in high-dimensional settings. This approach is particularly valuable when exploring relationships between groups of predictors and outcomes, such as relating brain imaging measures to cognitive assessments or linking genomic profiles to phenotypic characteristics. The posterior distribution provides natural measures of uncertainty for the canonical correlations and facilitates model comparison through Bayes factors or information criteria. Time Series Models: ARIMA and Seasonal Decomposition Time series data require specialized modeling approaches that account for temporal dependence and potential seasonality. Bayesian autoregressive integrated moving average (ARIMA) models provide a flexible framework for modeling univariate time series with trend and seasonal components. The Bayesian approach to ARIMA modeling offers advantages over classical methods by naturally incorporating parameter uncertainty and allowing for informative priors based on domain knowledge. Seasonal decomposition within the Bayesian framework explicitly separates time series into trend, seasonal, and irregular components. This decomposition can be achieved through state space formulations that treat each component as a latent process with its own dynamics. The Bayesian treatment provides posterior distributions for each component, enabling uncertainty quantification for trend estimates and seasonal patterns. This approach is particularly useful for time series exhibiting clear seasonal patterns, such as economic indicators, climate data, or retail sales. The implementation of Bayesian ARIMA and seasonal decomposition models often requires careful specification of priors on autoregressive and moving average parameters, ensuring stationarity and invertibility constraints are respected. Modern packages like bsts in R provide convenient interfaces for these models, handling the complexities of state space formulations and MCMC sampling. Exponential Smoothing: Bayesian Holt-Winters The Holt-Winters exponential smoothing method can be formulated within a Bayesian framework to provide uncertainty quantification for forecasts. Bayesian Holt-Winters models treat the level, trend, and seasonal components as evolving parameters with their own prior distributions. This approach naturally handles the adaptive nature of exponential smoothing while providing credible intervals for predictions. The Bayesian formulation typically specifies prior distributions on the smoothing parameters (alpha, beta, gamma) and the initial state values. This allows for the incorporation of prior beliefs about the relative importance of recent versus historical observations and the stability of trend and seasonal patterns. The resulting posterior distributions provide not only point forecasts but also prediction intervals that reflect both parameter uncertainty and inherent noise in the time series. Bayesian Holt-Winters models are particularly valuable for forecasting applications where quantifying prediction uncertainty is crucial, such as supply chain planning, financial forecasting, or resource allocation. The ability to incorporate prior knowledge about seasonal patterns and trend behavior makes this approach especially suitable for business applications with strong domain expertise. Bayesian Structural Time Series (BSTS) Bayesian structural time series models represent a comprehensive approach to time series analysis that combines the flexibility of state space models with the principled uncertainty quantification of Bayesian inference. BSTS models decompose time series into interpretable components such as trends, seasonal patterns, regression effects, and external interventions, all within a unified probabilistic framework. The key advantage of BSTS lies in its ability to handle complex time series structures while maintaining interpretability. The model can incorporate external regressors, detect structural breaks, and account for irregular seasonal patterns through flexible specification of state evolution equations. The Bayesian treatment provides posterior distributions for all model components, enabling inference about the relative importance of different factors driving the time series. BSTS models are particularly powerful for causal inference in time series settings, as they can incorporate control variables and assess the impact of interventions through counterfactual analysis. The spike-and-slab priors commonly used in BSTS facilitate automatic variable selection among potential regressors, making the approach suitable for high-dimensional settings with many candidate predictors. Implementation of BSTS models typically requires careful specification of prior distributions on state variances and regression coefficients. The bsts package in R provides a mature implementation with extensive functionality for model specification, fitting, and diagnostic checking. The resulting models offer both excellent predictive performance and interpretable decomposition of time series components. Nonlinear and Nonparametric Models In some applications, the relationship between predictors and the response variable is inherently nonlinear or unknown. Bayesian nonparametric models, such as Gaussian process regression, offer a flexible solution by modeling the function space directly. Gaussian processes define a prior over functions and use observed data to update this prior, resulting in a posterior distribution over functions. Gaussian process regression is particularly powerful when the form of the relationship is unknown or when modeling smooth, nonlinear trends is important. However, it comes at a higher computational cost and may not scale well with large datasets. Nevertheless, for problems involving spatial data, temporal trends, or complex functional relationships, Gaussian processes provide a valuable modeling tool. Mixture Models and Latent Structure Data arising from heterogeneous populations may be better modeled using mixture models. Bayesian Gaussian mixture models, for instance, assume that the data are generated from a mixture of several Gaussian distributions, each representing a subpopulation. These models can uncover latent structure in the data, such as clusters or subtypes. Mixture models introduce additional complexity due to the need to estimate both the component parameters and the mixing proportions. Bayesian inference provides a principled framework for dealing with this uncertainty, often using techniques such as latent variable augmentation and label switching adjustments. When there is reason to believe that the data comprise distinct subgroups with different underlying characteristics, mixture models offer an effective approach to modeling such heterogeneity. Comparative Summary Table Model Type Use Case Key Assumptions Priors Limitations Linear Regression Continuous outcome, low noise Linearity, normal errors Normal, Inverse-Gamma Poor with outliers Robust Regression Heavy-tailed residuals t-distributed residuals Prior on ν Increased complexity Regularized Regression High-dimensional predictors Sparsity Laplace, Gaussian Shrinkage may hide effects GLMs Binary/count/categorical outcomes Appropriate link function Varied Can overfit without strong priors Hierarchical Models Nested/grouped data Partial pooling Hierarchical priors Sensitive to group size Canonical Correlation Multivariate association Linear relationships Matrix-variate priors Assumes linear associations ARIMA Univariate time series Stationarity after diff. Constrained AR/MA priors Limited to linear dynamics Seasonal Decomposition Time series with seasonality Additive/multiplicative Component-specific priors Assumes stable patterns Holt-Winters Time series forecasting Exponential smoothing Smoothing parameter priors Limited to trend/seasonal forms BSTS Complex time series State space structure Spike-and-slab, others Computationally intensive Gaussian Processes Unknown nonlinear function Smoothness in kernel GP prior Poor scaling (O(n³)) Mixture Models Latent structure/clustering Finite components Dirichlet, etc. Label switching, identifiability Model Diagnostics and Comparison Choosing the right model also involves evaluating its performance and comparing it to alternative specifications. Bayesian model diagnostics include posterior predictive checks, which assess how well the model reproduces the observed data. Graphical comparisons between observed and replicated data can reveal model misfit or systematic discrepancies. Information criteria such as the Widely Applicable Information Criterion (WAIC) and Leave-One-Out Cross-Validation (LOO-CV) provide tools for model comparison, balancing fit and complexity. These criteria estimate the expected out-of-sample predictive performance and are particularly useful for selecting among nested or non-nested models (Vehtari, Gelman, &amp; Gabry, 2017). Bayes factors offer another method for model comparison, based on the ratio of marginal likelihoods. However, they are sensitive to prior specification and can be computationally intensive. In practice, WAIC and LOO-CV are often preferred for their robustness and ease of computation. A Decision Framework for Model Selection To aid practitioners in selecting the appropriate Bayesian model, a structured decision framework can be employed. This framework begins with identifying the type of response variable: continuous, binary, count, or categorical. Next, the data should be assessed for features such as outliers, overdispersion, hierarchical structure, and nonlinearity. Based on these characteristics, the modeler can then choose among linear models, robust regressions, generalized linear models, multilevel models, or nonparametric approaches. This decision process is iterative and should incorporate model diagnostics and domain knowledge. Starting with a simple model and progressively introducing complexity allows for a more transparent understanding of the data and the modeling assumptions. Each modeling choice should be justified in terms of its contribution to answering the research question and improving model fit. Conclusion Bayesian modeling offers unparalleled flexibility and rigor in statistical inference, but this power comes with the responsibility of thoughtful model selection. This guide has outlined the key considerations for choosing among the diverse array of Bayesian models available in tools like Stan and brms. By grounding model selection in the objectives of the analysis, the characteristics of the data, and robust diagnostic procedures, practitioners can make informed choices that enhance both the interpretability and predictive performance of their models. As with all statistical modeling, the process is iterative and benefits from a combination of statistical insight, computational tools, and substantive expertise. With this guide, researchers are better equipped to navigate the Bayesian modeling landscape and apply the appropriate models to their specific challenges. "],["bayesian-modeling-in-r-and-stan.html", "Chapter 2 Bayesian Modeling in R and Stan 2.1 What is stan? 2.2 Model file 2.3 Fit the model 2.4 MCMC diagnostics 2.5 Conclusions 2.6 References", " Chapter 2 Bayesian Modeling in R and Stan The aim of this post is to provide a quick overview and introduction to fitting Bayesian models using stan and R. For this, I strongly recommend installing Rstudio, an integrated development environment that allows a “user-friendly” interaction with R. 2.1 What is stan? stan is a tool for analysing Bayesian models using Markov Chain Monte Carlo (MCMC) methods. MCMC is a sampling method for estimating a probability distribution without knowing all of the features of the distribution. STAN is a probabilistic programming language and free software for specifying statistical models utilising Hamiltonian Monte Carlo methods (HMC), a type of MCMC algorithm. Stan works with the most widely used data-analysis languages including R and Python. In this quick overview, we’ll focus on the rstan package and demonstrate how to fit stan models with it. For an example dataset, here we simulate our own data in R. We firsty create a continuous outcome variable y as a function of one predictor x and a disturbance term ϵ. I simulate a dataset with 100 observations. Create the error term, the predictor and the outcome using a linear form with an intercept β0, slope β1, etc. coefficients. set.seed(123) n &lt;- 1000 a &lt;- 40 #intercept b &lt;- c(-2, 3, 4, 1 , 0.25) #slopes sigma2 &lt;- 25 #residual variance (sd=5) x &lt;- matrix(rnorm(5000),1000,5) eps &lt;- rnorm(n, mean = 0, sd = sqrt(sigma2)) #residuals y &lt;- a +x%*%b+ eps #response variable data &lt;- data.frame(y, x) #dataset head(data)%&gt;%kableExtra::kable() y X1 X2 X3 X4 X5 33.51510 -0.5604756 -0.9957987 -0.5116037 -0.1503075 0.1965498 43.76098 -0.2301775 -1.0399550 0.2369379 -0.3277571 0.6501132 27.64712 1.5587083 -0.0179802 -0.5415892 -1.4481653 0.6710042 50.72614 0.0705084 -0.1321751 1.2192276 -0.6972846 -1.2841578 39.46286 0.1292877 -2.5493428 0.1741359 2.5984902 -2.0261096 39.42009 1.7150650 1.0405735 -0.6152683 -0.0374150 2.2053261 2.2 Model file STAN models are written in an imperative programming language, which means the order in which you write the elements in your model file matters, i.e. you must first define your variables (e.g. integers, vectors, matrices, etc.), then the constraints that define the range of values your variable can take (e.g. only positive values for standard deviations), and finally the relationship between the variables. A Stan model is defined by different blocks including: Data (required): The data block reads information from the outside world, such as data vectors, matrices, integers, and so on. We also need to define the lengths and dimensions of objects, which may appear unusual to those who are used to R or Python. The number of observations is first declared as an integer variable N: int N; (note the use of semicolon to denote the end of a line). The number of predictors in our model, which is K. The intercept is included in this count, so we end up with two predictors (2 columns in the model matrix). Transformed Data (optional): The converted data block enables data preprocessing, such as data transformation or rescaling. Parameters (required): The parameters block specifies the parameters that must be assigned to prior distributions. Transformed parameters (optional): Before computing the posterior, the changed parameters block provides for parameter processing, such as transformation or rescaling of the parameters. model_stan = &quot; data { // declare the input data / parameters } transformed data { // optional - for transforming/scaling input data } parameters { // define model parameters } transformed parameters { // optional - for deriving additional non-model parameters // note however, as they are part of the sampling chain // transformed parameters slow sampling down. } model { // specifying priors and likelihood as well as the linear predictor } generated quantities { // optional - derivatives (posteriors) of the samples } &quot; cat(model_stan) ## ## data { ## // declare the input data / parameters ## } ## transformed data { ## // optional - for transforming/scaling input data ## } ## parameters { ## // define model parameters ## } ## transformed parameters { ## // optional - for deriving additional non-model parameters ## // note however, as they are part of the sampling chain ## // transformed parameters slow sampling down. ## } ## model { ## // specifying priors and likelihood as well as the linear predictor ## } ## generated quantities { ## // optional - derivatives (posteriors) of the samples ## } ## For this introduction, I’ll use a very simple model in the STAN model that only involves the specification of three blocks. In the data block, I declare the variables y and x as reals (or vectors) with length equal to N and declare the sample size n sim as a positive integer number using the phrase int n sim. I define the coefficients for the linear regression alpha and beta (as real values) and the standard deviation parameter sigma in the parameters block (as a positive real number). Finally, in the model block, I give the regression coefficients and standard deviation parameters weakly informative priors, and I use a normal distribution indexed by the conditional mean mu and standard deviation sigma parameters to model the outcome data y. I give full recognition to McElreath’s outstanding Statistical Rethinking (2020) book for this section. stan_mod = &quot;data { int&lt;lower=0&gt; N; // number of observations int&lt;lower=0&gt; K; // number of predictors matrix[N, K] X; // predictor matrix vector[N] y; // outcome vector } parameters { real alpha; // intercept vector[K] beta; // coefficients for predictors real&lt;lower=0&gt; sigma; // error scale } model { y ~ normal(alpha + X * beta, sigma); // target density }&quot; writeLines(stan_mod, con = &quot;stan_mod.stan&quot;) cat(stan_mod) 2.3 Fit the model We’ll need to organise the information into a list for Stan. This list should contain everything we defined in the data block of our Stan code. Then it is possible to run the model with the stan function. library(tidyverse) predictors &lt;- data %&gt;% select(-y) stan_data &lt;- list( N = 1000, K = 5, X = predictors, y = data$y ) fit_rstan &lt;- rstan::stan( file = &quot;stan_mod.stan&quot;, data = stan_data ) fit_rstan ## Inference for Stan model: anon_model. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% ## alpha 40.19 0.00 0.15 39.89 40.08 40.19 40.29 40.48 ## beta[1] -2.07 0.00 0.16 -2.39 -2.18 -2.07 -1.97 -1.76 ## beta[2] 2.73 0.00 0.16 2.42 2.62 2.73 2.84 3.04 ## beta[3] 3.83 0.00 0.16 3.51 3.72 3.83 3.93 4.13 ## beta[4] 1.23 0.00 0.16 0.92 1.11 1.23 1.34 1.54 ## beta[5] 0.34 0.00 0.16 0.04 0.23 0.34 0.45 0.65 ## sigma 4.96 0.00 0.11 4.74 4.88 4.95 5.03 5.17 ## lp__ -2098.32 0.04 1.88 -2102.75 -2099.39 -2097.99 -2096.92 -2095.67 ## n_eff Rhat ## alpha 5280 1 ## beta[1] 5552 1 ## beta[2] 6251 1 ## beta[3] 5988 1 ## beta[4] 6172 1 ## beta[5] 6343 1 ## sigma 5687 1 ## lp__ 1916 1 ## ## Samples were drawn using NUTS(diag_e) at Wed May 4 13:23:35 2022. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 2.4 MCMC diagnostics For Bayesian analysis, it is required to investigate the properties of the MCMC chains and the sampler in general, in addition to the standard model diagnostic checks (such as residual plots). Remember that the goal of MCMC sampling is to reproduce the posterior distribution of the model likelihood and priors by formulating a probability distribution. This method reliable, only if the MCMC samples accurately reflect the posterior. For each parameter, traceplots show the MCMC sample values after each iteration along the chain. Bad chain mixing (any pattern) indicates that the MCMC sample chains may not have spanned all aspects of the posterior distribution and that further iterations are needed to ensure that the distribution is accurately represented. Each parameter’s autocorrelation graphic shows the degree of correlation between MCMC samples separated by different lags. The degree of correlation between each MCMC sample and itself, for example, is represented by a lag of (obviously this will be a correlation of ). The MCMC samples should be independent in order to obtain unbiased parameter estimations (uncorrelated). For each parameter, the potential scale reduction factor (Rhat) statistic offers a measure of sampling efficiency/effectiveness. All values should, in theory, be less than 1, if the sampler has values of or greater than 1, it is likely that it was not particularly efficient or effective. A misspecified model or extremely unclear priors that lead to misspecified parameter space might lead to this. We should have looked at the convergence diagnostics before looking at the summaries. For the effects model, we utilise the package mcmcplots to generate density and trace graphs. It’s crucial to evaluate if the chains have converged when using MCMC to fit a model. To visually inspect MCMC diagnostics, we propose the bayesplot software. The bayesplot package includes routines for displaying MCMC diagnostics and supports model objects from both rstan and rstanarm. We’ll show how to make a trace plot with the mcmc_trace() function and a plot of Rhat values with the mcmc rhat() function. By printing the model fit, we can examine the parameters in the console. For each parameter, we derive posterior means, standard errors, and quantiles. n eff and Rhat are two other terms we use. These are the results of Stan’s engine’s exploration of the parameter space. For the time being, it’s enough to know that when Rhat is 1, everything is well. fit_rstan %&gt;% mcmc_trace() fit_rstan %&gt;% rhat() %&gt;% mcmc_rhat() + yaxis_text() 2.5 Conclusions To finish this post, I’d like to point out that the rstanarm package makes it possible to fit STAN models without having to write them down and instead using standard R syntax, such as that found in a glm(). So, what’s the point of learning all this STAN jargon? It depends: if you’re merely fitting “classical” models to your data with no fanfare, just use rstanarm; it’ll save you time and the models in this package are unquestionably better parametrized (i.e. faster) than the one I presented here. Learning STAN, on the other hand, is a good approach to get into a very flexible and strong language that will continue to evolve if you believe you will need to fit your own models one day. rstanarm is a package that acts as a user interface for Stan on the front end, and enables R users to create Bayesian models without needing to learn how to code in Stan. Using the standard formula and data, you can fit a model in rstanarm. If you want to use rstan to fit a different model type, you’ll have to code it yourself. The prefix stan_ precedes the model fitting functions and is followed by the model type. stan glm() and stan glmer() are two examples. A complete list of rstanarm functions can be found on Cran’s package guide. We provided only a quick overview of how to fit simple linear regression models with the Bayesian software STAN and the rstan library and how to get a collection of useful summaries from the models. This is just a taste of the many models that STAN can fit; in future postings, we’ll look at generalised linear models, as well as non-normal models with various link functions and hierarchical models. The STAN model as provided here is quite adaptable and able to accommodate datasets of various sizes. Although this may appear to be a more difficult technique than just fitting a linear model in a frequentist framework, the real benefits of Bayesian methods become apparent as the analysis becomes more sophisticated (which is often the case in real applications), the flexibility of Bayesian modelling makes it very simple to account for increasingly complicated models. 2.6 References Stan Functions Reference Stan Users Guide Some things i’ve learned about stan Stan (+R) Workshop# The pool of tears "],["bayesian-regression-models-for-non-normal-data.html", "Chapter 3 Bayesian Regression Models for Non-Normal Data 3.1 Logistic Regression 3.2 Negative Binomial 3.3 Conclusion 3.4 References", " Chapter 3 Bayesian Regression Models for Non-Normal Data Our last post covered how to do bayesian regression for normally distributed data in R using STAN. In this post, we’ll take a look at how to fit a regression model adapted to non-normal model in STAN using two common distributions seen in empirical data, namely, binomial and negative-binomial. As mentioned before in Bayesian modelling we use a set of sampling methods known as Markov Chain Monte Carlo (MCMC), we define a statistical model and find probabilistic estimates for the parameters. 3.1 Logistic Regression The likelihood of a binary outcome, such as pass or fail, is estimated using logistic models (but this model can be extended to include more than two outcomes). This is accomplished by using the logit function to convert a standard regression. The main parameter that we focus on here describes odds of the outcome derived from probabilities and transformed using a logit function. The following is the code for a logistic regression model with one predictor and an intercept. library(tidyverse) library(kableExtra) library(arm) library(emdbook) library(rstan) library(rstanarm) set.seed(1234) x1 = rnorm(10000) z = 1 + 2*x1 pr = 1/(1+exp(-z)) y = rbinom(10000,1,pr) df = data.frame(y=y,x1=x1) head(df)%&gt;%kableExtra::kable() y x1 0 -1.2070657 0 0.2774292 1 1.0844412 0 -2.3456977 1 0.4291247 1 0.5060559 glm( y~x1,data=df,family=&quot;binomial&quot;)%&gt;%summary()%&gt;%pluck(coefficients)%&gt;%kableExtra::kable() Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.036922 0.0294863 35.16621 0 x1 1.979352 0.0417219 47.44162 0 model_stan = &quot; data { int&lt;lower=0&gt; N; vector[N] x; int&lt;lower=0,upper=1&gt; y[N]; } parameters { real alpha; real beta; } model { y ~ bernoulli_logit(alpha + beta * x); } &quot; writeLines(model_stan, con = &quot;model_stan.stan&quot;) cat(model_stan) ## ## data { ## int&lt;lower=0&gt; N; ## vector[N] x; ## int&lt;lower=0,upper=1&gt; y[N]; ## } ## parameters { ## real alpha; ## real beta; ## } ## model { ## y ~ bernoulli_logit(alpha + beta * x); ## } ## stan_data &lt;- list( N = 10000, x = df$x1, y = df$y ) fit_rstan &lt;- rstan::stan( file = &quot;model_stan.stan&quot;, data = stan_data ) fit_rstan ## Inference for Stan model: anon_model. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff ## alpha 1.04 0.00 0.03 0.98 1.02 1.04 1.06 1.10 1936 ## beta 1.98 0.00 0.04 1.90 1.95 1.98 2.01 2.06 1804 ## lp__ -4339.45 0.03 1.05 -4342.29 -4339.80 -4339.12 -4338.74 -4338.49 1479 ## Rhat ## alpha 1 ## beta 1 ## lp__ 1 ## ## Samples were drawn using NUTS(diag_e) at Tue Jun 21 14:03:59 2022. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). This also can be expanded to several predictors. Below you can find the code as well as some recommendations for making sense of priors. set.seed(1234) x1 = rnorm(10000) x2 = rnorm(10000) x3 = rnorm(10000) z = 1 + 2*x1 + 3*x2 - 1*x3 pr = 1/(1+exp(-z)) y = rbinom(10000,1,pr) df2 = data.frame(y=y,x1=x1,x2=x2,x3=x3) head(df2)%&gt;%kableExtra::kable() y x1 x2 x3 0 -1.2070657 -1.8168975 -1.6878627 1 0.2774292 0.6271668 -0.9552011 1 1.0844412 0.5180921 -0.6480572 0 -2.3456977 0.1409218 0.2610342 1 0.4291247 1.4572719 -1.2196940 1 0.5060559 -0.4935965 -1.5501888 glm( y~x1+x2+x3,data=df2,family=&quot;binomial&quot;)%&gt;%summary()%&gt;%pluck(coefficients)%&gt;%kableExtra::kable() Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.0108545 0.0367817 27.48253 0 x1 1.9471836 0.0494298 39.39289 0 x2 2.9598129 0.0641890 46.11088 0 x3 -0.9448316 0.0372918 -25.33619 0 model_stan2 =&quot; data { int&lt;lower=0&gt; N; // number of observations int&lt;lower=0&gt; K; // number of predictors matrix[N, K] X; // predictor matrix int&lt;lower=0,upper=1&gt; y[N]; // outcome vector } parameters { real alpha; // intercept vector[K] beta; // coefficients for predictors } model { y ~ bernoulli_logit(alpha + X * beta); } &quot; writeLines(model_stan2, con = &quot;model_stan2.stan&quot;) cat(model_stan2) ## ## ## ## data { ## int&lt;lower=0&gt; N; // number of observations ## int&lt;lower=0&gt; K; // number of predictors ## matrix[N, K] X; // predictor matrix ## int&lt;lower=0,upper=1&gt; y[N]; // outcome vector ## } ## parameters { ## real alpha; // intercept ## vector[K] beta; // coefficients for predictors ## } ## model { ## y ~ bernoulli_logit(alpha + X * beta); ## } predictors &lt;- df2[,2:4] stan_data2 &lt;- list( N = 10000, K = 3, X = predictors, y = df2$y ) fit_rstan2 &lt;- rstan::stan( file = &quot;model_stan2.stan&quot;, data = stan_data2 ) ## Trying to compile a simple C file fit_rstan2 ## Inference for Stan model: anon_model. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% ## alpha 1.01 0.00 0.04 0.94 0.99 1.01 1.04 1.08 ## beta[1] 1.95 0.00 0.05 1.85 1.92 1.95 1.98 2.05 ## beta[2] 2.96 0.00 0.06 2.84 2.92 2.96 3.00 3.09 ## beta[3] -0.95 0.00 0.04 -1.02 -0.97 -0.95 -0.92 -0.87 ## lp__ -3006.39 0.03 1.38 -3009.82 -3007.07 -3006.08 -3005.37 -3004.68 ## n_eff Rhat ## alpha 2845 1 ## beta[1] 2338 1 ## beta[2] 2221 1 ## beta[3] 2160 1 ## lp__ 1788 1 ## ## Samples were drawn using NUTS(diag_e) at Tue Jun 21 14:05:38 2022. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 3.2 Negative Binomial The Poisson distribution, which assumes that the variance is equal to the mean, is a popular choice for modelling count data. The data are said to be overdispersed when the variance exceeds the mean, the Negative Binomial distribution can be used in this case. Note that the negative binomial distribution is a probability distribution of discrete random variables. Let’s say we have a response variable y that has a negative binomial distribution and is influenced by a collection of k explanatory variables X. The negative binomial distribution to model this data has two parameters: The μ or the expected value that need to be positive so a log link function can be used to map the linear predictor (the explanatory variables times the regression parameters) and ϕ which is the overdispersion parameter, where a small value means a large deviation from a Poisson distribution, as ϕ gets larger the negative binomial looks more and more like a Poisson distribution. Let’s simulate some data and fit a STAN model to them: N&lt;-100000 df3 &lt;-data.frame(x1=runif(N,-2,2),x2=runif(N,-2,2)) #the model X&lt;-model.matrix(~x1*x2,df3) K&lt;-dim(X)[2] #number of regression params #the regression slopes betas&lt;-runif(K,-1,1) #the overdispersion for the simulated data phi&lt;-5 #simulate the response y_nb&lt;-rnbinom(N,size=phi,mu=exp(X%*%betas)) hist(y_nb) MASS::glm.nb(y_nb ~ X[,2:K])%&gt;%summary()%&gt;%pluck(coefficients)%&gt;%kableExtra::kable() Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.0788902 0.0039192 20.12938 0 X[, 2:K]x1 -0.0815968 0.0032703 -24.95109 0 X[, 2:K]x2 0.7941334 0.0031740 250.20341 0 X[, 2:K]x1:x2 0.4301282 0.0026264 163.77308 0 data { int N; //the number of observations int K; //the number of columns in the model matrix int y[N]; //the response matrix[N,K] X; //the model matrix } parameters { vector[K] beta; //the regression parameters real phi; //the overdispersion parameters } transformed parameters { vector[N] mu;//the linear predictor mu &lt;- exp(X*beta); //using the log link } model { beta[1] ~ cauchy(0,10); //prior for the intercept following Gelman 2008 for(i in 2:K) beta[i] ~ cauchy(0,2.5);//prior for the slopes following Gelman 2008 y ~ neg_binomial_2(mu,phi); } generated quantities { vector[N] y_rep; for(n in 1:N){ y_rep[n] &lt;- neg_binomial_2_rng(mu[n],phi); //posterior draws to get posterior predictive checks } } stan_mod = &quot;data { int N; //the number of observations int K; //the number of columns in the model matrix int y[N]; //the response matrix[N,K] X; //the model matrix } parameters { vector[K] beta; //the regression parameters real phi; //the overdispersion parameters } transformed parameters { vector[N] mu;//the linear predictor mu &lt;- exp(X*beta); //using the log link } model { beta[1] ~ cauchy(0,10); //prior for the intercept following Gelman 2008 for(i in 2:K) beta[i] ~ cauchy(0,2.5);//prior for the slopes following Gelman 2008 y ~ neg_binomial_2(mu,phi); } &quot; writeLines(stan_mod, con = &quot;stan_mod.stan&quot;) cat(stan_mod) ## data { ## int N; //the number of observations ## int K; //the number of columns in the model matrix ## int y[N]; //the response ## matrix[N,K] X; //the model matrix ## } ## parameters { ## vector[K] beta; //the regression parameters ## real phi; //the overdispersion parameters ## } ## transformed parameters { ## vector[N] mu;//the linear predictor ## mu &lt;- exp(X*beta); //using the log link ## } ## model { ## beta[1] ~ cauchy(0,10); //prior for the intercept following Gelman 2008 ## ## for(i in 2:K) ## beta[i] ~ cauchy(0,2.5);//prior for the slopes following Gelman 2008 ## ## y ~ neg_binomial_2(mu,phi); ## } stan_data3 &lt;- list( N = N, K = K, X = X, y = y_nb ) fit_rstan3 &lt;- rstan::stan( file = &quot;stan_mod.stan&quot;, data = stan_data3 ) fit_rstan3%&gt;%summary() ## $summary ## mean se_mean sd 2.5% 25% ## beta[1] 7.899069e-02 6.825217e-05 0.0039010484 7.162531e-02 7.635923e-02 ## beta[2] -8.157657e-02 5.689044e-05 0.0032236243 -8.786688e-02 -8.376046e-02 ## beta[3] 7.941039e-01 5.618629e-05 0.0031476117 7.880475e-01 7.919520e-01 ## beta[4] 4.301036e-01 4.552321e-05 0.0026265744 4.248578e-01 4.283603e-01 ## phi 4.908958e+00 2.039012e-03 0.0831883094 4.745765e+00 4.854542e+00 3.3 Conclusion This tutorial provided only a quick overview of how to fit logistic and negative binomial regression models with the Bayesian software STAN using the rstan library/API and to extract a collection of useful summaries from the models. Future postings will address the question of outliers and the use of robust linear models. 3.4 References Stan Functions Reference Stan Users Guide Some things i’ve learned about stan Stan (+R) Workshop "],["robust-t-regression.html", "Chapter 4 Robust t-regression 4.1 Introduction 4.2 Concepts and code 4.3 Conclusion 4.4 References", " Chapter 4 Robust t-regression 4.1 Introduction Simple linear regression is a widely used method for estimating the linear relation between two (or more) variables and for predicting the value of one variable (the response variable) based on the value of the other (the explanatory variable). The explanatory variable is typically represented on the x-axis and the response variable on the y-axis for visualising the results of linear regression. The regression coefficients can be distorted by outlying data points. Due to the normality assumption used by traditional regression techniques, noisy or outlier data can greatly affect their accuracy. Since the normal distribution must move to a new place in the parameter space to best accommodate the outliers in the data, this frequently leads to an underestimate of the relationship between the variables. In a frequentist framework, creating a linear regression model that is resistant to outliers necessitates the use of very complex statistical techniques; however, in a Bayesian framework, we can achieve robustness by simply using the t-distribution. in this context, Robust Regression refers to regression methods which are less sensitive to outliers. Bayesian robust regression uses distributions with wider tails than the normal. This means that outliers will have less of an affect on the models and the regression line would need to move less incorporate those observations since the error distribution will not consider them as unusual. Utilizing Student’s t density with an unidentified degrees of freedom parameter is a well-liked substitution for normal errors in regression investigations. The Student’s t distribution has heavier tails than the normal for low degrees of freedom, but it leans toward the normal as the degrees of freedom parameter rises. A check on the suitability of the normal is thus made possible by treating the degrees of freedom parameter as an unknown quantity that must be approximated. The degrees of freedom, or parameter ν, of this probability distribution determines how close to normal the distribution is: Low small values of produce a distribution with thicker tails (that is, a greater spread around the mean) than the normal distribution, but big values of (approximately &gt; 30) give a distribution that is quite similar to the normal distribution. As a result, we can allow the distribution of the regression line to be as normal or non-normal as the data imply while still capturing the underlying relationship between the variables by substituting the normal distribution above with a t-distribution and adding as an extra parameter to the model. 4.2 Concepts and code The standard approach to linear regression is defining the equation for a straight line that represents the relationship between the variables as accurately as possible. The equation for the line defines y (the response variable) as a linear function of x (the explanatory variable): 𝑦 = 𝛼 + 𝛽𝑥 + 𝜀 In this equation, ε represents the error in the linear relationship: if no noise were allowed, then the paired x- and y-values would need to be arranged in a perfect straight line (for example, as in y = 2x + 1). Because we assume that the relationship between x and y is truly linear, any variation observed around the regression line must be random noise, and therefore normally distributed. From a probabilistic standpoint, such relationship between the variables could be formalised as 𝑦 ~ 𝓝(𝛼 + 𝛽𝑥, 𝜎) That is, the response variable follows a normal distribution with mean equal to the regression line, and some standard deviation σ. Such a probability distribution of the regression line is illustrated in the figure below. The formulation of the robust simple linear regression Bayesian model is given below. We define a t likelihood for the response variable, y, and suitable vague priors on all the model parameters: normal for α and β, half-normal for σ and gamma for ν. 𝑦 ~ 𝓣(𝛼 + 𝛽𝑥, 𝜎, 𝜈) 𝛼, 𝛽 ~ 𝓝(0, 1000) 𝜎 ~ 𝓗𝓝(0, 1000) 𝜈 ~ 𝚪(2, 0.1) Below you can find R and Stan code for a simple Bayesian t-regression model with nu unknown. First let’s create data with and without ourliers library(readr) library(tidyverse) library(gridExtra) library(kableExtra) library(arm) library(emdbook) library(rstan) library(rstanarm) library(brms) s &lt;- matrix(c(1, .8, .8, 1), nrow = 2, ncol = 2) m &lt;- c(3, 3) set.seed(1234) data_n &lt;- MASS::mvrnorm(n = 100, mu = m, Sigma = s) %&gt;% as_tibble() %&gt;% rename(y = V1, x = V2) data_n &lt;- data_n %&gt;% arrange(x) head(data_n)%&gt;%kableExtra::kable() y x 0.9335732 0.6157783 1.0317982 0.8318674 1.9763140 0.9326985 2.3439117 1.1117327 1.4059413 1.1673553 2.0360302 1.3253002 data_o &lt;- data_n data_o[c(1:2), 1] &lt;- c(7.5, 8.5) head(data_o)%&gt;%kableExtra::kable() y x 7.500000 0.6157783 8.500000 0.8318674 1.976314 0.9326985 2.343912 1.1117327 1.405941 1.1673553 2.036030 1.3253002 ols_n &lt;- lm(data = data_n, y ~ 1 + x) ols_o &lt;- lm(data = data_o, y ~ 1 + x) p1 &lt;- ggplot(data = data_n, aes(x = x, y = y)) + stat_smooth(method = &quot;lm&quot;, color = &quot;grey92&quot;, fill = &quot;grey67&quot;, alpha = 1, fullrange = T) + geom_point(size = 1, alpha = 3/4) + scale_x_continuous(limits = c(0, 9)) + coord_cartesian(xlim = c(0, 9), ylim = c(0, 9)) + labs(title = &quot;No Outliers&quot;) + theme(panel.grid = element_blank()) # the data with two outliers p2 &lt;- ggplot(data = data_o, aes(x = x, y = y, color = y &gt; 7)) + stat_smooth(method = &quot;lm&quot;, color = &quot;grey92&quot;, fill = &quot;grey67&quot;, alpha = 1, fullrange = T) + geom_point(size = 1, alpha = 3/4) + scale_color_viridis_d(option = &quot;A&quot;, end = 4/7) + scale_x_continuous(limits = c(0, 9)) + coord_cartesian(xlim = c(0, 9), ylim = c(0, 9)) + labs(title = &quot;Two Outliers&quot;) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) grid.arrange(p1 ,p2) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; model_stan = &quot; data { int&lt;lower=1&gt; N; int&lt;lower=0&gt; M; int&lt;lower=0&gt; P; vector[N] x; vector[N] y; vector[M] x_cred; vector[P] x_pred; } parameters { real alpha; real beta; real&lt;lower=0&gt; sigma; real&lt;lower=1&gt; nu; } transformed parameters { vector[N] mu = alpha + beta * x; vector[M] mu_cred = alpha + beta * x_cred; vector[P] mu_pred = alpha + beta * x_pred; } model { y ~ student_t(nu, mu, sigma); alpha ~ normal(0, 1000); beta ~ normal(0, 1000); sigma ~ normal(0, 1000); nu ~ gamma(2, 0.1); } generated quantities { real y_pred[P]; for (p in 1:P) { y_pred[p] = student_t_rng(nu, mu_pred[p], sigma); } } &quot; writeLines(model_stan, con = &quot;model_stan.stan&quot;) cat(model_stan) ## ## data { ## int&lt;lower=1&gt; N; ## int&lt;lower=0&gt; M; ## int&lt;lower=0&gt; P; ## vector[N] x; ## vector[N] y; ## vector[M] x_cred; ## vector[P] x_pred; ## } ## ## parameters { ## real alpha; ## real beta; ## real&lt;lower=0&gt; sigma; ## real&lt;lower=1&gt; nu; ## } ## ## transformed parameters { ## vector[N] mu = alpha + beta * x; ## vector[M] mu_cred = alpha + beta * x_cred; ## vector[P] mu_pred = alpha + beta * x_pred; ## } ## ## model { ## y ~ student_t(nu, mu, sigma); ## alpha ~ normal(0, 1000); ## beta ~ normal(0, 1000); ## sigma ~ normal(0, 1000); ## nu ~ gamma(2, 0.1); ## } ## ## generated quantities { ## real y_pred[P]; ## for (p in 1:P) { ## y_pred[p] = student_t_rng(nu, mu_pred[p], sigma); ## } ## } ## stan_data &lt;- list(x=data_o$x, y=data_o$y, N=length(data_o$y), M=0, P=0, x_cred=numeric(0), x_pred=numeric(0)) fit_rstan &lt;- rstan::stan( file = &quot;model_stan.stan&quot;, data = stan_data ) ## Trying to compile a simple C file ## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c ## clang -mmacosx-version-min=10.13 -I&quot;/Library/Frameworks/R.framework/Resources/include&quot; -DNDEBUG -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/unsupported&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/BH/include&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/src/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppParallel/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/rstan/include&quot; -DEIGEN_NO_DEBUG -DBOOST_DISABLE_ASSERTS -DBOOST_PENDING_INTEGER_LOG2_HPP -DSTAN_THREADS -DUSE_STANC3 -DSTRICT_R_HEADERS -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION -DBOOST_NO_AUTO_PTR -include &#39;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp&#39; -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1 -I/usr/local/include -fPIC -Wall -g -O2 -c foo.c -o foo.o ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:88: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name &#39;namespace&#39; ## namespace Eigen { ## ^ ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected &#39;;&#39; after top level declarator ## namespace Eigen { ## ^ ## ; ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found ## #include &lt;complex&gt; ## ^~~~~~~~~ ## 3 errors generated. ## make: *** [foo.o] Error 1 ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 3.9e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.39 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.114 seconds (Warm-up) ## Chain 1: 0.122 seconds (Sampling) ## Chain 1: 0.236 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 1.3e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.126 seconds (Warm-up) ## Chain 2: 0.11 seconds (Sampling) ## Chain 2: 0.236 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 9e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.122 seconds (Warm-up) ## Chain 3: 0.111 seconds (Sampling) ## Chain 3: 0.233 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 1.1e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.134 seconds (Warm-up) ## Chain 4: 0.113 seconds (Sampling) ## Chain 4: 0.247 seconds (Total) ## Chain 4: trace &lt;- stan_trace(fit_rstan, pars=c(&quot;alpha&quot;, &quot;beta&quot;, &quot;sigma&quot;, &quot;nu&quot;)) trace + scale_color_brewer(type = &quot;div&quot;) + theme(legend.position = &quot;none&quot;) ## Scale for &#39;colour&#39; is already present. Adding another scale for &#39;colour&#39;, ## which will replace the existing scale. stan_dens(fit_rstan, pars=c(&quot;alpha&quot;, &quot;beta&quot;, &quot;sigma&quot;, &quot;nu&quot;), fill = &quot;skyblue&quot;) stan_plot(fit_rstan, pars=c(&quot;alpha&quot;, &quot;beta&quot;, &quot;sigma&quot;, &quot;nu&quot;), show_density = TRUE, fill_color = &quot;maroon&quot;) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) Bayesian regression models can be used to estimate highest posterior density or credible intervals (intervals of the distribution of the regression linefor), and prediction values, through predictive posterior distributions. More specifically, the prediction intervals are obtained by first drawing samples of the mean response at specific x-values of interest and then, for each of these samples, randomly selecting a y-value from a t-distribution with location mu pred. In contrast, the credible intervals are obtained by drawing MCMC samples of the mean response at regularly spaced points along the x-axis. The distributions of mu cred and y pred are represented, respectively, by the credible and prediction intervals. x.cred = seq(from=min(data_o$x), to=max(data_o$x), length.out=50) x.pred = c(0, 8) stan_data2 &lt;- list(x=data_o$x, y=data_o$y, N=length(data_o$y), x_cred=x.cred, x_pred=x.pred, M=length(x.cred), P=length(x.pred)) fit_rstan2 &lt;- rstan::stan( file = &quot;model_stan.stan&quot;, data = stan_data2 ) ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 2.6e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.157 seconds (Warm-up) ## Chain 1: 0.134 seconds (Sampling) ## Chain 1: 0.291 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 1e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.148 seconds (Warm-up) ## Chain 2: 0.101 seconds (Sampling) ## Chain 2: 0.249 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 1.4e-05 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.165 seconds (Warm-up) ## Chain 3: 0.132 seconds (Sampling) ## Chain 3: 0.297 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 1.2e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.151 seconds (Warm-up) ## Chain 4: 0.145 seconds (Sampling) ## Chain 4: 0.296 seconds (Total) ## Chain 4: For each value in x.cred, the mu cred parameter’s MCMC samples are contained in a separate column of mu.cred. In a similar manner, the columns of y.pred include the MCMC samples of the posterior expected response values (y pred values) for the x-values in x.pred. summary(extract(fit_rstan2, &quot;mu_cred&quot;)[[1]])%&gt;%kableExtra::kable() V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 V29 V30 V31 V32 V33 V34 V35 V36 V37 V38 V39 V40 V41 V42 V43 V44 V45 V46 V47 V48 V49 V50 Min. :0.4535 Min. :0.553 Min. :0.6525 Min. :0.752 Min. :0.8515 Min. :0.951 Min. :1.051 Min. :1.150 Min. :1.250 Min. :1.349 Min. :1.449 Min. :1.548 Min. :1.642 Min. :1.734 Min. :1.826 Min. :1.918 Min. :2.010 Min. :2.102 Min. :2.194 Min. :2.285 Min. :2.377 Min. :2.458 Min. :2.531 Min. :2.603 Min. :2.675 Min. :2.747 Min. :2.820 Min. :2.892 Min. :2.964 Min. :3.037 Min. :3.109 Min. :3.181 Min. :3.245 Min. :3.297 Min. :3.349 Min. :3.401 Min. :3.453 Min. :3.505 Min. :3.557 Min. :3.609 Min. :3.661 Min. :3.713 Min. :3.765 Min. :3.817 Min. :3.869 Min. :3.921 Min. :3.973 Min. :4.025 Min. :4.077 Min. :4.129 1st Qu.:0.9464 1st Qu.:1.029 1st Qu.:1.1113 1st Qu.:1.193 1st Qu.:1.2762 1st Qu.:1.359 1st Qu.:1.441 1st Qu.:1.524 1st Qu.:1.605 1st Qu.:1.688 1st Qu.:1.769 1st Qu.:1.851 1st Qu.:1.931 1st Qu.:2.012 1st Qu.:2.093 1st Qu.:2.174 1st Qu.:2.254 1st Qu.:2.335 1st Qu.:2.415 1st Qu.:2.495 1st Qu.:2.574 1st Qu.:2.654 1st Qu.:2.733 1st Qu.:2.810 1st Qu.:2.888 1st Qu.:2.966 1st Qu.:3.043 1st Qu.:3.119 1st Qu.:3.195 1st Qu.:3.270 1st Qu.:3.344 1st Qu.:3.419 1st Qu.:3.493 1st Qu.:3.569 1st Qu.:3.644 1st Qu.:3.717 1st Qu.:3.792 1st Qu.:3.867 1st Qu.:3.942 1st Qu.:4.015 1st Qu.:4.089 1st Qu.:4.163 1st Qu.:4.237 1st Qu.:4.311 1st Qu.:4.386 1st Qu.:4.460 1st Qu.:4.534 1st Qu.:4.608 1st Qu.:4.681 1st Qu.:4.753 Median :1.0605 Median :1.138 Median :1.2165 Median :1.295 Median :1.3724 Median :1.450 Median :1.528 Median :1.606 Median :1.684 Median :1.762 Median :1.840 Median :1.918 Median :1.996 Median :2.074 Median :2.152 Median :2.230 Median :2.308 Median :2.386 Median :2.463 Median :2.542 Median :2.619 Median :2.698 Median :2.775 Median :2.853 Median :2.931 Median :3.010 Median :3.087 Median :3.165 Median :3.243 Median :3.322 Median :3.400 Median :3.478 Median :3.557 Median :3.634 Median :3.713 Median :3.790 Median :3.868 Median :3.947 Median :4.024 Median :4.102 Median :4.180 Median :4.257 Median :4.335 Median :4.413 Median :4.491 Median :4.569 Median :4.647 Median :4.726 Median :4.803 Median :4.881 Mean :1.0605 Mean :1.138 Mean :1.2164 Mean :1.294 Mean :1.3723 Mean :1.450 Mean :1.528 Mean :1.606 Mean :1.684 Mean :1.762 Mean :1.840 Mean :1.918 Mean :1.996 Mean :2.074 Mean :2.152 Mean :2.230 Mean :2.308 Mean :2.386 Mean :2.464 Mean :2.542 Mean :2.619 Mean :2.697 Mean :2.775 Mean :2.853 Mean :2.931 Mean :3.009 Mean :3.087 Mean :3.165 Mean :3.243 Mean :3.321 Mean :3.399 Mean :3.477 Mean :3.555 Mean :3.633 Mean :3.711 Mean :3.789 Mean :3.867 Mean :3.945 Mean :4.023 Mean :4.101 Mean :4.178 Mean :4.256 Mean :4.334 Mean :4.412 Mean :4.490 Mean :4.568 Mean :4.646 Mean :4.724 Mean :4.802 Mean :4.880 3rd Qu.:1.1739 3rd Qu.:1.247 3rd Qu.:1.3213 3rd Qu.:1.394 3rd Qu.:1.4688 3rd Qu.:1.543 3rd Qu.:1.616 3rd Qu.:1.691 3rd Qu.:1.766 3rd Qu.:1.840 3rd Qu.:1.915 3rd Qu.:1.989 3rd Qu.:2.063 3rd Qu.:2.137 3rd Qu.:2.212 3rd Qu.:2.286 3rd Qu.:2.362 3rd Qu.:2.436 3rd Qu.:2.512 3rd Qu.:2.587 3rd Qu.:2.663 3rd Qu.:2.740 3rd Qu.:2.817 3rd Qu.:2.895 3rd Qu.:2.973 3rd Qu.:3.052 3rd Qu.:3.132 3rd Qu.:3.212 3rd Qu.:3.292 3rd Qu.:3.372 3rd Qu.:3.453 3rd Qu.:3.535 3rd Qu.:3.616 3rd Qu.:3.698 3rd Qu.:3.779 3rd Qu.:3.861 3rd Qu.:3.943 3rd Qu.:4.025 3rd Qu.:4.107 3rd Qu.:4.189 3rd Qu.:4.271 3rd Qu.:4.353 3rd Qu.:4.436 3rd Qu.:4.518 3rd Qu.:4.600 3rd Qu.:4.682 3rd Qu.:4.765 3rd Qu.:4.847 3rd Qu.:4.929 3rd Qu.:5.012 Max. :1.6368 Max. :1.697 Max. :1.7566 Max. :1.817 Max. :1.8764 Max. :1.936 Max. :1.996 Max. :2.056 Max. :2.116 Max. :2.176 Max. :2.236 Max. :2.296 Max. :2.356 Max. :2.416 Max. :2.475 Max. :2.535 Max. :2.595 Max. :2.655 Max. :2.729 Max. :2.811 Max. :2.893 Max. :2.974 Max. :3.056 Max. :3.138 Max. :3.220 Max. :3.301 Max. :3.383 Max. :3.465 Max. :3.547 Max. :3.628 Max. :3.710 Max. :3.792 Max. :3.874 Max. :3.955 Max. :4.037 Max. :4.123 Max. :4.222 Max. :4.321 Max. :4.420 Max. :4.520 Max. :4.619 Max. :4.718 Max. :4.818 Max. :4.917 Max. :5.016 Max. :5.116 Max. :5.215 Max. :5.314 Max. :5.414 Max. :5.513 summary(extract(fit_rstan2, &quot;y_pred&quot;)[[1]])%&gt;%kableExtra::kable() V1 V2 Min. :-36.0432 Min. :-7.818 1st Qu.: 0.1482 1st Qu.: 6.466 Median : 0.5704 Median : 6.954 Mean : 0.5951 Mean : 6.946 3rd Qu.: 1.0130 3rd Qu.: 7.431 Max. : 19.3150 Max. :23.107 These models can be also etimated through brms package’s API for Stan as follows M_gaussian &lt;- brm(data = data_o, family = gaussian, y ~ 1 + x, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), seed = 1) ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 2.3e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.02 seconds (Warm-up) ## Chain 1: 0.019 seconds (Sampling) ## Chain 1: 0.039 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 5e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.019 seconds (Warm-up) ## Chain 2: 0.016 seconds (Sampling) ## Chain 2: 0.035 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 5e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.019 seconds (Warm-up) ## Chain 3: 0.016 seconds (Sampling) ## Chain 3: 0.035 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 4e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.019 seconds (Warm-up) ## Chain 4: 0.018 seconds (Sampling) ## Chain 4: 0.037 seconds (Total) ## Chain 4: summary(M_gaussian) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 1 + x ## Data: data_o (Number of observations: 100) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.56 0.34 0.88 2.22 1.00 3889 2871 ## x 0.50 0.11 0.28 0.72 1.00 3941 2995 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.13 0.08 0.98 1.30 1.00 3719 2831 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). M_student &lt;- brm(data = data_o, family = student, y ~ 1 + x, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(gamma(4, 1), class = nu), prior(cauchy(0, 1), class = sigma)), seed = 1) ## Compiling Stan program... ## Trying to compile a simple C file ## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c ## clang -mmacosx-version-min=10.13 -I&quot;/Library/Frameworks/R.framework/Resources/include&quot; -DNDEBUG -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/unsupported&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/BH/include&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/src/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppParallel/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/rstan/include&quot; -DEIGEN_NO_DEBUG -DBOOST_DISABLE_ASSERTS -DBOOST_PENDING_INTEGER_LOG2_HPP -DSTAN_THREADS -DUSE_STANC3 -DSTRICT_R_HEADERS -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION -DBOOST_NO_AUTO_PTR -include &#39;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp&#39; -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1 -I/usr/local/include -fPIC -Wall -g -O2 -c foo.c -o foo.o ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:88: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name &#39;namespace&#39; ## namespace Eigen { ## ^ ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected &#39;;&#39; after top level declarator ## namespace Eigen { ## ^ ## ; ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found ## #include &lt;complex&gt; ## ^~~~~~~~~ ## 3 errors generated. ## make: *** [foo.o] Error 1 ## Start sampling ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.000456 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 4.56 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.07 seconds (Warm-up) ## Chain 1: 0.063 seconds (Sampling) ## Chain 1: 0.133 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 1.8e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.069 seconds (Warm-up) ## Chain 2: 0.066 seconds (Sampling) ## Chain 2: 0.135 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 1.3e-05 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.071 seconds (Warm-up) ## Chain 3: 0.063 seconds (Sampling) ## Chain 3: 0.134 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 2.5e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.07 seconds (Warm-up) ## Chain 4: 0.065 seconds (Sampling) ## Chain 4: 0.135 seconds (Total) ## Chain 4: summary(M_student) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: y ~ 1 + x ## Data: data_o (Number of observations: 100) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.56 0.20 0.17 0.96 1.00 3835 2836 ## x 0.80 0.07 0.66 0.93 1.00 3836 3084 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.50 0.06 0.39 0.62 1.00 3183 2799 ## nu 2.80 0.70 1.71 4.49 1.00 3564 2917 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 4.3 Conclusion In this post, we have provided a simple Bayesian approach to robustly estimate both parameters β and σ of a simple linear regression where the estiamtes are robust to the variance of the error term. The specificity of this approach is to replace the traditional normal assumption on the dependant variable by a heavy-tailed t-distribution assumption. Robusness against outliers comes at a price of a loss of efficiency, especially when the observations are normally distributed. This is a low premium that comes with the robust alternatives that offers a large protection against over-fiting. 4.4 References Bayesian Robustness to Outliers in Linear Regression A New Bayesian Approach to Robustness Against Outliers in Linear Regression Robust Noise Models from stan manual "],["bayesian-regularized-regression.html", "Chapter 5 Bayesian Regularized Regression 5.1 Introduction 5.2 Ridge Regression and Gaussian Priors 5.3 LASSO Regression and Laplace Priors 5.4 Hierarchical Shrinkage and Adaptive Priors 5.5 Implementation 5.6 Empirical Comparison and Model Evaluation 5.7 Understanding Posterior Behavior 5.8 Choosing Among Regularization Methods 5.9 Conclusion 5.10 References", " Chapter 5 Bayesian Regularized Regression 5.1 Introduction In this post, we will explore Bayesian analogues of regularized linear regression models such as LASSO and Ridge regression, which extend traditional linear regression to handle challenging modeling scenarios. These methods prove particularly valuable for improving prediction accuracy, estimating regression models with many variables, and providing alternatives to traditional model selection approaches. The foundation of our discussion rests on the standard linear regression model \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\), where \\(\\mathbf{y} \\in \\mathbb{R}^n\\) represents the response vector, \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\) is the design matrix, \\(\\boldsymbol{\\beta} \\in \\mathbb{R}^p\\) contains the coefficients of interest, and \\(\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\sigma^2\\mathbf{I})\\) captures the random error. Traditional linear regression treats coefficients as independent, producing what statisticians call “unbiased” estimates. However, this independence assumption can lead to poor performance in settings with limited data, high-dimensional predictor spaces, or substantial noise. Regularized linear regression models deliberately introduce bias into coefficient estimates by pooling information across parameters, causing them to shrink toward a common value, typically zero. This shrinkage represents a fundamental trade-off captured by the bias-variance decomposition: \\(\\text{MSE}(\\hat{\\boldsymbol{\\beta}}) = \\text{Bias}^2(\\hat{\\boldsymbol{\\beta}}) + \\text{Var}(\\hat{\\boldsymbol{\\beta}})\\). While regularization increases bias, it often reduces variance substantially enough that the overall mean squared error decreases, particularly in high-dimensional or noisy settings where traditional methods tend to overfit. The Bayesian perspective reframes regularization as a problem of specifying appropriate prior distributions for model parameters. Rather than viewing penalty terms as arbitrary constraints on optimization, Bayesian regularization emerges naturally from the choice of prior distribution over coefficients. This connection provides both theoretical insight into why regularization works and practical advantages through full posterior distributions for all parameters. 5.2 Ridge Regression and Gaussian Priors Ridge regression modifies the ordinary least squares objective by adding a penalty proportional to the sum of squared coefficients. In the frequentist framework, ridge regression minimizes \\(\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_2^2\\), yielding the closed-form solution \\(\\hat{\\boldsymbol{\\beta}}_{\\text{ridge}} = (\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\\). The penalty parameter \\(\\lambda\\) controls the strength of regularization, with larger values producing more shrinkage toward zero. The Bayesian equivalent emerges by placing independent Gaussian priors on each coefficient: \\(\\beta_j \\sim \\mathcal{N}(0, \\tau^2)\\) for \\(j = 1, \\ldots, p\\). Under this prior specification, the posterior distribution becomes \\(p(\\boldsymbol{\\beta}|\\mathbf{y}, \\mathbf{X}, \\sigma^2, \\tau^2) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2 - \\frac{1}{2\\tau^2}\\|\\boldsymbol{\\beta}\\|_2^2\\right)\\). The mathematical equivalence becomes clear when we recognize that \\(\\lambda = \\sigma^2/\\tau^2\\), establishing a direct correspondence between the frequentist penalty parameter and the ratio of error variance to prior variance. This Bayesian formulation offers several advantages over its frequentist counterpart. Most importantly, it provides full posterior distributions for all parameters rather than just point estimates, enabling principled uncertainty quantification without requiring bootstrap procedures. The prior variance \\(\\tau^2\\) also admits a natural hierarchical extension by treating it as an unknown parameter with its own prior distribution, allowing the data to inform the appropriate level of shrinkage. 5.3 LASSO Regression and Laplace Priors LASSO regression replaces the ridge penalty with an L1 penalty on the coefficient magnitudes, minimizing \\(\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_1\\). This seemingly minor modification produces dramatically different behavior: while ridge regression shrinks coefficients toward zero, LASSO can set them exactly to zero, performing automatic variable selection. The geometric intuition reveals that the L1 penalty creates a diamond-shaped constraint region whose corners lie on the coordinate axes, making exact zeros more likely when the unconstrained optimum lies in particular regions of the parameter space. The Bayesian LASSO emerges by specifying independent Laplace priors for each coefficient: \\(\\beta_j \\sim \\text{Laplace}(0, b)\\) for \\(j = 1, \\ldots, p\\). The Laplace density \\(p(\\beta_j|b) = \\frac{1}{2b}\\exp\\left(-\\frac{|\\beta_j|}{b}\\right)\\) places much more probability mass at zero compared to a Gaussian distribution with the same variance, naturally encouraging sparsity. The relationship between the frequentist penalty and Bayesian scale parameter follows \\(\\lambda = \\sigma^2/b\\). While the Bayesian LASSO provides the same point estimates as its frequentist counterpart through the posterior mode, its real advantage lies in proper uncertainty quantification for the selected variables. The full posterior distribution accounts for both parameter uncertainty and model uncertainty, providing more honest assessments of prediction intervals and coefficient significance than frequentist methods that condition on the selected model. 5.4 Hierarchical Shrinkage and Adaptive Priors Both ridge and LASSO impose uniform shrinkage across all coefficients, but real data often contains a mixture of large, moderate, and negligible effects that would benefit from adaptive shrinkage. Hierarchical shrinkage priors address this limitation by allowing different coefficients to experience different amounts of shrinkage based on their apparent signal strength in the data. The general framework specifies \\(\\beta_j \\sim \\mathcal{N}(0, \\tau^2 \\lambda_j^2)\\) for \\(j = 1, \\ldots, p\\), where \\(\\tau\\) represents a global shrinkage parameter affecting all coefficients and \\(\\lambda_j\\) represents local shrinkage parameters that can vary across coefficients. This hierarchical structure enables the model to shrink small coefficients aggressively while leaving large coefficients relatively unshrunk. The horseshoe prior represents one particularly successful implementation of this idea, specifying \\(\\lambda_j \\sim \\text{Cauchy}^+(0, 1)\\) and \\(\\tau \\sim \\text{Cauchy}^+(0, 1)\\). The Cauchy distribution’s heavy tails ensure that large coefficients can escape shrinkage while still encouraging small coefficients toward zero. The horseshoe prior gets its name from the shape of its shrinkage function, which resembles an inverted horseshoe when plotted against the signal-to-noise ratio. 5.5 Implementation library(tidymodels) ## ── Attaching packages ───────────────────────────────────────────────────────────────────────────────────────────────────────── tidymodels 1.3.0 ── ## ✔ broom 1.0.8 ✔ recipes 1.3.1 ## ✔ dials 1.4.0 ✔ rsample 1.3.1 ## ✔ dplyr 1.1.4 ✔ tibble 3.2.1 ## ✔ ggplot2 3.5.2 ✔ tidyr 1.3.1 ## ✔ infer 1.0.9 ✔ tune 1.3.0 ## ✔ modeldata 1.4.0 ✔ workflows 1.2.0 ## ✔ parsnip 1.3.2 ✔ workflowsets 1.1.1 ## ✔ purrr 1.0.4 ✔ yardstick 1.3.2 ## Warning: package &#39;infer&#39; was built under R version 4.5.1 ## Warning: package &#39;rsample&#39; was built under R version 4.5.1 ## ── Conflicts ──────────────────────────────────────────────────────────────────────────────────────────────────────────── tidymodels_conflicts() ── ## ✖ purrr::discard() masks scales::discard() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ✖ recipes::step() masks stats::step() ## • Dig deeper into tidy modeling with R at https://www.tmwr.org library(rstan) ## Loading required package: StanHeaders ## ## rstan version 2.32.7 (Stan version 2.32.2) ## For execution on a local, multicore CPU with excess RAM we recommend calling ## options(mc.cores = parallel::detectCores()). ## To avoid recompilation of unchanged Stan programs, we recommend calling ## rstan_options(auto_write = TRUE) ## For within-chain threading using `reduce_sum()` or `map_rect()` Stan functions, ## change `threads_per_chain` option: ## rstan_options(threads_per_chain = 1) ## ## Attaching package: &#39;rstan&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract library(tidyverse) ## ── Attaching core tidyverse packages ─────────────────────────────────────────────────────────────────────────────────────────── tidyverse 2.0.0 ── ## ✔ forcats 1.0.0 ✔ readr 2.1.5 ## ✔ lubridate 1.9.4 ✔ stringr 1.5.1 ## ── Conflicts ───────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ readr::col_factor() masks scales::col_factor() ## ✖ purrr::discard() masks scales::discard() ## ✖ rstan::extract() masks tidyr::extract() ## ✖ dplyr::filter() masks stats::filter() ## ✖ stringr::fixed() masks recipes::fixed() ## ✖ dplyr::lag() masks stats::lag() ## ✖ readr::spec() masks yardstick::spec() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors # Data simulation set.seed(123) n &lt;- 1000 n_train &lt;- 700 n_test &lt;- 300 alpha_true &lt;- 40 # intercept beta_true &lt;- c(-2, 3, 4, 1, 0.25) # slopes sigma_true &lt;- 5 # residual standard deviation p &lt;- length(beta_true) # Generate design matrix and response X &lt;- matrix(rnorm(n * p), n, p) epsilon &lt;- rnorm(n, mean = 0, sd = sigma_true) y &lt;- alpha_true + X %*% beta_true + epsilon # Create dataset data &lt;- data.frame(y = y, X) colnames(data) &lt;- c(&quot;y&quot;, paste0(&quot;X&quot;, 1:p)) # Train-test split set.seed(42) data_split &lt;- initial_split(data, prop = 0.7) train_data &lt;- training(data_split) test_data &lt;- testing(data_split) # Prepare data for Stan X_train &lt;- as.matrix(train_data[, -1]) X_test &lt;- as.matrix(test_data[, -1]) y_train &lt;- train_data$y y_test &lt;- test_data$y 5.5.1 Bayesian Ridge Regression # Stan model for Bayesian Ridge Regression stan_ridge &lt;- &quot; data { int&lt;lower=0&gt; N_train; int&lt;lower=0&gt; N_test; int&lt;lower=0&gt; p; matrix[N_train, p] X_train; matrix[N_test, p] X_test; vector[N_train] y_train; } parameters { real alpha; vector[p] beta; real&lt;lower=0&gt; sigma; real&lt;lower=0&gt; tau; // prior standard deviation for beta } model { // Priors alpha ~ normal(0, 10); tau ~ cauchy(0, 1); sigma ~ cauchy(0, 5); beta ~ normal(0, tau); // Ridge prior // Likelihood y_train ~ normal(alpha + X_train * beta, sigma); } generated quantities { vector[N_test] y_pred; for (i in 1:N_test) { y_pred[i] = normal_rng(alpha + X_test[i] * beta, sigma); } } &quot; # Prepare data stan_data_ridge &lt;- list( N_train = n_train, N_test = n_test, p = p, X_train = X_train, X_test = X_test, y_train = y_train ) # Fit model fit_ridge &lt;- stan(model_code = stan_ridge, data = stan_data_ridge, chains = 4, iter = 2000, cores = parallel::detectCores()) print(fit_ridge, pars = c(&quot;alpha&quot;, &quot;beta&quot;, &quot;sigma&quot;, &quot;tau&quot;)) ## Inference for Stan model: anon_model. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## alpha 40.32 0.00 0.18 39.96 40.19 40.31 40.44 40.67 7148 1 ## beta[1] -1.86 0.00 0.19 -2.24 -1.99 -1.86 -1.73 -1.48 6679 1 ## beta[2] 2.64 0.00 0.18 2.28 2.52 2.64 2.76 2.99 5238 1 ## beta[3] 3.91 0.00 0.20 3.53 3.77 3.91 4.04 4.28 6158 1 ## beta[4] 1.09 0.00 0.19 0.70 0.97 1.10 1.22 1.47 6568 1 ## beta[5] 0.32 0.00 0.19 -0.04 0.20 0.32 0.45 0.70 6110 1 ## sigma 4.97 0.00 0.13 4.72 4.88 4.97 5.06 5.24 7300 1 ## tau 2.53 0.02 0.94 1.42 1.92 2.32 2.88 4.87 2591 1 ## ## Samples were drawn using NUTS(diag_e) at Thu Oct 16 08:50:41 2025. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 5.5.2 Bayesian LASSO Regression # Stan model for Bayesian LASSO stan_lasso &lt;- &quot; data { int&lt;lower=0&gt; N_train; int&lt;lower=0&gt; N_test; int&lt;lower=0&gt; p; matrix[N_train, p] X_train; matrix[N_test, p] X_test; vector[N_train] y_train; } parameters { real alpha; vector[p] beta; real&lt;lower=0&gt; sigma; real&lt;lower=0&gt; b; // Laplace scale parameter } model { // Priors alpha ~ normal(0, 10); b ~ cauchy(0, 1); sigma ~ cauchy(0, 5); beta ~ double_exponential(0, b); // LASSO prior // Likelihood y_train ~ normal(alpha + X_train * beta, sigma); } generated quantities { vector[N_test] y_pred; for (i in 1:N_test) { y_pred[i] = normal_rng(alpha + X_test[i] * beta, sigma); } } &quot; # Prepare data (same as Ridge) stan_data_lasso &lt;- stan_data_ridge # Fit model fit_lasso &lt;- stan(model_code = stan_lasso, data = stan_data_lasso, chains = 4, iter = 2000, cores = parallel::detectCores()) print(fit_lasso, pars = c(&quot;alpha&quot;, &quot;beta&quot;, &quot;sigma&quot;, &quot;b&quot;)) ## Inference for Stan model: anon_model. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## alpha 40.32 0.00 0.19 39.95 40.19 40.32 40.44 40.68 7136 1 ## beta[1] -1.85 0.00 0.19 -2.21 -1.98 -1.85 -1.72 -1.48 7288 1 ## beta[2] 2.64 0.00 0.19 2.27 2.52 2.64 2.77 3.01 7301 1 ## beta[3] 3.92 0.00 0.20 3.53 3.78 3.92 4.06 4.29 7501 1 ## beta[4] 1.08 0.00 0.19 0.71 0.95 1.08 1.22 1.46 7458 1 ## beta[5] 0.31 0.00 0.18 -0.06 0.19 0.31 0.43 0.67 6465 1 ## sigma 4.97 0.00 0.13 4.72 4.88 4.97 5.06 5.24 6770 1 ## b 2.12 0.02 1.04 0.90 1.41 1.88 2.53 4.78 4190 1 ## ## Samples were drawn using NUTS(diag_e) at Thu Oct 16 08:51:46 2025. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 5.5.3 Hierarchical Shrinkage (Horseshoe Prior) # Stan model for Horseshoe prior stan_horseshoe &lt;- &quot; data { int&lt;lower=0&gt; N_train; int&lt;lower=0&gt; N_test; int&lt;lower=0&gt; p; matrix[N_train, p] X_train; matrix[N_test, p] X_test; vector[N_train] y_train; } parameters { real alpha; vector[p] beta_raw; real&lt;lower=0&gt; sigma; real&lt;lower=0&gt; tau; // global shrinkage vector&lt;lower=0&gt;[p] lambda; // local shrinkage } transformed parameters { vector[p] beta; beta = beta_raw .* lambda * tau; // hierarchical shrinkage } model { // Priors alpha ~ normal(0, 10); tau ~ cauchy(0, 1); // global shrinkage lambda ~ cauchy(0, 1); // local shrinkage beta_raw ~ normal(0, 1); sigma ~ cauchy(0, 5); // Likelihood y_train ~ normal(alpha + X_train * beta, sigma); } generated quantities { vector[N_test] y_pred; for (i in 1:N_test) { y_pred[i] = normal_rng(alpha + X_test[i] * beta, sigma); } } &quot; # Prepare data (same as before) stan_data_horseshoe &lt;- stan_data_ridge # Fit model fit_horseshoe &lt;- stan(model_code = stan_horseshoe, data = stan_data_horseshoe, chains = 4, iter = 2000, cores = parallel::detectCores()) ## Warning: There were 106 divergent transitions after warmup. See ## https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## to find out why this is a problem and how to eliminate them. ## Warning: Examine the pairs() plot to diagnose sampling problems print(fit_horseshoe, pars = c(&quot;alpha&quot;, &quot;beta&quot;, &quot;sigma&quot;, &quot;tau&quot;)) ## Inference for Stan model: anon_model. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## alpha 40.32 0.00 0.19 39.95 40.19 40.32 40.45 40.70 3660 1 ## beta[1] -1.84 0.00 0.20 -2.23 -1.98 -1.84 -1.71 -1.46 3905 1 ## beta[2] 2.65 0.00 0.18 2.30 2.52 2.65 2.77 3.01 3678 1 ## beta[3] 3.93 0.00 0.20 3.54 3.80 3.93 4.06 4.31 3803 1 ## beta[4] 1.07 0.00 0.19 0.69 0.94 1.07 1.20 1.44 4014 1 ## beta[5] 0.26 0.00 0.18 -0.05 0.13 0.26 0.39 0.64 2376 1 ## sigma 4.98 0.00 0.14 4.72 4.88 4.98 5.07 5.24 3966 1 ## tau 2.21 0.04 1.50 0.56 1.21 1.83 2.75 6.06 1429 1 ## ## Samples were drawn using NUTS(diag_e) at Thu Oct 16 08:52:58 2025. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 5.6 Empirical Comparison and Model Evaluation Understanding the theoretical properties of different regularization approaches provides essential intuition, but empirical evaluation reveals how these methods perform in practice. The code below demonstrates how to compare ridge regression, LASSO, and horseshoe priors using both predictive accuracy and coefficient recovery metrics. # Extract predictions and compute metrics extract_metrics &lt;- function(fit, true_values) { y_pred &lt;- extract(fit)$y_pred y_pred_mean &lt;- colMeans(y_pred) # RMSE rmse &lt;- sqrt(mean((y_pred_mean - true_values)^2)) # Coverage (95% credible intervals) y_pred_lower &lt;- apply(y_pred, 2, quantile, 0.025) y_pred_upper &lt;- apply(y_pred, 2, quantile, 0.975) coverage &lt;- mean(true_values &gt;= y_pred_lower &amp; true_values &lt;= y_pred_upper) return(list(rmse = rmse, coverage = coverage)) } # True test values (computed in R, not passed to Stan) y_test_true &lt;- alpha_true + X_test %*% beta_true # Compare models metrics_ridge &lt;- extract_metrics(fit_ridge, y_test_true) metrics_lasso &lt;- extract_metrics(fit_lasso, y_test_true) metrics_horseshoe &lt;- extract_metrics(fit_horseshoe, y_test_true) # Print comparison cat(&quot;Model Comparison (Test Set):\\n&quot;) cat(&quot;Ridge - RMSE:&quot;, round(metrics_ridge$rmse, 3), &quot;Coverage:&quot;, round(metrics_ridge$coverage, 3), &quot;\\n&quot;) cat(&quot;LASSO - RMSE:&quot;, round(metrics_lasso$rmse, 3), &quot;Coverage:&quot;, round(metrics_lasso$coverage, 3), &quot;\\n&quot;) cat(&quot;Horseshoe - RMSE:&quot;, round(metrics_horseshoe$rmse, 3), &quot;Coverage:&quot;, round(metrics_horseshoe$coverage, 3), &quot;\\n&quot;) # Plot coefficient estimates library(bayesplot) library(ggplot2) # Extract posterior samples beta_ridge &lt;- extract(fit_ridge)$beta beta_lasso &lt;- extract(fit_lasso)$beta beta_horseshoe &lt;- extract(fit_horseshoe)$beta # Create comparison plot beta_df &lt;- data.frame( coefficient = rep(paste0(&quot;beta&quot;, 1:p), 3), method = rep(c(&quot;Ridge&quot;, &quot;LASSO&quot;, &quot;Horseshoe&quot;), each = p), mean = c(colMeans(beta_ridge), colMeans(beta_lasso), colMeans(beta_horseshoe)), lower = c(apply(beta_ridge, 2, quantile, 0.025), apply(beta_lasso, 2, quantile, 0.025), apply(beta_horseshoe, 2, quantile, 0.025)), upper = c(apply(beta_ridge, 2, quantile, 0.975), apply(beta_lasso, 2, quantile, 0.975), apply(beta_horseshoe, 2, quantile, 0.975)), true_value = rep(beta_true, 3) ) ggplot(beta_df, aes(x = coefficient, y = mean, color = method)) + geom_point(position = position_dodge(0.3)) + geom_errorbar(aes(ymin = lower, ymax = upper), position = position_dodge(0.3), width = 0.2) + geom_point(aes(y = true_value), color = &quot;black&quot;, shape = 4, size = 3) + labs(title = &quot;Coefficient Estimates Comparison&quot;, subtitle = &quot;Black crosses show true values&quot;, x = &quot;Coefficient&quot;, y = &quot;Estimate&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) The empirical comparison typically reveals distinct patterns in how each method handles different types of signals. Ridge regression tends to shrink all coefficients proportionally, maintaining the relative magnitudes while reducing overall scale. This behavior works well when most predictors contribute meaningfully to the response, even if their individual effects are modest. LASSO regression exhibits more dramatic behavior, often setting smaller coefficients to exactly zero while leaving larger ones relatively unshrunk. This creates sparse solutions that can be easier to interpret but may sacrifice some predictive accuracy when the true model is dense. Hierarchical shrinkage methods like the horseshoe prior attempt to get the best of both worlds by adapting the amount of shrinkage to each coefficient individually. Large coefficients experience minimal shrinkage, allowing them to maintain their full predictive power, while small coefficients get shrunk aggressively toward zero, reducing noise in the model. This adaptivity often leads to superior predictive performance, especially in settings where the true coefficient vector contains a mixture of large, moderate, and negligible effects. 5.7 Understanding Posterior Behavior The mathematical structure of each prior determines not just point estimates but the entire posterior distribution. Ridge regression produces a posterior mean with the closed form \\(E[\\boldsymbol{\\beta}|\\mathbf{y}] = \\left(\\frac{1}{\\sigma^2}\\mathbf{X}^T\\mathbf{X} + \\frac{1}{\\tau^2}\\mathbf{I}\\right)^{-1}\\frac{1}{\\sigma^2}\\mathbf{X}^T\\mathbf{y}\\), which clearly shows how the prior variance \\(\\tau^2\\) balances against the data-driven term \\(\\mathbf{X}^T\\mathbf{X}\\). When \\(\\tau^2\\) is small relative to \\(\\sigma^2\\), the prior dominates and coefficients shrink strongly toward zero. When \\(\\tau^2\\) is large, the data term dominates and estimates approach ordinary least squares. LASSO regression does not admit a closed-form posterior mean due to the non-conjugate Laplace prior, but the posterior mode corresponds exactly to the frequentist LASSO solution. The full posterior distribution, obtained through MCMC sampling, provides uncertainty intervals that properly account for both the sparsity-inducing prior and the inherent variability in coefficient estimates. This represents a substantial improvement over frequentist LASSO inference, which requires complex procedures to obtain valid confidence intervals for selected variables. The horseshoe prior creates particularly interesting posterior behavior through its adaptive shrinkage mechanism. Each coefficient experiences shrinkage according to \\(E[\\beta_j|\\text{data}] \\approx (1 - \\kappa_j) \\hat{\\beta}_j^{\\text{OLS}}\\), where \\(\\kappa_j \\in (0,1)\\) represents a data-dependent shrinkage factor. Coefficients with strong signals in the data have shrinkage factors close to zero, leaving them essentially unshrunk, while coefficients with weak signals have shrinkage factors approaching one, causing aggressive shrinkage toward zero. 5.8 Choosing Among Regularization Methods The choice between ridge regression, LASSO, and hierarchical shrinkage depends critically on the structure expected in the true coefficient vector and the goals of the analysis. Ridge regression works best when most predictors contribute meaningfully to the response, even if some effects are small. This commonly occurs in settings where the predictors represent different aspects of the same underlying phenomenon or when domain knowledge suggests that excluding variables entirely would be inappropriate. LASSO regression excels when the true model is sparse, meaning that only a subset of predictors have non-zero effects. The automatic variable selection property makes LASSO particularly attractive for exploratory analyses where identifying the most important predictors is as important as achieving good predictive performance. However, LASSO can struggle when several predictors are highly correlated, as it tends to select one arbitrarily and zero out the others, potentially missing important relationships. Hierarchical shrinkage methods like the horseshoe prior offer the most flexibility by allowing the data to determine the appropriate level of sparsity. These methods work well across a wide range of scenarios, from dense models where most predictors matter to sparse models where only a few predictors have substantial effects. The primary cost of this flexibility is computational complexity, as hierarchical models typically require more sophisticated MCMC sampling schemes and longer chains to achieve convergence. 5.9 Conclusion Bayesian regularized regression represents a natural and principled approach to handling the challenges that arise in modern statistical modeling. By reformulating penalty-based methods in terms of prior distributions, the Bayesian framework provides not only point estimates equivalent to their frequentist counterparts but also full posterior distributions that enable proper uncertainty quantification and model comparison. The connection between prior specifications and regularization behavior offers valuable insight into why different methods work well in different contexts and how to choose appropriately among them. The three approaches examined here span a useful range of assumptions about coefficient behavior. Ridge regression assumes all coefficients are a priori similar and should be shrunk proportionally, LASSO assumes that many coefficients are exactly zero and should be eliminated entirely, and hierarchical shrinkage assumes that coefficients vary in importance and should be shrunk adaptively. Modern computing makes all three approaches feasible for most practical applications, shifting the primary challenge from computational limitations to thoughtful model selection based on domain knowledge and data characteristics. These Bayesian methods also integrate naturally into larger modeling workflows that require uncertainty quantification, model averaging, or decision-making under uncertainty. The posterior distributions provide the building blocks for more complex analyses while maintaining the computational efficiency and theoretical elegance that make regularized regression so appealing in the first place. 5.10 References Carvalho, C. M., Polson, N. G., &amp; Scott, J. G. (2010). The horseshoe estimator for sparse signals. Biometrika, 97(2), 465-480. Park, T., &amp; Casella, G. (2008). The Bayesian lasso. Journal of the American Statistical Association, 103(482), 681-686. Piironen, J., &amp; Vehtari, A. (2017). Sparsity information and regularization in the horseshoe and other shrinkage priors. Electronic Journal of Statistics, 11(2), 5018-5051. Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, 58(1), 267-288. "],["bayesian-quantile-regression.html", "Chapter 6 Bayesian Quantile Regression 6.1 Introduction 6.2 Model Specification 6.3 Conclusion 6.4 References", " Chapter 6 Bayesian Quantile Regression 6.1 Introduction Quantile regression extends traditional regression analysis beyond the conditional mean to model the entire conditional distribution of the response variable. For a response variable \\(Y\\) and predictor variables \\(\\mathbf{X}\\), the \\(\\tau\\)-th quantile function is defined as \\(Q_\\tau(Y|\\mathbf{X}) = \\inf\\{y : F_{Y|\\mathbf{X}}(y) \\geq \\tau\\}\\), where \\(F_{Y|\\mathbf{X}}(y)\\) represents the conditional cumulative distribution function and \\(\\tau \\in (0,1)\\) denotes the quantile level. This formulation allows researchers to examine how predictor variables influence different portions of the response distribution, providing insights that may be obscured when focusing solely on conditional means. The mathematical foundation of quantile regression rests on the asymmetric Laplace distribution (ALD), which serves as the working likelihood. For the \\(\\tau\\)-th quantile, the ALD has the probability density function: \\[f(y|\\mu, \\sigma, \\tau) = \\frac{\\tau(1-\\tau)}{\\sigma} \\exp\\left(-\\rho_\\tau\\left(\\frac{y-\\mu}{\\sigma}\\right)\\right)\\] where \\(\\mu\\) represents the location parameter (the \\(\\tau\\)-th quantile), \\(\\sigma &gt; 0\\) is the scale parameter, and \\(\\rho_\\tau(u) = u(\\tau - I(u &lt; 0))\\) is the check function with \\(I(\\cdot)\\) being the indicator function. This check function is fundamental to quantile regression as it provides the loss function that is minimized during estimation: \\(\\sum_{i=1}^n \\rho_\\tau(y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})\\). The Bayesian framework transforms quantile regression into a hierarchical model where uncertainty is explicitly modeled through prior distributions. Consider the linear quantile regression model \\(Q_\\tau(Y_i|\\mathbf{x}_i) = \\mathbf{x}_i^T\\boldsymbol{\\beta}_\\tau\\), where \\(\\boldsymbol{\\beta}_\\tau\\) represents the vector of regression coefficients specific to the \\(\\tau\\)-th quantile. In the Bayesian setting, we specify prior distributions for these parameters, typically \\(\\boldsymbol{\\beta}_\\tau \\sim \\mathcal{N}(\\boldsymbol{\\mu}_0, \\boldsymbol{\\Sigma}_0)\\) for the regression coefficients and \\(\\sigma \\sim \\text{Inv-Gamma}(a, b)\\) for the scale parameter. The posterior distribution is then proportional to the product of the likelihood and priors: \\(p(\\boldsymbol{\\beta}_\\tau, \\sigma | \\mathbf{y}, \\mathbf{X}) \\propto \\prod_{i=1}^n f(y_i|\\mathbf{x}_i^T\\boldsymbol{\\beta}_\\tau, \\sigma, \\tau) \\cdot p(\\boldsymbol{\\beta}_\\tau) \\cdot p(\\sigma)\\). Stan implements this Bayesian quantile regression through efficient Hamiltonian Monte Carlo (HMC) sampling, which provides several computational advantages over traditional optimization-based methods. The HMC algorithm exploits the geometry of the posterior distribution by using gradient information to propose new states in the Markov chain, resulting in more efficient exploration of the parameter space. This is particularly beneficial for quantile regression models, which often exhibit multimodal or skewed posterior distributions due to the asymmetric nature of the loss function. The resulting MCMC samples provide a complete characterization of parameter uncertainty, enabling probabilistic statements about quantile estimates and facilitating hypothesis testing through posterior probability calculations. The incorporation of prior information represents a significant advantage of the Bayesian approach, particularly in healthcare applications where domain expertise can inform model specification. Informative priors can be constructed based on previous studies, clinical knowledge, or expert opinion. For instance, if prior research suggests that a treatment effect is positive but with considerable uncertainty, this can be encoded through a normal prior centered at a positive value with appropriate variance. Mathematically, this might be expressed as \\(\\beta_{\\text{treatment}} \\sim \\mathcal{N}(0.5, 1.0^2)\\), indicating a belief that the treatment effect is likely positive but allowing for substantial uncertainty. Such priors serve as regularization mechanisms, particularly valuable when dealing with limited data or complex models prone to overfitting. The Bayesian framework naturally provides uncertainty quantification through the posterior distribution, which contrasts sharply with frequentist approaches that rely on asymptotic approximations. For any function of the parameters \\(g(\\boldsymbol{\\theta})\\), credible intervals can be constructed directly from the posterior samples without relying on distributional assumptions. This is particularly valuable for quantile regression, where the sampling distribution of quantile estimates can be complex and non-normal. The posterior predictive distribution \\(p(\\tilde{y}|\\mathbf{x}, \\mathbf{y}, \\mathbf{X})\\) provides a complete description of uncertainty in future observations, enabling probabilistic forecasting and risk assessment. Model comparison and selection within the Bayesian quantile regression framework can be performed using information-theoretic criteria such as the Widely Applicable Information Criterion (WAIC) or Leave-One-Out Cross-Validation (LOO-CV). These criteria balance model fit against complexity, providing guidance for variable selection and model specification. For nested models, Bayes factors can be computed to quantify the evidence in favor of one model over another: \\(BF_{12} = \\frac{p(\\mathbf{y}|M_1)}{p(\\mathbf{y}|M_2)}\\), where \\(p(\\mathbf{y}|M_i)\\) represents the marginal likelihood under model \\(M_i\\). Additionally, posterior predictive checking enables model validation by comparing observed data patterns with those generated from the fitted model. Healthcare applications particularly benefit from Bayesian quantile regression due to the heterogeneous nature of clinical data and the importance of understanding distributional effects rather than just average treatment effects. Consider a clinical trial where the primary outcome exhibits heteroscedasticity across patient subgroups. Traditional mean regression might indicate no overall treatment effect, while quantile regression could reveal significant benefits for patients in the upper quantiles of the outcome distribution, corresponding perhaps to those with more severe disease. The Bayesian approach allows for the incorporation of prior knowledge about biological mechanisms, previous trial results, and regulatory requirements, while providing uncertainty estimates that are crucial for clinical decision-making. The computational implementation in Stan leverages automatic differentiation to compute gradients of the log-posterior density, enabling efficient HMC sampling even for complex models with hundreds of parameters. Stan’s modeling language allows for flexible specification of hierarchical structures, time-varying coefficients, and non-linear relationships, making it particularly suitable for the complex models often encountered in healthcare research. The software automatically handles the transformation of constrained parameters, monitors convergence diagnostics, and provides comprehensive posterior summaries, making advanced Bayesian quantile regression accessible to applied researchers. To illustrate the practical implementation of Bayesian quantile regression with Stan, we now examine a synthetic heteroskedastic dataset that demonstrates the method’s ability to capture distributional heterogeneity across the predictor space. This example will showcase how the Bayesian framework handles uncertainty quantification while revealing patterns that would be missed by traditional mean-based regression approaches. 6.2 Model Specification 6.2.1 Using bayesQR package Simulate data from heteroskedastic regression set.seed(66) library(brms) ## Loading required package: Rcpp ## ## Attaching package: &#39;Rcpp&#39; ## The following object is masked from &#39;package:rsample&#39;: ## ## populate ## Loading &#39;brms&#39; package (version 2.22.0). Useful instructions ## can be found by typing help(&#39;brms&#39;). A more detailed introduction ## to the package is available through vignette(&#39;brms_overview&#39;). ## ## Attaching package: &#39;brms&#39; ## The following object is masked from &#39;package:rstan&#39;: ## ## loo ## The following object is masked from &#39;package:dials&#39;: ## ## mixture ## The following object is masked from &#39;package:stats&#39;: ## ## ar library(bayesQR) ## ## Attaching package: &#39;bayesQR&#39; ## The following object is masked from &#39;package:brms&#39;: ## ## prior n &lt;- 200 X &lt;- runif(n=n,min=0,max=10) X &lt;- X y &lt;- 1 + 2*X + rnorm(n=n, mean=0, sd=.6*X) Estimate series of quantile regressions with adaptive lasso to limit execution time of the example, ndraw is set to a very low value. Set value to 5000 for a better approximation of the posterior distirubtion. library(brms) library(bayesQR) out &lt;- bayesQR(y~X, quantile=c(.05,.25,.5,.75,.95), alasso=TRUE, ndraw=500) ## ************************************************ ## * Start estimating quantile 1 of 5 in total * ## ************************************************ ## Current iteration : ## [1] 500 ## ************************************************ ## * Start estimating quantile 2 of 5 in total * ## ************************************************ ## Current iteration : ## [1] 500 ## ************************************************ ## * Start estimating quantile 3 of 5 in total * ## ************************************************ ## Current iteration : ## [1] 500 ## ************************************************ ## * Start estimating quantile 4 of 5 in total * ## ************************************************ ## Current iteration : ## [1] 500 ## ************************************************ ## * Start estimating quantile 5 of 5 in total * ## ************************************************ ## Current iteration : ## [1] 500 Initiate plot plot(X, y, main=&quot;&quot;, cex=.6, xlab=&quot;X&quot;) ## Add quantile regression lines to the plot (exclude first 500 burn-in draws) sum &lt;- summary(out, burnin=50) for (i in 1:length(sum)){ abline(a=sum[[i]]$betadraw[1,1],b=sum[[i]]$betadraw[2,1],lty=i,col=i) } outOLS = lm(y~X) plot(X, y, pch = 19, col = &quot;blue&quot;) abline(outOLS,lty=1,lwd=2,col=6) # Add legend to plot legend(x=0,y=max(y),legend=c(.05,.25,.50,.75,.95,&quot;OLS&quot;),lty=c(1,2,3,4,5,1), lwd=c(1,1,1,1,1,2),col=c(1:6),title=&quot;Quantile&quot;) 6.2.2 Using brms package n &lt;- 200 x &lt;- runif(n = n, min = 0, max = 10) y &lt;- 1 + 2 * x + rnorm(n = n, mean = 0, sd = 0.6*x) dat &lt;- data.frame(x, y) # fit the 20%-quantile fit &lt;- brm(bf(y ~ x, quantile = 0.2), data = dat, family = asym_laplace()) ## Compiling Stan program... ## Start sampling ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.00012 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.2 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.246 seconds (Warm-up) ## Chain 1: 0.218 seconds (Sampling) ## Chain 1: 0.464 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 3.3e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.33 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.247 seconds (Warm-up) ## Chain 2: 0.277 seconds (Sampling) ## Chain 2: 0.524 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 5e-05 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.5 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.237 seconds (Warm-up) ## Chain 3: 0.228 seconds (Sampling) ## Chain 3: 0.465 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 3.1e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.226 seconds (Warm-up) ## Chain 4: 0.218 seconds (Sampling) ## Chain 4: 0.444 seconds (Total) ## Chain 4: summary(fit) ## Family: asym_laplace ## Links: mu = identity; sigma = identity; quantile = identity ## Formula: y ~ x ## quantile = 0.2 ## Data: dat (Number of observations: 200) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Regression Coefficients: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.08 0.20 0.66 1.46 1.00 3146 2472 ## x 1.43 0.05 1.33 1.54 1.00 2156 1963 ## ## Further Distributional Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.84 0.06 0.73 0.96 1.00 2340 2532 ## quantile 0.20 0.00 0.20 0.20 NA NA NA ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 6.2.3 Backend stan model functions { /* helper function for asym_laplace_lpdf * Args: * y: the response value * quantile: quantile parameter in (0, 1) */ real rho_quantile(real y, real quantile) { if (y &lt; 0) { return y * (quantile - 1); } else { return y * quantile; } } /* asymmetric laplace log-PDF for a single response * Args: * y: the response value * mu: location parameter * sigma: positive scale parameter * quantile: quantile parameter in (0, 1) * Returns: * a scalar to be added to the log posterior */ real asym_laplace_lpdf(real y, real mu, real sigma, real quantile) { return log(quantile * (1 - quantile)) - log(sigma) - rho_quantile((y - mu) / sigma, quantile); } /* asymmetric laplace log-CDF for a single quantile * Args: * y: a quantile * mu: location parameter * sigma: positive scale parameter * quantile: quantile parameter in (0, 1) * Returns: * a scalar to be added to the log posterior */ real asym_laplace_lcdf(real y, real mu, real sigma, real quantile) { if (y &lt; mu) { return log(quantile) + (1 - quantile) * (y - mu) / sigma; } else { return log1m((1 - quantile) * exp(-quantile * (y - mu) / sigma)); } } /* asymmetric laplace log-CCDF for a single quantile * Args: * y: a quantile * mu: location parameter * sigma: positive scale parameter * quantile: quantile parameter in (0, 1) * Returns: * a scalar to be added to the log posterior */ real asym_laplace_lccdf(real y, real mu, real sigma, real quantile) { if (y &lt; mu) { return log1m(quantile * exp((1 - quantile) * (y - mu) / sigma)); } else { return log1m(quantile) - quantile * (y - mu) / sigma; } } } data { int&lt;lower=1&gt; N; // total number of observations vector[N] Y; // response variable int&lt;lower=1&gt; K; // number of population-level effects matrix[N, K] X; // population-level design matrix int prior_only; // should the likelihood be ignored? } transformed data { int Kc = K - 1; matrix[N, Kc] Xc; // centered version of X without an intercept vector[Kc] means_X; // column means of X before centering for (i in 2:K) { means_X[i - 1] = mean(X[, i]); Xc[, i - 1] = X[, i] - means_X[i - 1]; } } parameters { vector[Kc] b; // population-level effects real Intercept; // temporary intercept for centered predictors real&lt;lower=0&gt; sigma; // dispersion parameter } transformed parameters { real quantile = 0.2; // quantile parameter real lprior = 0; // prior contributions to the log posterior lprior += student_t_lpdf(Intercept | 3, 11, 7.8); lprior += student_t_lpdf(sigma | 3, 0, 7.8) - 1 * student_t_lccdf(0 | 3, 0, 7.8); } model { // likelihood including constants if (!prior_only) { // initialize linear predictor term vector[N] mu = rep_vector(0.0, N); mu += Intercept + Xc * b; for (n in 1:N) { target += asym_laplace_lpdf(Y[n] | mu[n], sigma, quantile); } } // priors including constants target += lprior; } generated quantities { // actual population-level intercept real b_Intercept = Intercept - dot_product(means_X, b); } This Stan code specifies a Bayesian model for asymmetric Laplace regression, where the main goal is to estimate the population-level effects and dispersion parameter of the model from the provided data. The asymmetric Laplace distribution is used as the likelihood function for the response variable. The functions block contains three helper functions: rho_quantile, asym_laplace_lpdf, and asym_laplace_lccdf. These functions are used to calculate the asymmetric Laplace log-PDF, log-CDF, and log-CCDF for a single response variable. The data block defines the input data for the model, including the total number of observations N, the response variable Y, the number of population-level effects K, the population-level design matrix X, and a binary variable prior_only that indicates whether to ignore the likelihood (for prior-only sampling). The transformed data block preprocesses the data. It calculates the centered version of the design matrix Xc, removes the intercept from the design matrix X, and stores the column means of X before centering in the vector means_X. The parameters block defines the parameters to be estimated in the model. It includes the population-level effects b, the temporary intercept for centered predictors Intercept, and the dispersion parameter sigma. The transformed parameters block calculates the quantile parameter quantile (set to 0.2 in this case) and the prior contributions to the log posterior (lprior). The lprior term includes the priors for the Intercept and sigma parameters, which are specified as Student’s t-distributions. The model block defines the likelihood and priors for the model. The likelihood accounts for the asymmetric Laplace distribution for the response variable Y, given the linear predictor mu (calculated using the population-level effects b and Intercept) and the dispersion parameter sigma. If prior_only is true, the likelihood is ignored, and the model only considers the priors. The generated quantities block computes the actual population-level intercept b_Intercept by removing the effect of the centered predictors from the temporary intercept Intercept. 6.3 Conclusion Bayesian quantile regression extends classical regression analysis by allowing researchers to model the entire conditional distribution of outcomes rather than restricting inference to the mean. By leveraging the asymmetric Laplace distribution as a working likelihood and incorporating prior information, this approach offers a principled framework for uncertainty quantification, flexible model specification, and probabilistic forecasting. The use of Hamiltonian Monte Carlo, as implemented in Stan, enables efficient exploration of complex posterior distributions, making Bayesian quantile regression both theoretically rigorous and computationally feasible. The examples with simulated heteroskedastic data highlight its capacity to reveal heterogeneous effects across the outcome distribution that would remain hidden under mean-based methods, underscoring its practical relevance for applied domains such as healthcare research, where distributional differences often carry critical substantive implications. 6.4 References Yu, K., &amp; Moyeed, R. A. (2001). Bayesian quantile regression. Statistics &amp; Probability Letters, 54(4), 437-447. Kottas, A., &amp; Gelfand, A. E. (2001). Bayesian semiparametric median regression modeling. Journal of the American Statistical Association, 97(457), 109-121. Koenker, R., &amp; Xiao, Z. (2006). Quantile autoregression. Journal of the American Statistical Association, 101(475), 980-990. Yu, K., &amp; Moyeed, R. A. (2000). Bayesian quantile regression. Journal of the Royal Statistical Society: Series D (The Statistician), 49(3), 385-392. Koenker, R., &amp; Xiao, Z. (2004). Inference on the quantile regression process. Econometrica, 72(1), 71-104. https://cran.r-project.org/web/packages/bayesQR/bayesQR.pdf "],["bayesian-multilevel-regression-in-stan.html", "Chapter 7 Bayesian Multilevel Regression in Stan 7.1 Introduction 7.2 Implementation in R 7.3 Visualization and Interpretation 7.4 Conclusion 7.5 References", " Chapter 7 Bayesian Multilevel Regression in Stan 7.1 Introduction When analyzing data with inherent hierarchical structures—such as students nested within schools, patients within hospitals, or repeated measurements within individuals—traditional linear regression falls short by assuming independence among all observations. This assumption becomes problematic when observations within groups are more similar to each other than to observations from other groups, leading to underestimated standard errors and inflated Type I error rates. Bayesian multilevel (mixed effects) regression models address this challenge by explicitly modeling the hierarchical structure of data through the incorporation of both fixed effects (population-level parameters) and random effects (group-specific parameters). Unlike their frequentist counterparts, Bayesian multilevel models leverage prior distributions to achieve partial pooling of information across groups, resulting in more stable parameter estimates and better predictive performance, particularly when dealing with small sample sizes within groups. The fundamental insight of multilevel modeling lies in partial pooling—a middle ground between complete pooling (ignoring group structure) and no pooling (estimating parameters independently for each group). Through hierarchical priors, Bayesian models naturally shrink group-specific estimates toward the population mean in proportion to the uncertainty in group-level estimates, effectively borrowing strength across groups while preserving important group-level variation. This comprehensive guide explores three fundamental approaches to Bayesian multilevel modeling: random intercepts models that allow baseline differences across groups, random slopes models that permit varying treatment effects, and hierarchical priors models that capture correlations between group-level parameters. We implement each approach in Stan, providing mathematical foundations, practical R code, and visualization techniques to illuminate the behavior of these models. Consider a dataset with \\(i = 1, \\ldots, n_j\\) observations nested within \\(j = 1, \\ldots, J\\) groups, where \\(y_{ij}\\) represents the outcome for observation \\(i\\) in group \\(j\\), and \\(x_{ij}\\) represents the corresponding predictor. The general form of a multilevel model can be written as: \\[y_{ij} = \\alpha_j + \\beta_j x_{ij} + \\epsilon_{ij}\\] where \\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\) represents the individual-level residual. The group-specific parameters \\(\\alpha_j\\) and \\(\\beta_j\\) are themselves modeled as random variables: \\[\\alpha_j = \\alpha + u_{j}^{(\\alpha)}\\] \\[\\beta_j = \\beta + u_{j}^{(\\beta)}\\] where \\(\\alpha\\) and \\(\\beta\\) are population-level (fixed) effects, and \\(u_{j}^{(\\alpha)}\\) and \\(u_{j}^{(\\beta)}\\) are group-level random effects typically assumed to follow a multivariate normal distribution: \\[\\begin{pmatrix} u_{j}^{(\\alpha)} \\\\ u_{j}^{(\\beta)} \\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\mathbf{\\Sigma}\\right)\\] The covariance matrix \\(\\mathbf{\\Sigma}\\) captures both the variability of group-level effects and their potential correlation. This hierarchical structure enables the model to pool information across groups while accounting for group-specific variation. 7.2 Implementation in R 7.2.0.1 Data Generation and Preparation To demonstrate these concepts, we’ll work with simulated data that exhibits realistic hierarchical structure. Our simulation includes correlated group effects to showcase the full capability of multilevel models. library(tidyverse) library(tidymodels) library(rstan) library(bayesplot) ## This is bayesplot version 1.12.0 ## - Online documentation and vignettes at mc-stan.org/bayesplot ## - bayesplot theme set to bayesplot::theme_default() ## * Does _not_ affect other ggplot2 plots ## * See ?bayesplot_theme_set for details on theme setting ## ## Attaching package: &#39;bayesplot&#39; ## The following object is masked from &#39;package:brms&#39;: ## ## rhat library(ggplot2) library(corrplot) ## corrplot 0.95 loaded library(patchwork) # Set random seed for reproducibility set.seed(123) # Data generation parameters n_total &lt;- 1000 n_groups &lt;- 20 n_per_group &lt;- n_total / n_groups # Population-level parameters alpha_pop &lt;- 40 # population intercept beta_pop &lt;- 3 # population slope sigma_y &lt;- 4 # residual standard deviation # Group-level variation parameters sigma_alpha &lt;- 8 # SD of group intercepts sigma_beta &lt;- 2 # SD of group slopes rho &lt;- 0.3 # correlation between random intercepts and slopes # Create covariance matrix for group effects Sigma_group &lt;- matrix(c(sigma_alpha^2, rho * sigma_alpha * sigma_beta, rho * sigma_alpha * sigma_beta, sigma_beta^2), nrow = 2, ncol = 2) # Generate group effects group_effects &lt;- MASS::mvrnorm(n_groups, mu = c(0, 0), Sigma = Sigma_group) group_intercepts &lt;- group_effects[, 1] group_slopes &lt;- group_effects[, 2] # Generate individual-level data group_id &lt;- rep(1:n_groups, each = n_per_group) x &lt;- rnorm(n_total, mean = 0, sd = 1.5) y &lt;- (alpha_pop + group_intercepts[group_id]) + (beta_pop + group_slopes[group_id]) * x + rnorm(n_total, mean = 0, sd = sigma_y) # Create dataset data_full &lt;- tibble( y = y, x = x, group_id = factor(group_id), group_intercept_true = group_intercepts[group_id], group_slope_true = group_slopes[group_id] ) # Train-test split set.seed(42) data_split &lt;- initial_split(data_full, prop = 0.75, strata = group_id) ## Warning: Too little data to stratify. ## • Resampling will be unstratified. train_data &lt;- training(data_split) test_data &lt;- testing(data_split) # Display data structure cat(&quot;Dataset structure:\\n&quot;) ## Dataset structure: cat(&quot;Total observations:&quot;, nrow(data_full), &quot;\\n&quot;) ## Total observations: 1000 cat(&quot;Number of groups:&quot;, n_groups, &quot;\\n&quot;) ## Number of groups: 20 cat(&quot;Average observations per group:&quot;, n_per_group, &quot;\\n&quot;) ## Average observations per group: 50 cat(&quot;Training set size:&quot;, nrow(train_data), &quot;\\n&quot;) ## Training set size: 750 cat(&quot;Test set size:&quot;, nrow(test_data), &quot;\\n&quot;) ## Test set size: 250 # Visualize the hierarchical structure p1 &lt;- ggplot(train_data, aes(x = x, y = y, color = group_id)) + geom_point(alpha = 0.6, size = 1.2) + geom_smooth(method = &quot;lm&quot;, se = FALSE, size = 0.8) + labs(title = &quot;Data Structure: Group-Specific Regression Lines&quot;, subtitle = &quot;Each color represents a different group&quot;, x = &quot;Predictor (x)&quot;, y = &quot;Outcome (y)&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) + scale_color_viridis_d() ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. print(p1) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 7.2.1 Random Intercepts Model: Modeling Baseline Differences The random intercepts model represents the simplest multilevel structure, allowing each group to have its own baseline level while constraining all groups to have the same slope. This model is particularly useful when we believe the effect of our predictor is consistent across groups, but groups differ in their baseline outcomes. Mathematically, the random intercepts model is specified as: \\[y_{ij} = (\\alpha + u_j) + \\beta x_{ij} + \\epsilon_{ij}\\] where \\(u_j \\sim N(0, \\sigma_\\alpha^2)\\) represents the group-specific deviation from the population intercept \\(\\alpha\\). In the Bayesian framework, we place priors on all parameters, including hyperpriors on the variance components. # Stan model for random intercepts stan_code_intercepts &lt;- &quot; data { int&lt;lower=0&gt; N; // number of observations int&lt;lower=0&gt; J; // number of groups vector[N] y; // outcome variable vector[N] x; // predictor variable array[N] int&lt;lower=1, upper=J&gt; group_id; // group indicators } parameters { real alpha; // population intercept real beta; // population slope real&lt;lower=0&gt; sigma_y; // residual standard deviation real&lt;lower=0&gt; sigma_alpha; // standard deviation of group intercepts vector[J] alpha_raw; // non-centered group intercepts } transformed parameters { vector[J] alpha_group = alpha + sigma_alpha * alpha_raw; // centered parameterization } model { // Priors alpha ~ normal(40, 10); beta ~ normal(3, 5); sigma_y ~ exponential(0.2); sigma_alpha ~ exponential(0.1); alpha_raw ~ std_normal(); // non-centered parameterization // Likelihood for (n in 1:N) { y[n] ~ normal(alpha_group[group_id[n]] + beta * x[n], sigma_y); } } generated quantities { vector[N] y_pred; // posterior predictive samples vector[N] log_lik; // log-likelihood for model comparison for (n in 1:N) { y_pred[n] = normal_rng(alpha_group[group_id[n]] + beta * x[n], sigma_y); log_lik[n] = normal_lpdf(y[n] | alpha_group[group_id[n]] + beta * x[n], sigma_y); } } &quot; # Prepare data for Stan stan_data_intercepts &lt;- list( N = nrow(train_data), J = length(unique(train_data$group_id)), y = train_data$y, x = train_data$x, group_id = as.numeric(train_data$group_id) ) # Compile and fit the model rstan_options(auto_write = TRUE) options(mc.cores = parallel::detectCores()) fit_intercepts &lt;- stan( model_code = stan_code_intercepts, data = stan_data_intercepts, chains = 4, iter = 2000, warmup = 1000, seed = 42 ) ## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. ## Running the chains for more iterations may help. See ## https://mc-stan.org/misc/warnings.html#bulk-ess # Model diagnostics print(fit_intercepts, pars = c(&quot;alpha&quot;, &quot;beta&quot;, &quot;sigma_y&quot;, &quot;sigma_alpha&quot;)) ## Inference for Stan model: anon_model. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## alpha 39.21 0.11 1.88 35.55 37.90 39.19 40.50 42.94 314 1.00 ## beta 3.39 0.00 0.11 3.17 3.31 3.39 3.47 3.61 1313 1.00 ## sigma_y 4.64 0.00 0.12 4.42 4.56 4.64 4.72 4.88 1575 1.00 ## sigma_alpha 8.27 0.08 1.42 5.96 7.25 8.11 9.13 11.45 315 1.01 ## ## Samples were drawn using NUTS(diag_e) at Thu Oct 16 08:54:58 2025. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # Extract posterior samples posterior_intercepts &lt;- rstan::extract(fit_intercepts) # Visualization of results alpha_group_post &lt;- summary(fit_intercepts, pars = &quot;alpha_group&quot;)$summary alpha_group_est &lt;- alpha_group_post[, &quot;mean&quot;] alpha_group_lower &lt;- alpha_group_post[, &quot;2.5%&quot;] alpha_group_upper &lt;- alpha_group_post[, &quot;97.5%&quot;] 7.2.2 Random Slopes Model: Capturing Varying Effects The random slopes model extends the random intercepts approach by allowing both intercepts and slopes to vary across groups. This flexibility is crucial when the effect of a predictor genuinely differs across groups, such as when educational interventions have varying effectiveness across different schools. The mathematical specification becomes: \\[y_{ij} = (\\alpha + u_j^{(\\alpha)}) + (\\beta + u_j^{(\\beta)}) x_{ij} + \\epsilon_{ij}\\] where both \\(u_j^{(\\alpha)}\\) and \\(u_j^{(\\beta)}\\) are group-specific random effects that may be correlated. # Stan model for random slopes stan_code_slopes &lt;- &quot; data { int&lt;lower=0&gt; N; int&lt;lower=0&gt; J; vector[N] y; vector[N] x; array[N] int&lt;lower=1, upper=J&gt; group_id; } parameters { real alpha; // population intercept real beta; // population slope real&lt;lower=0&gt; sigma_y; // residual standard deviation vector&lt;lower=0&gt;[2] sigma_group; // standard deviations of group effects cholesky_factor_corr[2] L_Rho; // Cholesky factor of correlation matrix matrix[2, J] z_group; // non-centered group effects } transformed parameters { matrix[J, 2] group_effects; group_effects = (diag_pre_multiply(sigma_group, L_Rho) * z_group)&#39;; } model { // Priors alpha ~ normal(40, 10); beta ~ normal(3, 5); sigma_y ~ exponential(0.2); sigma_group ~ exponential(0.1); L_Rho ~ lkj_corr_cholesky(2); to_vector(z_group) ~ std_normal(); // Likelihood for (n in 1:N) { y[n] ~ normal(alpha + group_effects[group_id[n], 1] + (beta + group_effects[group_id[n], 2]) * x[n], sigma_y); } } generated quantities { matrix[2, 2] Rho = L_Rho * L_Rho&#39;; // correlation matrix vector[N] y_pred; vector[N] log_lik; for (n in 1:N) { y_pred[n] = normal_rng(alpha + group_effects[group_id[n], 1] + (beta + group_effects[group_id[n], 2]) * x[n], sigma_y); log_lik[n] = normal_lpdf(y[n] | alpha + group_effects[group_id[n], 1] + (beta + group_effects[group_id[n], 2]) * x[n], sigma_y); } } &quot; # Fit random slopes model fit_slopes &lt;- stan( model_code = stan_code_slopes, data = stan_data_intercepts, chains = 4, iter = 2000, warmup = 1000, seed = 42 ) print(fit_slopes, pars = c(&quot;alpha&quot;, &quot;beta&quot;, &quot;sigma_y&quot;, &quot;sigma_group&quot;, &quot;Rho&quot;)) ## Inference for Stan model: anon_model. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## alpha 39.25 0.06 1.84 35.73 37.99 39.25 40.47 42.91 848 1.00 ## beta 3.30 0.01 0.42 2.46 3.02 3.29 3.57 4.14 1275 1.00 ## sigma_y 4.02 0.00 0.10 3.81 3.95 4.01 4.09 4.23 2995 1.00 ## sigma_group[1] 8.32 0.05 1.45 5.96 7.31 8.18 9.12 11.67 707 1.01 ## sigma_group[2] 1.79 0.01 0.35 1.25 1.54 1.74 1.99 2.59 1135 1.00 ## Rho[1,1] 1.00 NaN 0.00 1.00 1.00 1.00 1.00 1.00 NaN NaN ## Rho[1,2] 0.24 0.00 0.20 -0.16 0.11 0.25 0.38 0.61 1584 1.00 ## Rho[2,1] 0.24 0.00 0.20 -0.16 0.11 0.25 0.38 0.61 1584 1.00 ## Rho[2,2] 1.00 0.00 0.00 1.00 1.00 1.00 1.00 1.00 3944 1.00 ## ## Samples were drawn using NUTS(diag_e) at Thu Oct 16 08:56:33 2025. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). # Extract and visualize group effects group_effects_post &lt;- summary(fit_slopes, pars = &quot;group_effects&quot;)$summary group_effects_matrix &lt;- matrix(group_effects_post[, &quot;mean&quot;], nrow = n_groups, ncol = 2) slopes_comparison &lt;- tibble( group = 1:n_groups, true_intercept = group_intercepts, true_slope = group_slopes, est_intercept = group_effects_matrix[, 1], est_slope = group_effects_matrix[, 2] ) p3 &lt;- slopes_comparison %&gt;% ggplot(aes(x = true_slope, y = est_slope)) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;, color = &quot;red&quot;) + geom_point(size = 3, alpha = 0.7, color = &quot;steelblue&quot;) + labs(title = &quot;Random Slopes: Estimated vs. True Group Slopes&quot;, x = &quot;True Group Slopes&quot;, y = &quot;Estimated Group Slopes&quot;) + theme_minimal() + coord_fixed() p4 &lt;- slopes_comparison %&gt;% ggplot(aes(x = est_intercept, y = est_slope)) + geom_point(size = 3, alpha = 0.7, color = &quot;darkgreen&quot;) + geom_smooth(method = &quot;lm&quot;, se = TRUE, color = &quot;red&quot;) + labs(title = &quot;Correlation Between Group Intercepts and Slopes&quot;, subtitle = paste(&quot;Estimated correlation:&quot;, round(summary(fit_slopes, pars = &quot;Rho&quot;)$summary[2, &quot;mean&quot;], 3)), x = &quot;Group Intercepts&quot;, y = &quot;Group Slopes&quot;) + theme_minimal() print(p3 + p4) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 7.2.3 Model Comparison and Visualization To understand the practical differences between these approaches, we’ll compare their predictive performance and examine how they handle the bias-variance tradeoff inherent in multilevel modeling. # Extract log-likelihood for model comparison log_lik_intercepts &lt;- extract_log_lik(fit_intercepts) log_lik_slopes &lt;- extract_log_lik(fit_slopes) # Calculate WAIC for model comparison waic_intercepts &lt;- waic(log_lik_intercepts) waic_slopes &lt;- waic(log_lik_slopes) # Display model comparison cat(&quot;Model Comparison (WAIC):\\n&quot;) cat(&quot;Random Intercepts WAIC:&quot;, round(waic_intercepts$estimates[&quot;waic&quot;, &quot;Estimate&quot;], 2), &quot;\\n&quot;) cat(&quot;Random Slopes WAIC:&quot;, round(waic_slopes$estimates[&quot;waic&quot;, &quot;Estimate&quot;], 2), &quot;\\n&quot;) cat(&quot;Difference:&quot;, round(waic_intercepts$estimates[&quot;waic&quot;, &quot;Estimate&quot;] - waic_slopes$estimates[&quot;waic&quot;, &quot;Estimate&quot;], 2), &quot;\\n&quot;) # Posterior predictive checks y_pred_intercepts &lt;- posterior_intercepts$y_pred y_pred_slopes &lt;- rstan::extract(fit_slopes)$y_pred ppc_intercepts &lt;- ppc_dens_overlay(train_data$y, y_pred_intercepts[1:100, ]) + ggtitle(&quot;Posterior Predictive Check: Random Intercepts&quot;) ppc_slopes &lt;- ppc_dens_overlay(train_data$y, y_pred_slopes[1:100, ]) + ggtitle(&quot;Posterior Predictive Check: Random Slopes&quot;) print(ppc_intercepts + ppc_slopes) # Residual analysis residuals_intercepts &lt;- train_data$y - colMeans(y_pred_intercepts) residuals_slopes &lt;- train_data$y - colMeans(y_pred_slopes) residual_comparison &lt;- tibble( observed = rep(train_data$y, 2), residuals = c(residuals_intercepts, residuals_slopes), model = rep(c(&quot;Random Intercepts&quot;, &quot;Random Slopes&quot;), each = length(residuals_intercepts)), group_id = rep(train_data$group_id, 2) ) p5 &lt;- ggplot(residual_comparison, aes(x = observed, y = residuals)) + geom_point(alpha = 0.6) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;red&quot;) + geom_smooth(method = &quot;loess&quot;, se = FALSE) + facet_wrap(~model) + labs(title = &quot;Residual Analysis: Model Comparison&quot;, x = &quot;Observed Values&quot;, y = &quot;Residuals&quot;) + theme_minimal() print(p5) 7.3 Visualization and Interpretation Understanding multilevel models requires visualizing both the population-level and group-level effects. The following visualizations illuminate the key concepts of shrinkage and partial pooling. # Create comprehensive visualization of group effects group_summary &lt;- train_data %&gt;% group_by(group_id) %&gt;% summarise( n = n(), mean_y = mean(y), mean_x = mean(x), .groups = &quot;drop&quot; ) %&gt;% mutate( group_num = as.numeric(group_id), est_intercept_ri = alpha_group_est, est_intercept_rs = group_effects_matrix[, 1], est_slope_rs = group_effects_matrix[, 2], true_intercept = group_intercepts, true_slope = group_slopes ) # Shrinkage visualization p6 &lt;- group_summary %&gt;% ggplot(aes(x = n)) + geom_point(aes(y = abs(est_intercept_ri - true_intercept)), color = &quot;blue&quot;, alpha = 0.7, size = 2) + geom_smooth(aes(y = abs(est_intercept_ri - true_intercept)), method = &quot;loess&quot;, color = &quot;blue&quot;, se = FALSE) + labs(title = &quot;Shrinkage Effect: Estimation Error vs. Group Sample Size&quot;, subtitle = &quot;Smaller groups show more shrinkage toward population mean&quot;, x = &quot;Group Sample Size&quot;, y = &quot;Absolute Estimation Error&quot;) + theme_minimal() # Correlation structure visualization posterior_rho &lt;- rstan::extract(fit_slopes)$Rho[, 1, 2] p7 &lt;- tibble(rho = posterior_rho) %&gt;% ggplot(aes(x = rho)) + geom_histogram(bins = 30, fill = &quot;steelblue&quot;, alpha = 0.7, color = &quot;white&quot;) + geom_vline(xintercept = rho, color = &quot;red&quot;, linetype = &quot;dashed&quot;, size = 1) + geom_vline(xintercept = mean(posterior_rho), color = &quot;blue&quot;, size = 1) + labs(title = &quot;Posterior Distribution of Intercept-Slope Correlation&quot;, subtitle = paste(&quot;True correlation (red):&quot;, rho, &quot;| Estimated mean (blue):&quot;, round(mean(posterior_rho), 3)), x = &quot;Correlation&quot;, y = &quot;Density&quot;) + theme_minimal() print(p6 + p7) ## `geom_smooth()` using formula = &#39;y ~ x&#39; # Group-specific predictions visualization pred_data &lt;- expand_grid( x = seq(min(train_data$x), max(train_data$x), length.out = 50), group_id = factor(1:min(6, n_groups)) # Show first 6 groups ) %&gt;% mutate(group_num = as.numeric(group_id)) # Add predictions from random slopes model group_effects_sample &lt;- group_effects_matrix[pred_data$group_num, ] pred_data$y_pred &lt;- (alpha_pop + group_effects_sample[, 1]) + (beta_pop + group_effects_sample[, 2]) * pred_data$x p8 &lt;- ggplot() + geom_point(data = filter(train_data, as.numeric(group_id) &lt;= 6), aes(x = x, y = y, color = group_id), alpha = 0.6) + geom_line(data = pred_data, aes(x = x, y = y_pred, color = group_id), size = 1.2) + facet_wrap(~group_id, ncol = 3, labeller = label_both) + labs(title = &quot;Group-Specific Predictions: Random Slopes Model&quot;, subtitle = &quot;Points show observed data, lines show group-specific regression&quot;, x = &quot;Predictor (x)&quot;, y = &quot;Outcome (y)&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) + scale_color_viridis_d() print(p8) 7.4 Conclusion Bayesian multilevel regression models provide a principled approach to analyzing hierarchical data structures, offering several key advantages over traditional regression methods. Through the implementation of random intercepts, random slopes, and hierarchical priors in Stan, we have demonstrated how these models achieve partial pooling of information across groups, resulting in more robust parameter estimates and improved predictive performance. The random intercepts model proves most useful when groups differ primarily in baseline levels but exhibit similar relationships between predictors and outcomes. This approach provides substantial computational efficiency while capturing the essential hierarchical structure. The random slopes model extends this flexibility by allowing treatment effects to vary across groups, making it ideal for situations where intervention effectiveness or predictor relationships genuinely differ between contexts. The mathematical foundation of these models reveals their elegant handling of the bias-variance tradeoff through hierarchical shrinkage. Groups with smaller sample sizes experience greater shrinkage toward population estimates, while groups with more data retain their distinctive characteristics. This adaptive borrowing of strength across groups represents a fundamental advantage of the Bayesian approach. From a practical standpoint, several recommendations emerge from our analysis. First, always examine the correlation structure between random effects, as this often reveals important insights about the underlying data-generating process. Second, use posterior predictive checks and residual analysis to validate model assumptions and identify potential improvements. Third, consider the computational trade-offs between model complexity and interpretability, particularly when working with large numbers of groups or complex hierarchical structures. The Stan implementations presented here incorporate modern best practices, including non-centered parameterization for improved sampling efficiency and appropriate prior specifications that regularize estimates without overwhelming the data. These technical considerations prove crucial for achieving reliable convergence and meaningful posterior inference in practice. Future extensions of these models might include temporal dynamics, non-linear relationships, or more complex hierarchical structures with multiple levels of nesting. The Bayesian framework naturally accommodates such extensions while maintaining coherent uncertainty quantification throughout the modeling process, making it an invaluable tool for modern data analysis in the presence of hierarchical structures. 7.5 References Gelman, A., &amp; Hill, J. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press. Stan Development Team. (2023). Stan User’s Guide: Hierarchical Models. https://mc-stan.org/docs/stan-users-guide/hierarchical.html McElreath, R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan. CRC Press. Sorensen, T., &amp; Vasishth, S. (2015). Bayesian linear mixed models using Stan. Journal of Statistical Software, 80(1), 1-28. "],["gaussian-process-regression-gpr.html", "Chapter 8 Gaussian Process Regression (GPR) 8.1 Introduction 8.2 Challenges 8.3 GPfit package 8.4 Bayesian GPR 8.5 References", " Chapter 8 Gaussian Process Regression (GPR) 8.1 Introduction Gaussian process regression (GPR) is a machine learning method based on non-parametric regression method that can be used to fit arbitrary scalar and vectorial quantities. GPR provides a probabilistic model that can be used to make predictions and estimate the uncertainty of those predictions. A Gaussian process is a generalization of the Gaussian probability distribution to functions, where any finite set of function values has a joint Gaussian distribution. The mean function and covariance function of the Gaussian process describe the prior distribution of the function, and the observations are used to update the prior to the posterior distribution of the function. In GPR, the output variable is assumed to be a function of the input variables, and the function is modeled as a sample from a Gaussian process. The goal is to predict the value of the output variable at a new input point, given the observed data. The predicted value is given by the posterior mean of the Gaussian process, and the uncertainty of the prediction is given by the posterior variance. GPR is particularly useful when the data is noisy or when the function being modeled is complex and nonlinear. The key advantages of GPR over other regression techniques are its flexibility and its ability to provide a probabilistic framework for uncertainty quantification. GPR can be used for both regression and classification problems, and it can handle both scalar and vector-valued outputs. Moreover, GPR can be easily extended to handle non-stationary and non-Gaussian data. In practice, GPR is often implemented using the kernlab or gpflow packages in R or Python, respectively. These packages provide functions for specifying the kernel function, which is used to model the covariance between the input variables, and for estimating the hyperparameters of the kernel function using maximum likelihood or Bayesian methods. 8.2 Challenges Overfitting occurs when a model is too complex and fits the training data too closely, resulting in poor generalization to new data. Like some other machine learning techniques, GPR is prone to overfitting if the model is too complex relative to the amount of data available. Specifically, if the number of hyperparameters of the Gaussian process model is large, or if the covariance function is too flexible, the model may fit the noise in the data rather than the underlying signal. This can result in poor generalization performance, where the model performs well on the training data but poorly on new, unseen data. To mitigate the risk of overfitting in GPR, it is important to carefully select the kernel function and the hyperparameters of the model based on the available data. Cross-validation can be used to estimate the generalization error of the model and to select the optimal values of the hyperparameters. Regularization techniques, such as adding a prior distribution on the hyperparameters or using Bayesian model selection, can also be used to prevent overfitting. Another way to prevent overfitting in GPR is to use a simpler covariance function that captures the key features of the data, rather than trying to fit the noise in the data. Overall, while GPR is a powerful and flexible regression technique, it requires careful tuning of the hyperparameters and selection of the kernel function to prevent overfitting and achieve good generalization performance. GPR use in domains such as healthcare comes with certain challenges and limitations that should be considered. Computational complexity poses a significant challenge, particularly with large datasets, necessitating efficient algorithms and computational resources to handle the complexity. Hyperparameter tuning is another consideration, involving the selection of optimal values for parameters such as the kernel function and noise level. This task can be challenging and may require expert knowledge or extensive experimentation. Furthermore, as GPR models complex relationships, the interpretability of the learned models can become intricate. Understanding the underlying factors contributing to predictions becomes more challenging in highly nonlinear models. These challenges highlight the need for careful consideration and expertise when applying GPR in healthcare settings. GPRs ability to model complex relationships, estimate uncertainties, and provide interpretable predictions makes it an invaluable asset for predictive modeling in healthcare, with a postential to enhance disease progression modeling, personalize treatment plans, detect diseases early, and improve medical imaging analysis. While challenges exist, ongoing research and advancements in computational techniques are addressing these limitations, making GPR an increasingly valuable tool in healthcare. As the field continues to evolve, GPR is poised to revolutionize healthcare by enabling more accurate predictions, better decision-making, and improved patient outcomes. 8.3 GPfit package # Load necessary packages library(kernlab) library(GPfit) library(ggplot2) # Generate simulated data set.seed(123) x &lt;- seq(0, 10, length = 50) y &lt;- sin(x) + rnorm(50, 0, 0.2) df &lt;- data.frame(x = x, y = y) # Fit Gaussian process regression model gpr_model &lt;- gausspr(y ~ x, data = df) y_pred &lt;- predict(gpr_model, x) # Visualize results ggplot(df, aes(x = x, y = y)) + geom_point() + geom_line(aes(y = y_pred), color = &quot;red&quot;) + labs(title = &quot;Gaussian Process Regression&quot;, x = &quot;x&quot;, y = &quot;y&quot;) This R code performs Gaussian process regression (GPR) on simulated data and visualizes the results. Let’s break down each part of the code step-by-step: Load Necessary Packages: library(kernlab) library(GPfit) library(ggplot2) This part loads the required R packages: kernlab for kernel-based machine learning, GPfit for Gaussian process modeling, and ggplot2 for data visualization. Generate Simulated Data: set.seed(123) x &lt;- seq(0, 10, length = 50) y &lt;- sin(x) + rnorm(50, 0, 0.2) df &lt;- data.frame(x = x, y = y) Simulated data is generated for the predictor variable x and the response variable y. The x values are generated as a sequence from 0 to 10 with 50 points. The y values are generated by taking the sine of each x value and adding random noise from a normal distribution with mean 0 and standard deviation 0.2. The data is then combined into a data frame df. Fit Gaussian Process Regression Model: gpr_model &lt;- gausspr(y ~ x, data = df) A Gaussian process regression model is fitted using the gausspr function from the GPfit package. The model specification is y ~ x, indicating that we want to model y as a function of x using Gaussian process regression. Predict Values of y and Visualize Results: y_pred &lt;- predict(gpr_model, x) ggplot(df, aes(x = x, y = y)) + geom_point() + geom_line(aes(y = y_pred), color = &quot;red&quot;) + labs(title = &quot;Gaussian Process Regression&quot;, x = &quot;x&quot;, y = &quot;y&quot;) This part predicts the values of the response variable y_pred for the predictor variable x using the fitted Gaussian process regression model. The predict function is used to make the predictions based on the model gpr_model. The results are then visualized using ggplot2. A scatter plot of the original data points (x and y) is created with blue points (geom_point()). Overlaid on the scatter plot is a red line representing the predictions of the response variable (y_pred) from the Gaussian process regression model (geom_line(aes(y = y_pred), color = \"red\")). 8.4 Bayesian GPR Gaussian process regression can also be implemented in a Bayesian context using Stan. In Bayesian GPR, we assume a prior distribution for the unknown function and then update our beliefs about the function based on the observed data. The prior distribution is typically specified as a Gaussian process with a mean function and covariance function that depend on hyperparameters. The likelihood function for the observed data is also assumed to be Gaussian with a mean function equal to the prior mean function and a covariance function equal to the sum of the prior covariance function and a noise term. The hyperparameters of the prior and likelihood functions are estimated from the data using Markov chain Monte Carlo (MCMC) methods. Here is an example of R code for fitting a Bayesian GPR model using Stan. Let’s break down each part of the code step-by-step: library(rstan) library(ggplot2) # Generate simulated data set.seed(123) x &lt;- seq(0, 10, length = 50) y &lt;- sin(x) + rnorm(50, 0, 0.2) df &lt;- data.frame(x = x, y = y) # Stan model code stan_model_code &lt;- &quot; functions { vector gp_pred_rng(array[] real x2, vector y1, array[] real x1, real sigma_f, real lengthscale_f, real sigma, real jitter) { int N1 = rows(y1); int N2 = size(x2); vector[N2] f2; { matrix[N1, N1] L_K; vector[N1] K_div_y1; matrix[N1, N2] k_x1_x2; matrix[N1, N2] v_pred; vector[N2] f2_mu; matrix[N2, N2] cov_f2; matrix[N1, N1] K; K = gp_exp_quad_cov(x1, sigma_f, lengthscale_f); for (n in 1:N1) K[n, n] = K[n,n] + square(sigma); L_K = cholesky_decompose(K); K_div_y1 = mdivide_left_tri_low(L_K, y1); K_div_y1 = mdivide_right_tri_low(K_div_y1&#39;, L_K)&#39;; k_x1_x2 = gp_exp_quad_cov(x1, x2, sigma_f, lengthscale_f); f2_mu = (k_x1_x2&#39; * K_div_y1); v_pred = mdivide_left_tri_low(L_K, k_x1_x2); cov_f2 = gp_exp_quad_cov(x2, sigma_f, lengthscale_f) - v_pred&#39; * v_pred; f2 = multi_normal_rng(f2_mu, add_diag(cov_f2, rep_vector(jitter, N2))); } return f2; } } data { int&lt;lower=1&gt; N; // number of observations vector[N] x; // univariate covariate vector[N] y; // target variable int&lt;lower=1&gt; N2; // number of test points vector[N2] x2; // univariate test points } transformed data { // Normalize data real xmean = mean(x); real ymean = mean(y); real xsd = sd(x); real ysd = sd(y); array[N] real xn = to_array_1d((x - xmean)/xsd); array[N2] real x2n = to_array_1d((x2 - xmean)/xsd); vector[N] yn = (y - ymean)/ysd; real sigma_intercept = 1; vector[N] zeros = rep_vector(0, N); } parameters { real&lt;lower=0&gt; lengthscale_f; // lengthscale of f real&lt;lower=0&gt; sigma_f; // scale of f real&lt;lower=0&gt; sigman; // noise sigma } model { // covariances and Cholesky decompositions matrix[N, N] K_f = gp_exp_quad_cov(xn, sigma_f, lengthscale_f)+ sigma_intercept^2; matrix[N, N] L_f = cholesky_decompose(add_diag(K_f, sigman^2)); // priors lengthscale_f ~ normal(0, 1); sigma_f ~ normal(0, 1); sigman ~ normal(0, 1); // model yn ~ multi_normal_cholesky(zeros, L_f); } generated quantities { // function scaled back to the original scale vector[N2] f = gp_pred_rng(x2n, yn, xn, sigma_f, lengthscale_f, sigman, 1e-9)*ysd + ymean; real sigma = sigman*ysd; } &quot; # Compile Stan model gpr_stan_model &lt;- stan_model(model_code = stan_model_code) # Prepare data for Stan model stan_data &lt;- list(x=df$x, x2=df$x, y=df$y, N=length(df$x), N2=length(df$x)) # Fit Bayesian GPR model using Stan gpr_fit &lt;- sampling(gpr_stan_model, data = stan_data) f_samples &lt;- extract(gpr_fit, &quot;f&quot;)$f sigma_samples &lt;- extract(gpr_fit, &quot;sigma&quot;)$sigma df %&gt;% mutate(Ef=colMeans(f_samples), sigma=mean(sigma_samples)) %&gt;% ggplot(aes(x=x,y=y))+ geom_point()+ labs(x=&quot;Time (ms)&quot;, y=&quot;Acceleration (g)&quot;)+ geom_line(aes(y=Ef), color=&#39;red&#39;)+ geom_line(aes(y=Ef-2*sigma), color=&#39;red&#39;,linetype=&quot;dashed&quot;)+ geom_line(aes(y=Ef+2*sigma), color=&#39;red&#39;,linetype=&quot;dashed&quot;) Generate Simulated Data: set.seed(123) x &lt;- seq(0, 10, length = 50) y &lt;- sin(x) + rnorm(50, 0, 0.2) df &lt;- data.frame(x = x, y = y) Simulated data is generated for the predictor variable x and the response variable y. The x values are generated as a sequence from 0 to 10 with 50 points. The y values are generated by taking the sine of each x value and adding random noise from a normal distribution with mean 0 and standard deviation 0.2. The data is then combined into a data frame df. Specify Stan Model Code: 1. data block data { int&lt;lower=1&gt; N; // number of observations vector[N] x; // univariate covariate vector[N] y; // target variable int&lt;lower=1&gt; N2; // number of test points vector[N2] x2; // univariate test points } x, y — training inputs and targets. x2 — test inputs where predictions will be made. N, N2 — number of training and test points. 2. transformed data block transformed data { // Normalize data real xmean = mean(x); real ymean = mean(y); real xsd = sd(x); real ysd = sd(y); array[N] real xn = to_array_1d((x - xmean)/xsd); array[N2] real x2n = to_array_1d((x2 - xmean)/xsd); vector[N] yn = (y - ymean)/ysd; real sigma_intercept = 1; vector[N] zeros = rep_vector(0, N); } This normalizes both inputs and outputs for numerical stability: \\(x_n = \\frac{x - \\bar{x}}{sd(x)}\\) \\(y_n = \\frac{y - \\bar{y}}{sd(y)}\\) The test inputs (x2) are normalized using the training mean and sd. sigma_intercept = 1 acts like a constant term in the kernel (bias). zeros is a zero vector used as the GP mean function (mean-zero prior). 3. parameters block parameters { real&lt;lower=0&gt; lengthscale_f; // lengthscale of f real&lt;lower=0&gt; sigma_f; // scale of f real&lt;lower=0&gt; sigman; // noise sigma } lengthscale_f: controls how quickly the function varies with x. sigma_f: output scale (amplitude) of the GP prior. sigman: standard deviation of observation noise. 4. model block model { // covariances and Cholesky decompositions matrix[N, N] K_f = gp_exp_quad_cov(xn, sigma_f, lengthscale_f) + sigma_intercept^2; matrix[N, N] L_f = cholesky_decompose(add_diag(K_f, sigman^2)); // priors lengthscale_f ~ normal(0, 1); sigma_f ~ normal(0, 1); sigman ~ normal(0, 1); // model yn ~ multi_normal_cholesky(zeros, L_f); } 8.4.0.1 Kernel and covariance gp_exp_quad_cov computes the squared exponential (RBF) covariance: \\[ K_{ij} = \\sigma_f^2 \\exp\\!\\left(-\\frac{(x_i - x_j)^2}{2\\,l^2}\\right) \\] Adds a constant term sigma_intercept^2 (bias kernel). Adds observation noise sigman^2 to the diagonal: \\(K = K_f + \\sigma_n^2 I\\). 8.4.0.2 Likelihood yn ~ multi_normal_cholesky(zeros, L_f); means \\(y_n \\sim \\mathcal{N}(0, K)\\). 5. generated quantities block generated quantities { // function scaled back to the original scale vector[N2] f = gp_pred_rng(x2n, yn, xn, sigma_f, lengthscale_f, sigman, 1e-9)*ysd + ymean; real sigma = sigman*ysd; } gp_pred_rng draws samples from the posterior predictive distribution of the GP at the normalized test inputs x2n. Predictions are re-scaled back to the original y scale using: \\[ f = \\text{(predicted normalized value)} \\times sd(y) + \\bar{y} \\] The posterior noise standard deviation sigma is also rescaled to match the original data scale. Summary of the model’s function This code: Normalizes the training data for stable GP computation. Defines a zero-mean GP prior with RBF covariance. Learns the hyperparameters lengthscale_f, sigma_f, and sigman from the data. Generates posterior predictive draws at new points x2, rescaled to the original units. The Stan model code is specified as a character string. The model defines the data, parameters, and the statistical model for Bayesian GPR. It uses a Gaussian process kernel to model the relationship between the predictor variable x and the response variable y. The parameters mu, sigma_f, sigma_n, and eta represent the mean function, the covariance function for the underlying Gaussian process, the noise standard deviation, and the latent function values, respectively. Compile Stan Model: gpr_stan_model &lt;- stan_model(model_code = stan_model_code) The Stan model is compiled using the stan_model function from the rstan package. This step converts the Stan model code into a C++ program that will be used for Bayesian inference. Prepare Data for Stan Model: stan_data &lt;- list(x=df$x, x2=df$x, y=df$y, N=length(df$x), N2=length(df$x)) The data is prepared as a list stan_data with the number of rows N, the predictor variable x, and the response variable y. This data will be used as input to the Stan model during sampling. Fit Bayesian GPR Model using Stan: gpr_fit &lt;- sampling(gpr_stan_model, data = stan_data) The Bayesian GPR model is fitted using the sampling function from rstan. This step performs Markov chain Monte Carlo (MCMC) sampling to estimate the posterior distribution of the model parameters. Extract Posterior Samples of f for Prediction: f_samples &lt;- extract(gpr_fit, &quot;f&quot;)$f sigma_samples &lt;- extract(gpr_fit, &quot;sigma&quot;)$sigma df %&gt;% mutate(Ef=colMeans(f_samples), sigma=mean(sigma_samples)) %&gt;% ggplot(aes(x=x,y=y))+ geom_point()+ labs(x=&quot;Time (ms)&quot;, y=&quot;Acceleration (g)&quot;)+ geom_line(aes(y=Ef), color=&#39;red&#39;)+ geom_line(aes(y=Ef-2*sigma), color=&#39;red&#39;,linetype=&quot;dashed&quot;)+ geom_line(aes(y=Ef+2*sigma), color=&#39;red&#39;,linetype=&quot;dashed&quot;) The extract function is used to extract the posterior samples of the latent function f from the fitted GPR model. These samples will be used to predict new values of f for new values of x. 8.5 References Duvenaud, D. K., Nickisch, H., &amp; Rasmussen, C. E. (2013). Gaussian processes for machine learning: tutorial. In S. Sra, S. Nowozin, &amp; S. J. Wright (Eds.), Optimization for Machine Learning (pp. 133-181). MIT Press. Nguyen, T. D., &amp; Nguyen, T. T. (2018). Multi-task Gaussian process models for biomedical applications. arXiv preprint arXiv:1806.03836. Alaa, A. M., &amp; van der Schaar, M. (2018). Prognostication and risk factors for cystic fibrosis via automated machine learning and Gaussian process regression. Scientific Reports, 8(1), 1-12. Nguyen, T. T., Nguyen, H. T., Nguyen, T. L., &amp; Chetty, G. (2017). Gaussian process regression for predicting 30-day readmission of heart failure patients. Journal of Biomedical Informatics, 71, 199-209. Kazemi, S., &amp; Soltanian-Zadeh, H. (2013). A new Gaussian process regression-based method for segmentation of brain tissues from MRI. Medical Image Analysis, 17(3), 225-234. Gaussian process demonstration with Stan "],["bayesian-gaussian-mixture-models.html", "Chapter 9 Bayesian Gaussian Mixture Models 9.1 Single varaible example 9.2 Example with multiple variable 9.3 Conclusion 9.4 References", " Chapter 9 Bayesian Gaussian Mixture Models A mixture model is a probabilistic framework used to describe populations composed of several unobserved subpopulations or latent components. Rather than assuming that each observation belongs to a single, known class, a mixture model treats membership as uncertain and infers the relative contribution of each latent process to the observed data. This approach is particularly useful when the overall distribution is multimodal or exhibits patterns that cannot be captured by a single parametric form. By representing the data as a weighted sum of simpler distributions, mixture models can approximate complex densities while retaining interpretability. In the Gaussian case, the model assumes that each observation arises from one of ( K ) multivariate normal components, each with its own mean and covariance structure. Let ( y_1, , y_N ^D ) denote the observed data points, where ( D ) is the dimensionality of the space. Each ( y_n ) is modeled as [ p(y_n , {_k, _k}*{k=1}^K) = ^K _k , (y_n _k, _k),] where ( = (_1, , _K) ) are the mixture weights satisfying ( k ) and ( {k=1}^K _k = 1 ). The parameters ( _k ^D ) and ( _k ^{D D} ) denote the mean vector and covariance matrix of component ( k ). The overall likelihood of the dataset is thus [ p(y_{1:N} , {1:K}, {1:K}) = {n=1}^{N} {k=1}^{K} _k , (y_n _k, _k).] The Bayesian formulation introduces priors over all unknown parameters, enabling coherent uncertainty quantification and regularization. In the Stan implementation, the mixture weights () follow a simplex constraint, ensuring they sum to one. The means (_k) are given weakly informative Gaussian priors, [ _k (0, 3^2),] while the covariance structure is encoded through the Cholesky factor of the correlation matrix (L_k). The prior [ L_k _{}(= 4)] induces a correlation prior with concentration parameter 4, slightly favoring correlations near zero without enforcing independence. The full covariance matrix for component (k) can then be reconstructed as (_k = L_k L_k^). Because Stan cannot sample discrete latent variables directly, the component indicators (z_n) are marginalized out analytically. For each data point, the model computes the log-density contribution across all components: [ [k] = (_k) + (y_n _k, L_k L_k^),] and then uses the log-sum-exp transformation to combine them stably: [ += _{k=1}^K ([k]).] This marginalization preserves the full probabilistic structure of the model while allowing the use of gradient-based samplers such as the No-U-Turn Sampler (NUTS) in Stan. The model thus represents a fully Bayesian Gaussian mixture with continuous parameter inference. Each parameter sample corresponds to a plausible mixture configuration, and posterior uncertainty reflects both component overlap and weight variability. After inference, one can compute the posterior probability that observation (y_n) belongs to component (k), [ p(z_n = k y_n, , {1:K}, {1:K}) = ,] yielding soft, probabilistic cluster assignments rather than discrete labels. In practice, Bayesian Gaussian mixtures implemented in this way are both expressive and stable. They avoid the degeneracies common in maximum-likelihood estimation and automatically incorporate regularization through their priors. Moreover, they provide not only point estimates of the latent structure but also credible intervals for the mixture weights and component parameters, allowing for a principled assessment of uncertainty in clustering and density estimation alike. In contrast to the Bayesian formulation, the frequentist approach to Gaussian mixture modeling typically relies on the Expectation–Maximization (EM) algorithm for maximum-likelihood estimation. The EM procedure alternates between computing responsibilities—the conditional probabilities (r_{nk} = p(z_n = k y_n, , , ))—in the E-step, and updating the parameters (_k, _k, _k) to maximize the expected complete-data log-likelihood in the M-step. While efficient and deterministic, this approach produces point estimates that ignore posterior uncertainty and can overfit when data are scarce or components overlap. The Bayesian mixture model, by contrast, integrates over parameter uncertainty and regularizes through its priors, yielding a full posterior distribution rather than a single best-fit configuration. In practice, this means that Bayesian inference captures the uncertainty about both component parameters and cluster assignments—whereas EM simply converges to one mode of the likelihood surface, often sensitive to initialization and prone to singular covariance estimates when clusters are small or poorly separated. Here we first introduce how mixture models are implemented in Bayesian inference. It is noteworthy to take into consideration non-identifiability inherent these models how the non-identifiability can be tempered with principled prior information. Michael Betancourt has a blogpost describing the problems often encountered with gaussian mixture models, specifically the estimation of parameters of a mixture model and identifiability i.e. the problem with labelling mixtures. 9.1 Single varaible example library(dplyr) library(ggplot2) library(ggthemes) N &lt;- 500 # three clusters mu &lt;- c(1, 4, 9) sigma &lt;- c(1.2, 1, 0.8) # probability of each cluster Theta &lt;- c(.3, .5, .3) # Draw which model each belongs to z &lt;- sample(1:3, size = N, prob = Theta, replace = T) # white noise epsilon &lt;- rnorm(N) # Simulate the data using the fact that y ~ normal(mu, sigma) can be # expressed as y = mu + sigma*epsilon for epsilon ~ normal(0, 1) y &lt;- mu[z] + sigma[z]*epsilon data_frame(y= y, z = as.factor(z)) %&gt;% ggplot(aes(x = y, fill = z)) + geom_density(alpha = 0.3) + ggtitle(&quot;Three clusters&quot;) mixture_model&lt;-&#39; // saved as finite_mixture_linear_regression.stan data { int N; vector[N] y; int n_groups; } parameters { vector[n_groups] mu; vector&lt;lower = 0&gt;[n_groups] sigma; simplex[n_groups] Theta; } model { vector[n_groups] contributions; // priors mu ~ normal(0, 10); sigma ~ cauchy(0, 2); Theta ~ dirichlet(rep_vector(2.0, n_groups)); // likelihood for(i in 1:N) { for(k in 1:n_groups) { contributions[k] = log(Theta[k]) + normal_lpdf(y[i] | mu[k], sigma[k]); } target += log_sum_exp(contributions); } }&#39; Data Block - N: Number of observations. - y: Vector of observed responses. - n_groups: Number of mixture components or groups. Parameters Block - mu: Vector of means for each mixture component. - sigma: Vector of standard deviations for each mixture component. - Theta: Vector of mixing proportions, representing the probability of each group. Model Block - Priors: Normal priors are specified for the means mu with a mean of 0 and a standard deviation of 10. Cauchy priors are specified for the standard deviations sigma with a location of 0 and a scale of 2. Dirichlet priors are specified for the mixing proportions Theta with equal concentration parameters of 2.0 for each group. - Likelihood: The likelihood is constructed within a nested loop. For each observation i and each group k, it calculates the log-likelihood of the observation given the mean and standard deviation of that group. These log-likelihoods are stored in the contributions vector. - Log-Sum-Exp Trick: To avoid numerical instability when dealing with small probabilities, the log-sum-exp trick is used. The log_sum_exp function sums up the contributions after exponentiating them. This is done to compute the log-likelihood of the data given the mixture model. - Target: The target is incremented by the log of the sum of exponentiated contributions for each observation. The target is essentially the log-posterior, and the goal of Stan is to maximize it during sampling. library(rstan) options(mc.cores = parallel::detectCores()) fit=stan(model_code=mixture_model, data=list(N= N, y = y, n_groups = 3), iter=3000, warmup=500, chains=3) print(fit) params=extract(fit) #density plots of the posteriors of the mixture means par(mfrow=c(1,3)) plot(density(params$mu[,1]), ylab=&#39;&#39;, xlab=&#39;mu[1]&#39;, main=&#39;&#39;) abline(v=c(8), lty=&#39;dotted&#39;, col=&#39;red&#39;,lwd=2) plot(density(params$mu[,2]), ylab=&#39;&#39;, xlab=&#39;mu[1]&#39;, main=&#39;&#39;) abline(v=c(0), lty=&#39;dotted&#39;, col=&#39;red&#39;,lwd=2) plot(density(params$mu[,3]), ylab=&#39;&#39;, xlab=&#39;mu[1]&#39;, main=&#39;&#39;) abline(v=c(4), lty=&#39;dotted&#39;, col=&#39;red&#39;,lwd=2) Inference for Stan model: 9c40393d28e90e2c335fff95de690860. 3 chains, each with iter=3000; warmup=500; thin=1; post-warmup draws per chain=2500, total post-warmup draws=7500. mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat mu[1] 6.35 3.06 3.76 0.65 1.21 8.96 9.02 9.11 2 21.87 mu[2] 3.72 3.05 3.74 0.59 0.94 1.25 8.96 9.09 2 13.86 mu[3] 4.03 0.00 0.17 3.71 3.92 4.03 4.14 4.36 2575 1.00 sigma[1] 0.85 0.17 0.23 0.62 0.69 0.73 1.02 1.41 2 2.32 sigma[2] 1.01 0.19 0.27 0.64 0.73 1.03 1.19 1.60 2 1.76 sigma[3] 1.13 0.00 0.12 0.92 1.05 1.12 1.20 1.39 3232 1.00 Theta[1] 0.27 0.00 0.04 0.19 0.25 0.27 0.29 0.34 1186 1.02 Theta[2] 0.27 0.00 0.05 0.18 0.24 0.26 0.29 0.40 1553 1.00 Theta[3] 0.47 0.00 0.06 0.32 0.44 0.47 0.51 0.57 1702 1.00 lp__ -1161.00 0.05 2.18 -1166.34 -1162.15 -1160.64 -1159.40 -1157.91 2064 1.00 Samples were drawn using NUTS(diag_e) at Tue Feb 6 13:03:03 2024. For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). 9.2 Example with multiple variable library(MASS) #first cluster mu1=c(0,0,0,0) sigma1=matrix(c(0.2,0,0,0,0,0.2,0,0,0,0,0.1,0,0,0,0,0.1),ncol=4,nrow=4, byrow=TRUE) norm1=mvrnorm(30, mu1, sigma1) #second cluster mu2=c(10,10,10,10) sigma2=sigma1 norm2=mvrnorm(30, mu2, sigma2) #third cluster mu3=c(4,4,4,4) sigma3=sigma1 norm3=mvrnorm(30, mu3, sigma3) norms=rbind(norm1,norm2,norm3) #combine the 3 mixtures together N=90 #total number of data points Dim=4 #number of dimensions y=array(as.vector(norms), dim=c(N,Dim)) mixture_data=list(N=N, D=4, K=3, y=y) as.data.frame(norms) %&gt;% pivot_longer(colnames(as.data.frame(norms)), names_to = &quot;var&quot;, values_to = &quot;value&quot;)%&gt;% ggplot( aes(x=value, color=var)) + geom_density() + ggtitle(&quot;Three clusters on four variables&quot;) mixture_model&lt;-&#39; data { int D; //number of dimensions int K; //number of gaussians int N; //number of data vector[D] y[N]; //data } parameters { simplex[K] theta; //mixing proportions ordered[D] mu[K]; //mixture component means cholesky_factor_corr[D] L[K]; //cholesky factor of covariance } model { real ps[K]; for(k in 1:K){ mu[k] ~ normal(0,3); L[k] ~ lkj_corr_cholesky(4); } for (n in 1:N){ for (k in 1:K){ ps[k] = log(theta[k])+multi_normal_cholesky_lpdf(y[n] | mu[k], L[k]); } target += log_sum_exp(ps); } }&#39; Data Block - D: Number of dimensions. - K: Number of Gaussian components. - N: Number of data points. - y: An array of vectors, each representing a data point in D dimensions. Parameters Block - theta: Mixing proportions. It is a simplex, ensuring that the proportions sum to 1. - mu: Mixture component means. These are ordered variables. - L: Cholesky factors of the covariance matrices for each component. Model Block - Priors: Priors are specified for the means mu and the Cholesky factors L. Each mean is drawn from a normal distribution with a mean of 0 and a standard deviation of 3. The Cholesky factor is drawn from a LKJ correlation distribution with shape parameter 4. Log-Probability Calculation: For each data point n and each component k, the log-probability ps[k] is calculated. This log-probability is the logarithm of the product of the mixing proportion and the multivariate normal density of the data point under the k-th component. Target Increment: The target is incremented by the logarithm of the sum of exponentiated log-probabilities ps. This step ensures that the model assigns higher probability to data points that are well-explained by one of the Gaussian components. fit=stan(model_code=mixture_model, data=mixture_data, iter=3000, warmup=1000, chains=1) print(fit) Inference for Stan model: f913dae683b9f29657b0863fec348d71. 1 chains, each with iter=3000; warmup=1000; thin=1; post-warmup draws per chain=2000, total post-warmup draws=2000. mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat theta[1] 0.33 0.00 0.05 0.24 0.30 0.33 0.37 0.43 2120 1.00 theta[2] 0.33 0.00 0.05 0.24 0.30 0.33 0.36 0.44 1740 1.00 theta[3] 0.33 0.00 0.05 0.24 0.30 0.33 0.37 0.43 1854 1.00 mu[1,1] 3.80 0.00 0.11 3.55 3.74 3.80 3.86 3.99 632 1.01 mu[1,2] 3.91 0.01 0.12 3.67 3.84 3.90 3.98 4.15 236 1.00 mu[1,3] 4.02 0.01 0.13 3.78 3.92 4.02 4.11 4.29 344 1.00 mu[1,4] 4.09 0.01 0.15 3.82 3.99 4.09 4.20 4.39 259 1.00 mu[2,1] 9.77 0.01 0.12 9.51 9.70 9.79 9.86 9.98 405 1.00 mu[2,2] 9.96 0.00 0.11 9.74 9.90 9.96 10.03 10.20 794 1.00 mu[2,3] 10.09 0.00 0.11 9.91 10.01 10.07 10.15 10.33 518 1.00 mu[2,4] 10.18 0.01 0.12 9.97 10.09 10.17 10.25 10.43 498 1.00 mu[3,1] -0.22 0.01 0.13 -0.49 -0.30 -0.21 -0.14 0.01 409 1.01 mu[3,2] -0.10 0.01 0.13 -0.37 -0.17 -0.09 -0.02 0.17 218 1.00 mu[3,3] 0.07 0.01 0.13 -0.17 0.00 0.07 0.15 0.36 81 1.00 mu[3,4] 0.16 0.01 0.12 -0.03 0.08 0.15 0.23 0.42 541 1.00 For each parameter, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat=1). params=extract(fit) #density plots of the posteriors of the mixture means par(mfrow=c(1,3)) plot(density(params$mu[,1,1]), ylab=&#39;&#39;, xlab=&#39;mu[1]&#39;, main=&#39;&#39;) lines(density(params$mu[,1,2]), col=rgb(0,0,0,0.7)) lines(density(params$mu[,1,3]), col=rgb(0,0,0,0.4)) lines(density(params$mu[,1,4]), col=rgb(0,0,0,0.1)) abline(v=c(4), lty=&#39;dotted&#39;, col=&#39;red&#39;,lwd=2) plot(density(params$mu[,2,1]), ylab=&#39;&#39;, xlab=&#39;mu[2]&#39;, main=&#39;&#39;) lines(density(params$mu[,2,2]), col=rgb(0,0,0,0.7)) lines(density(params$mu[,2,3]), col=rgb(0,0,0,0.4)) lines(density(params$mu[,2,4]), col=rgb(0,0,0,0.1)) abline(v=c(10), lty=&#39;dotted&#39;, col=&#39;red&#39;,lwd=2) plot(density(params$mu[,3,1]), ylab=&#39;&#39;, xlab=&#39;mu[3]&#39;, main=&#39;&#39;) lines(density(params$mu[,3,2]), col=rgb(0,0,0,0.7)) lines(density(params$mu[,3,3]), col=rgb(0,0,0,0.4)) lines(density(params$mu[,3,4]), col=rgb(0,0,0,0.1)) abline(v=c(0), lty=&#39;dotted&#39;, col=&#39;red&#39;,lwd=2) 9.3 Conclusion Bayesian mixture models offer several advantages in statistical modeling. Their inherent flexibility makes them well-suited for diverse tasks such as clustering, data compression, outlier detection, and generative classification. The Bayesian framework’s ability to incorporate prior knowledge enhances model accuracy, especially when informative prior information is available. Moreover, these models effectively handle unobserved heterogeneity by integrating multiple data generating processes, proving valuable when data alone may not fully identify underlying patterns. The stability provided by Bayesian estimation ensures reliable posterior distributions, reducing sensitivity to issues like singularities, over-fitting, and violated identification criteria. Bayesian mixture models also facilitate the examination of the posterior distribution of the number of classes, offering insights into the underlying class structure of the data. However, the use of Bayesian mixture models comes with certain limitations. Applying these models demands a high level of statistical expertise to appropriately specify priors and ensure correct model formulation, presenting a challenge for practitioners lacking a strong background in Bayesian statistics. The complexity of posterior inference is compounded by label switching, a phenomenon that complicates the interpretation of results. Bayesian nonparametric mixture models, in particular, may suffer from inconsistency in estimating the number of clusters, impacting their performance in clustering applications. Additionally, model fitting challenges arise, and careful evaluation of inaccuracies in predictions and comparison with alternative models are essential to address potential shortcomings. In this post, we learned to fit mixture models using Stan. We saw how to evaluate model fit using the usual prior and posterior predictive checks, and to investigate parameter recovery. Such mixture models are notoriously difficult to fit, but they have a lot of potential in cognitive science applications, especially in developing computational models of different kinds of cognitive processes. The reader interested in a deeper understanding of potential challanges in the process can refer to Betancourt discussion of identification problems in Bayesian mixture models in a case study. 9.4 References Finite mixture models in Stan Multivariate Gaussian Mixture Model done properly Finite Mixtures Identifying Bayesian Mixture Models Mixture models Bayesian Density Estimation (Finite Mixture Model) Bayesian mixture models (in)consistency for the number of clusters Advantages of a Bayesian Approach for Examining Class Structure in Finite Mixture Models "],["bayesian-canonical-correlation-analysis-in-stan.html", "Chapter 10 Bayesian Canonical Correlation Analysis in Stan 10.1 Introduction 10.2 Conclusion 10.3 References", " Chapter 10 Bayesian Canonical Correlation Analysis in Stan 10.1 Introduction Canonical Correlation Analysis (CCA) is a multivariate statistical method that identifies and quantifies the relationships between two sets of variables by finding linear combinations (canonical variates) that maximize their correlation. Unlike standard correlation analysis, which examines relationships between individual variables, CCA explores the joint structure of two variable sets, such as psychological test scores and academic performance metrics. In a Bayesian framework, CCA incorporates priors on the canonical coefficients, allowing for uncertainty quantification and regularization, which can improve estimation in small or noisy datasets. In this post, we will implement a Bayesian CCA model in Stan, focusing on estimating canonical coefficients and their correlations, and compare it to frequentist CCA. We will use synthetic data to illustrate the approach and highlight the benefits of Bayesian inference, such as full posterior distributions for uncertainty quantification. 10.1.1 Canonical Correlation Analysis CCA seeks pairs of linear combinations (canonical variates) from two sets of variables, \\(X \\in \\mathbbr^{n \\times p}\\) (with \\(p\\) variables) and \\(Y \\in \\mathbbr^{n \\times q}\\) (with \\(q\\) variables), such that the correlation between the variates is maximized. Mathematically, for variables \\(X\\) and \\(Y\\), CCA finds vectors \\(a_i \\in \\mathbbr^p\\) and \\(b_i \\in \\mathbbr^q\\) such that the canonical variates \\(U_i = X a_i\\) and \\(V_i = Y b_i\\) have maximum correlation, i.e., \\[ (a_i, b_i) = \\arg\\max_{a, b} \\; \\text{corr}(X a, Y b), \\] subject to the constraints that the variates are uncorrelated with previous pairs, \\[ \\text{cov}(X a_i, X a_j) = 0, \\quad \\text{cov}(Y b_i, Y b_j) = 0 \\quad \\text{for } i \\neq j, \\] and have unit variance: \\[ \\text{var}(X a_i) = \\text{var}(Y b_i) = 1. \\] In a Bayesian framework, we place priors on the canonical coefficients (\\(a_i\\), \\(b_i\\)) and model the joint distribution of \\(X\\) and \\(Y\\), allowing for flexible regularization and uncertainty estimation. In the Bayesian CCA model, we assume that the observed variables \\(X\\) and \\(Y\\) are generated from latent canonical variates with a shared structure. Specifically, let \\[ U = X A, \\quad V = Y B, \\] where \\(A \\in \\mathbbr^{p \\times k}\\) and \\(B \\in \\mathbbr^{q \\times k}\\) are matrices of canonical weights, and \\(k \\leq \\min(p, q)\\) is the number of canonical components. We place normal priors on the canonical coefficients: \\[ A_{ij} \\sim \\mathcal{N}(0, \\sigma^2_A), \\quad B_{ij} \\sim \\mathcal{N}(0, \\sigma^2_B), \\] and use a multivariate normal likelihood to model the joint distribution of \\(X\\) and \\(Y\\): \\[ \\begin{bmatrix} X \\\\ Y \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\Sigma \\right), \\] with the cross-covariance structure implied by the canonical correlation: \\[ \\Sigma = \\begin{bmatrix} \\Sigma_{XX} &amp; \\Sigma_{XY} \\\\ \\Sigma_{YX} &amp; \\Sigma_{YY} \\end{bmatrix}, \\quad \\Sigma_{XY} = A C B^\\top, \\] where \\(C\\) is a diagonal matrix of canonical correlations. The model estimates the canonical coefficients and their correlations, with hyperpriors to regularize the variance parameters: \\[ \\sigma_A, \\sigma_B \\sim \\text{Half-Cauchy}(0, \\tau), \\] for some scale \\(\\tau &gt; 0\\). We start by generating synthetic data to demonstrate the Bayesian CCA model. # Improved Bayesian Canonical Correlation Analysis # Load required libraries library(tidyverse) library(rstan) library(rsample) library(bayesplot) library(ggplot2) # Set Stan options for better performance options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) # Data Generation with more realistic structure set.seed(123) n &lt;- 1000 # number of observations p &lt;- 4 # number of X variables (increased for more complexity) q &lt;- 3 # number of Y variables (increased for more complexity) # Generate multiple latent canonical variates for richer structure u1 &lt;- rnorm(n, 0, 1) u2 &lt;- rnorm(n, 0, 0.5) # Secondary canonical variate # Create correlated canonical variates for Y v1 &lt;- 0.8 * u1 + rnorm(n, 0, sqrt(1 - 0.8^2)) # Strong correlation v2 &lt;- 0.5 * u2 + rnorm(n, 0, sqrt(1 - 0.5^2)) # Moderate correlation # Generate X variables with varying loadings on canonical variates X1 &lt;- 0.9 * u1 + 0.3 * u2 + rnorm(n, 0, 0.5) X2 &lt;- 0.7 * u1 + 0.5 * u2 + rnorm(n, 0, 0.6) X3 &lt;- 0.5 * u1 + 0.8 * u2 + rnorm(n, 0, 0.4) X4 &lt;- 0.3 * u1 + 0.2 * u2 + rnorm(n, 0, 0.8) # Generate Y variables with varying loadings Y1 &lt;- 0.8 * v1 + 0.2 * v2 + rnorm(n, 0, 0.4) Y2 &lt;- 0.6 * v1 + 0.7 * v2 + rnorm(n, 0, 0.5) Y3 &lt;- 0.4 * v1 + 0.9 * v2 + rnorm(n, 0, 0.3) # Create properly structured data frame X_matrix &lt;- cbind(X1, X2, X3, X4) Y_matrix &lt;- cbind(Y1, Y2, Y3) data &lt;- data.frame( X_matrix, Y_matrix ) # Rename columns for clarity colnames(data) &lt;- c(paste0(&quot;X&quot;, 1:p), paste0(&quot;Y&quot;, 1:q)) cat(&quot;Data structure:\\n&quot;) str(data) cat(&quot;\\nFirst few rows:\\n&quot;) head(data) # Data splitting set.seed(42) data_split &lt;- initial_split(data, prop = 0.7) train_data &lt;- training(data_split) test_data &lt;- testing(data_split) # Extract X and Y matrices correctly X_train &lt;- as.matrix(train_data[, 1:p]) Y_train &lt;- as.matrix(train_data[, (p+1):(p+q)]) X_test &lt;- as.matrix(test_data[, 1:p]) Y_test &lt;- as.matrix(test_data[, (p+1):(p+q)]) 10.1.2 Stan Model Below is the Stan code for a Bayesian CCA model, which estimates the canonical coefficients for one pair of canonical variates. stan_model &lt;- &quot; data { int&lt;lower=0&gt; N; // number of observations int&lt;lower=0&gt; p; // number of X variables int&lt;lower=0&gt; q; // number of Y variables matrix[N, p] X; // X variables (standardized) matrix[N, q] Y; // Y variables (standardized) } parameters { vector[p] a; // canonical coefficients for X vector[q] b; // canonical coefficients for Y real&lt;lower=0&gt; sigma_u; // SD for canonical variate u real&lt;lower=0&gt; sigma_v; // SD for canonical variate v real&lt;lower=-1, upper=1&gt; rho; // canonical correlation // Hierarchical priors for better regularization real&lt;lower=0&gt; tau_a; // hierarchical scale for a real&lt;lower=0&gt; tau_b; // hierarchical scale for b } transformed parameters { vector[N] u; // canonical variate for X vector[N] v; // canonical variate for Y // Compute canonical variates u = X * a; v = Y * b; // Standardize canonical variates for identifiability real u_mean = mean(u); real v_mean = mean(v); real u_sd = sd(u); real v_sd = sd(v); } model { // Hierarchical priors tau_a ~ cauchy(0, 1); tau_b ~ cauchy(0, 1); // Priors for canonical coefficients with hierarchical regularization a ~ normal(0, tau_a); b ~ normal(0, tau_b); // Priors for variance parameters sigma_u ~ cauchy(0, 1); sigma_v ~ cauchy(0, 1); // Prior for canonical correlation rho ~ beta(2, 2); // Slightly informative prior favoring moderate correlations // Likelihood for canonical variates u ~ normal(0, sigma_u); v ~ normal(rho * u, sigma_v * sqrt(1 - rho^2)); // Constraint for identifiability (optional - can help with convergence) target += -0.5 * dot_self(a) - 0.5 * dot_self(b); } generated quantities { // Posterior predictions vector[N] u_pred; vector[N] v_pred; matrix[N, p] X_pred; matrix[N, q] Y_pred; // Log likelihood for model comparison real log_lik = 0; // Generate predictions for (n in 1:N) { u_pred[n] = normal_rng(0, sigma_u); v_pred[n] = normal_rng(rho * u_pred[n], sigma_v * sqrt(1 - rho^2)); } // Predicted X and Y based on canonical variates X_pred = rep_matrix(u_pred, p); Y_pred = rep_matrix(v_pred, q); // Calculate log likelihood for (n in 1:N) { log_lik += normal_lpdf(u[n] | 0, sigma_u); log_lik += normal_lpdf(v[n] | rho * u[n], sigma_v * sqrt(1 - rho^2)); } } &quot; This Stan model implements Bayesian Canonical Correlation Analysis by first defining the data structure with N observations, p X variables, and q Y variables stored in matrices X and Y. The parameters section declares the key unknowns: canonical coefficient vectors a and b that will create linear combinations of the original variables, standard deviations sigma_u and sigma_v for the canonical variates, the canonical correlation rho (constrained between -1 and 1), and hierarchical regularization parameters tau_a and tau_b. In the transformed parameters block, the model computes the canonical variates u = X * a and v = Y * b, which are the linear combinations that maximize correlation between the two sets of variables, along with their means and standard deviations for standardization purposes. The model block establishes the probabilistic relationships through priors and likelihood functions. Hierarchical priors are set with tau_a ~ cauchy(0, 1) and tau_b ~ cauchy(0, 1), which then inform the canonical coefficients a ~ normal(0, tau_a) and b ~ normal(0, tau_b), providing adaptive regularization. The variance parameters sigma_u and sigma_v receive Cauchy priors, while rho gets a Beta(2,2) prior favoring moderate correlations. The core likelihood assumes u ~ normal(0, sigma_u) and v ~ normal(rho * u, sigma_v * sqrt(1 - rho^2)), establishing that the canonical variates follow a bivariate normal distribution with correlation rho. The model adds an identifiability constraint target += -0.5 * dot_self(a) - 0.5 * dot_self(b) to prevent scaling issues. Finally, the generated quantities block creates posterior predictions by sampling new canonical variates u_pred and v_pred from their respective distributions, generates predicted data matrices X_pred and Y_pred, and computes the log-likelihood log_lik for model comparison purposes. 10.1.2.1 Fitting the Model Prepare the data and fit the model using rstan. X_scaled &lt;- scale(X_train) Y_scaled &lt;- scale(Y_train) stan_data &lt;- list( N = nrow(X_scaled), p = ncol(X_scaled), q = ncol(Y_scaled), X = X_scaled, Y = Y_scaled ) cat(&quot;Stan data structure:\\n&quot;) str(stan_data) # Compile and fit the model cat(&quot;\\nCompiling and fitting Stan model...\\n&quot;) fit_cca &lt;- stan( model_code = stan_model, data = stan_data, iter = 2000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = 0.95, max_treedepth = 12), verbose = TRUE ) # Model diagnostics cat(&quot;\\nModel Summary:\\n&quot;) print(fit_cca, pars = c(&quot;rho&quot;, &quot;sigma_u&quot;, &quot;sigma_v&quot;, &quot;tau_a&quot;, &quot;tau_b&quot;)) # Check convergence diagnostics cat(&quot;\\nConvergence Diagnostics:\\n&quot;) cat(&quot;R-hat values:\\n&quot;) rhat_values &lt;- rhat(fit_cca) print(rhat_values[rhat_values &gt; 1.1]) cat(&quot;\\nEffective sample sizes:\\n&quot;) eff_samples &lt;- neff_ratio(fit_cca) print(eff_samples[eff_samples &lt; 0.1]) # Extract posterior samples posterior_samples &lt;- extract(fit_cca) # Visualization of results if (require(bayesplot)) { # Trace plots for key parameters mcmc_trace(fit_cca, pars = c(&quot;rho&quot;, &quot;sigma_u&quot;, &quot;sigma_v&quot;)) # Posterior density plots mcmc_dens(fit_cca, pars = c(&quot;rho&quot;, &quot;sigma_u&quot;, &quot;sigma_v&quot;)) # Posterior intervals for canonical coefficients mcmc_intervals(fit_cca, pars = paste0(&quot;a[&quot;, 1:p, &quot;]&quot;)) mcmc_intervals(fit_cca, pars = paste0(&quot;b[&quot;, 1:q, &quot;]&quot;)) } # Summary statistics cat(&quot;\\nPosterior Summary for Key Parameters:\\n&quot;) cat(&quot;Canonical Correlation (rho):\\n&quot;) cat(&quot;Mean:&quot;, mean(posterior_samples$rho), &quot;\\n&quot;) cat(&quot;95% CI:&quot;, quantile(posterior_samples$rho, c(0.025, 0.975)), &quot;\\n&quot;) cat(&quot;\\nCanonical Coefficients for X (a):\\n&quot;) print(colMeans(posterior_samples$a)) cat(&quot;\\nCanonical Coefficients for Y (b):\\n&quot;) print(colMeans(posterior_samples$b)) 10.1.3 Model validation on test set X_test_scaled &lt;- scale(X_test, center = attr(X_scaled, &quot;scaled:center&quot;), scale = attr(X_scaled, &quot;scaled:scale&quot;)) Y_test_scaled &lt;- scale(Y_test, center = attr(Y_scaled, &quot;scaled:center&quot;), scale = attr(Y_scaled, &quot;scaled:scale&quot;)) # Calculate canonical variates for test set a_mean &lt;- colMeans(posterior_samples$a) b_mean &lt;- colMeans(posterior_samples$b) u_test &lt;- X_test_scaled %*% a_mean v_test &lt;- Y_test_scaled %*% b_mean test_correlation &lt;- cor(u_test, v_test) 10.2 Conclusion In this post, we explored Bayesian Canonical Correlation Analysis, a method to uncover relationships between two sets of variables through maximally correlated linear combinations. By implementing a Bayesian CCA model in Stan, we demonstrated how priors on canonical coefficients and hyperpriors on variance parameters enable robust estimation and uncertainty quantification. Compared to frequentist CCA, the Bayesian approach provides full posterior distributions, eliminating the need for bootstrapping to estimate uncertainty. This makes it particularly useful for small or noisy datasets. The Stan implementation allows for flexible extensions, such as modeling multiple canonical pairs or incorporating hierarchical structures. 10.3 References Stan User’s Guide on Multivariate Models Gelman, A., &amp; Hill, J. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models Krzanowski, W. J. (2000). Principles of Multivariate Analysis "],["bayesian-seasonal-decomposition-in-stan-and-rstan.html", "Chapter 11 Bayesian Seasonal Decomposition in Stan and RStan 11.1 Simulating Seasonal Time Series Data* 11.2 Bayesian Seasonal Decomposition Model in Stan 11.3 Fitting the Model in R 11.4 Extracting and Visualizing Components 11.5 Extensions and Applications 11.6 Conclusion", " Chapter 11 Bayesian Seasonal Decomposition in Stan and RStan Understanding the components that make up a time series is a critical first step in analysis and forecasting. Classical seasonal decomposition techniques, such as STL or X-11, offer ways to separate a series into trend, seasonal, and irregular components. However, these methods are often deterministic and lack a coherent probabilistic framework for uncertainty quantification. In contrast, Bayesian seasonal decomposition allows us to model uncertainty in each component explicitly. In this post, we demonstrate how to construct a Bayesian structural time series model in Stan, decompose a seasonal time series, and obtain posterior distributions for each latent component. 11.1 Simulating Seasonal Time Series Data* We first simulate a synthetic time series with additive trend, seasonality, and noise: set.seed(123) n &lt;- 120 t &lt;- 1:n trend &lt;- 0.05 * t seasonal_period &lt;- 12 seasonal &lt;- rep(sin(2 * pi * (1:seasonal_period) / seasonal_period), length.out = n) noise &lt;- rnorm(n, 0, 0.5) y &lt;- trend + seasonal + noise ts.plot(y, main = &quot;Simulated Time Series with Trend and Seasonality&quot;) 11.2 Bayesian Seasonal Decomposition Model in Stan We use an additive structural model: \\[ y_t = \\mu_t + \\gamma_t + \\epsilon_t \\] \\(\\mu_t\\): Local level (trend) \\(\\gamma_t\\): Seasonal component \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\): Irregular component We impose a local level model for the trend: \\[ \\mu_t = \\mu_{t-1} + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, \\sigma_{\\mu}^2) \\] And we model seasonality with a constraint that its sum over one period equals zero (to ensure identifiability): stan_model_1 &lt;-&#39;data { int&lt;lower=2&gt; N; int&lt;lower=2&gt; s; // seasonal period vector[N] y; } parameters { vector[N] mu; // trend vector[s] season_raw; // raw seasonal effects real&lt;lower=0&gt; sigma; real&lt;lower=0&gt; sigma_mu; real&lt;lower=0&gt; sigma_season; } transformed parameters { vector[N] season; vector[s] season_clean; // Center seasonal component to sum to zero season_clean = season_raw - mean(season_raw); for (t in 1:N) season[t] = season_clean[1 + ((t - 1) % s)]; } model { mu[2:N] ~ normal(mu[1:(N - 1)], sigma_mu); season_raw ~ normal(0, sigma_season); y ~ normal(mu + season, sigma); }&#39; 11.3 Fitting the Model in R We fit this model using RStan: library(rstan) data_list &lt;- list( N = length(y), s = seasonal_period, y = y ) fit &lt;- stan( model_code = stan_model_1, data = data_list, chains = 4, iter = 2000, warmup = 1000, seed = 123 ) ## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. ## Running the chains for more iterations may help. See ## https://mc-stan.org/misc/warnings.html#bulk-ess print(fit, pars = c(&quot;sigma&quot;, &quot;sigma_mu&quot;, &quot;sigma_season&quot;)) ## Inference for Stan model: anon_model. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## sigma 0.42 0 0.04 0.36 0.40 0.42 0.44 0.50 3026 1.00 ## sigma_mu 0.18 0 0.04 0.13 0.16 0.18 0.20 0.26 319 1.01 ## sigma_season 0.81 0 0.22 0.50 0.66 0.77 0.91 1.33 2846 1.00 ## ## Samples were drawn using NUTS(diag_e) at Thu Oct 16 08:57:50 2025. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 11.4 Extracting and Visualizing Components We now extract the posterior estimates and visualize the trend and seasonal components: posterior &lt;- extract(fit) mu_hat &lt;- apply(posterior$mu, 2, mean) season_hat &lt;- apply(posterior$season, 2, mean) par(mfrow = c(3, 1), mar = c(4, 4, 2, 1)) plot(y, type = &#39;l&#39;, main = &quot;Observed Time Series&quot;, ylab = &quot;y&quot;) plot(mu_hat, type = &#39;l&#39;, col = &quot;blue&quot;, main = &quot;Estimated Trend (mu)&quot;, ylab = &quot;mu_t&quot;) plot(season_hat, type = &#39;l&#39;, col = &quot;darkgreen&quot;, main = &quot;Estimated Seasonal Component&quot;, ylab = &quot;season_t&quot;) This visual decomposition gives insight into how much of the variation is driven by the underlying trend and how much by recurring seasonal patterns. The Bayesian framework also allows for full posterior uncertainty quantification, which can be visualized with credible intervals around each component. 11.5 Extensions and Applications Bayesian structural time series models can be extended in numerous ways. One may introduce regression components to account for covariates (leading to models like BSTS), or allow the trend to include a slope (i.e., a local linear trend). Multivariate or hierarchical seasonal decomposition is also possible and particularly useful for grouped or panel time series. 11.6 Conclusion Bayesian seasonal decomposition offers a principled approach to breaking down time series into interpretable components while rigorously accounting for uncertainty. Compared to classical methods, the Bayesian formulation in Stan provides flexibility, interpretability, and extensibility. Whether used for exploratory analysis or as a preprocessing step for forecasting, structural time series decomposition is a valuable tool in the Bayesian modeler’s toolkit. "],["bayesian-exponential-smoothing-and-holt-winters-models-in-stan-and-rstan.html", "Chapter 12 Bayesian Exponential Smoothing and Holt-Winters Models in Stan and RStan 12.1 Bayesian Simple Exponential Smoothing (SES) 12.2 Bayesian Holt-Winters Models 12.3 Conclusion", " Chapter 12 Bayesian Exponential Smoothing and Holt-Winters Models in Stan and RStan Time series data often reflect patterns of level, trend, and seasonality. While ARIMA-style models are foundational, exponential smoothing techniques are especially appealing when forecasting with recent observations is more informative than distant ones. In this post, we explore how to model these systems in a Bayesian framework using Stan and RStan, bringing full probabilistic inference and uncertainty quantification to these well-known methods. We walk through the following: Bayesian Simple Exponential Smoothing (SES) Bayesian Holt-Winters Models Without seasonality (trend only) With seasonality (additive seasonal component) 12.1 Bayesian Simple Exponential Smoothing (SES) Simple Exponential Smoothing assumes a local level that evolves recursively: \\[ \\ell_t = \\alpha y_{t-1} + (1 - \\alpha) \\ell_{t-1} \\\\ y_t \\sim \\mathcal{N}(\\ell_t, \\sigma^2) \\] To simulate a series under this model: set.seed(1) n &lt;- 100 alpha &lt;- 0.3 sigma &lt;- 1 l &lt;- numeric(n) y &lt;- numeric(n) l[1] &lt;- 10 y[1] &lt;- l[1] + rnorm(1, 0, sigma) for (t in 2:n) { l[t] &lt;- alpha * y[t - 1] + (1 - alpha) * l[t - 1] y[t] &lt;- l[t] + rnorm(1, 0, sigma) } ts.plot(y, main = &quot;Simulated SES Time Series&quot;) 12.1.1 Stan Model data { int&lt;lower=2&gt; N; vector[N] y; } parameters { real&lt;lower=0, upper=1&gt; alpha; real&lt;lower=0&gt; sigma; vector[N] l; // local level } model { l[1] ~ normal(y[1], sigma); for (t in 2:N) l[t] ~ normal(alpha * y[t-1] + (1 - alpha) * l[t-1], sigma); y ~ normal(l, sigma); } 12.1.2 Fitting in R library(rstan) data_list &lt;- list(N = length(y), y = y) fit &lt;- stan(model_code = stan_model_1, data = data_list, chains = 4, iter = 2000, warmup = 1000) print(fit, pars = c(&quot;alpha&quot;, &quot;sigma&quot;)) 12.1.3 Visualization post &lt;- extract(fit) l_hat &lt;- apply(post$l, 2, mean) l_ci_lower &lt;- apply(post$l, 2, quantile, 0.025) l_ci_upper &lt;- apply(post$l, 2, quantile, 0.975) df &lt;- data.frame( time = 1:n, observed = y, true_level = l, posterior_mean = l_hat, lower_95 = l_ci_lower, upper_95 = l_ci_upper ) ggplot(df, aes(x = time)) + geom_line(aes(y = observed), linetype = &quot;dashed&quot;, color = &quot;black&quot;) + geom_line(aes(y = true_level), color = &quot;blue&quot;, alpha = 0.5) + geom_line(aes(y = posterior_mean), color = &quot;red&quot;) + geom_ribbon(aes(ymin = lower_95, ymax = upper_95), fill = &quot;red&quot;, alpha = 0.2) + labs(title = &quot;Bayesian SES: Posterior Level Estimates vs True Level&quot;, y = &quot;Value&quot;) + theme_minimal() 12.2 Bayesian Holt-Winters Models 12.2.1 Without Seasonality (Additive Trend) Holt’s method models both level and trend: \\[ \\begin{aligned} \\ell_t &amp;= \\alpha y_{t-1} + (1 - \\alpha)(\\ell_{t-1} + b_{t-1}) \\\\ b_t &amp;= \\beta(\\ell_t - \\ell_{t-1}) + (1 - \\beta)b_{t-1} \\\\ y_t &amp;\\sim \\mathcal{N}(\\ell_t + b_t, \\sigma^2) \\end{aligned} \\] set.seed(2) n &lt;- 100 alpha &lt;- 0.3 beta &lt;- 0.1 sigma &lt;- 1 l &lt;- numeric(n) b &lt;- numeric(n) y &lt;- numeric(n) l[1] &lt;- 5 b[1] &lt;- 0.5 y[1] &lt;- l[1] + b[1] + rnorm(1, 0, sigma) for (t in 2:n) { l[t] &lt;- alpha * y[t - 1] + (1 - alpha) * (l[t - 1] + b[t - 1]) b[t] &lt;- beta * (l[t] - l[t - 1]) + (1 - beta) * b[t - 1] y[t] &lt;- l[t] + b[t] + rnorm(1, 0, sigma) } ts.plot(y, main = &quot;Simulated Holt-Winters Time Series&quot;) 12.2.2 Stan Model data { int&lt;lower=2&gt; N; vector[N] y; } parameters { real&lt;lower=0, upper=1&gt; alpha; real&lt;lower=0, upper=1&gt; beta; real&lt;lower=0&gt; sigma; vector[N] l; vector[N] b; } model { l[1] ~ normal(y[1], sigma); b[1] ~ normal(0, sigma); for (t in 2:N) { l[t] ~ normal(alpha * y[t-1] + (1 - alpha) * (l[t-1] + b[t-1]), sigma); b[t] ~ normal(beta * (l[t] - l[t-1]) + (1 - beta) * b[t-1], sigma); } y ~ normal(l + b, sigma); } 12.2.3 Visualization post &lt;- extract(fit) l_hat &lt;- apply(post$l, 2, mean) b_hat &lt;- apply(post$b, 2, mean) y_hat &lt;- l_hat + b_hat l_ci_lower &lt;- apply(post$l, 2, quantile, 0.025) l_ci_upper &lt;- apply(post$l, 2, quantile, 0.975) df &lt;- data.frame( time = 1:n, observed = y, predicted = y_hat, level_mean = l_hat, level_lower = l_ci_lower, level_upper = l_ci_upper ) ggplot(df, aes(x = time)) + geom_line(aes(y = observed), linetype = &quot;dashed&quot;, color = &quot;black&quot;) + geom_line(aes(y = predicted), color = &quot;red&quot;) + geom_ribbon(aes(ymin = level_lower, ymax = level_upper), fill = &quot;red&quot;, alpha = 0.2) + labs(title = &quot;Bayesian Holt-Winters: Posterior Level &amp; Trend Estimates&quot;, y = &quot;Value&quot;) + theme_minimal() 12.2.4 With Seasonality (Additive Seasonal Component) The full Holt-Winters seasonal model captures all three components: \\[ \\begin{aligned} \\ell_t &amp;= \\alpha(y_{t-1} - s_{t-m}) + (1 - \\alpha)(\\ell_{t-1} + b_{t-1}) \\\\ b_t &amp;= \\beta(\\ell_t - \\ell_{t-1}) + (1 - \\beta)b_{t-1} \\\\ s_t &amp;= \\gamma(y_{t-1} - \\ell_{t-1} - b_{t-1}) + (1 - \\gamma)s_{t-m} \\\\ y_t &amp;\\sim \\mathcal{N}(\\ell_t + b_t + s_{t-m}, \\sigma^2) \\end{aligned} \\] set.seed(3) n &lt;- 120 m &lt;- 12 alpha &lt;- 0.3; beta &lt;- 0.1; gamma &lt;- 0.2; sigma &lt;- 1 l &lt;- numeric(n); b &lt;- numeric(n); s &lt;- numeric(n); y &lt;- numeric(n) l[1] &lt;- 10; b[1] &lt;- 0.5; s[1:m] &lt;- sin(2 * pi * (1:m) / m) y[1] &lt;- l[1] + b[1] + s[1] + rnorm(1, 0, sigma) for (t in 2:n) { if (t &gt; m) { l[t] &lt;- alpha * (y[t - 1] - s[t - m]) + (1 - alpha) * (l[t - 1] + b[t - 1]) b[t] &lt;- beta * (l[t] - l[t - 1]) + (1 - beta) * b[t - 1] s[t] &lt;- gamma * (y[t - 1] - l[t - 1] - b[t - 1]) + (1 - gamma) * s[t - m] } else { l[t] &lt;- l[t - 1]; b[t] &lt;- b[t - 1]; s[t] &lt;- s[t - 1] } y[t] &lt;- l[t] + b[t] + s[t %% m + 1] + rnorm(1, 0, sigma) } ts.plot(y, main = &quot;Simulated Seasonal Holt-Winters Time Series&quot;) ### Stan Model (Additive Seasonal) stan_model_3=&#39;data { int&lt;lower=2&gt; N; // number of observations int&lt;lower=1&gt; m; // seasonal period vector[N] y; } parameters { real&lt;lower=0, upper=1&gt; alpha; real&lt;lower=0, upper=1&gt; beta; real&lt;lower=0, upper=1&gt; gamma; real&lt;lower=0&gt; sigma; vector[N] l; // level vector[N] b; // trend vector[N] s; // seasonal } model { l[1] ~ normal(y[1], sigma); b[1] ~ normal(0, sigma); for (i in 1:m) s[i] ~ normal(0, 1); for (t in 2:N) { if (t &gt; m) { l[t] ~ normal(alpha * (y[t - 1] - s[t - m]) + (1 - alpha) * (l[t - 1] + b[t - 1]), sigma); b[t] ~ normal(beta * (l[t] - l[t - 1]) + (1 - beta) * b[t - 1], sigma); s[t] ~ normal(gamma * (y[t - 1] - l[t - 1] - b[t - 1]) + (1 - gamma) * s[t - m], sigma); } else { l[t] ~ normal(l[t - 1], 1); b[t] ~ normal(b[t - 1], 1); s[t] ~ normal(s[t - 1], 1); } } for (t in 1:N) { if (t &gt; m) y[t] ~ normal(l[t] + b[t] + s[t - m], sigma); else y[t] ~ normal(l[t] + b[t] + s[t], sigma); } }&#39; 12.2.5 Fit library(rstan) data_list &lt;- list(N = length(y), y = y, m = m) fit &lt;- stan(model_code = stan_model_3, data = data_list, chains = 4, iter = 2000, warmup = 1000) print(fit, pars = c(&quot;alpha&quot;, &quot;beta&quot;, &quot;gamma&quot;, &quot;sigma&quot;)) post &lt;- rstan::extract(fit) l_hat &lt;- apply(post$l, 2, mean) l_ci_lower &lt;- apply(post$l, 2, quantile, probs = 0.025) l_ci_upper &lt;- apply(post$l, 2, quantile, probs = 0.975) df &lt;- data.frame( time = 1:n, observed = y, level_mean = l_hat, level_lower = l_ci_lower, level_upper = l_ci_upper ) 12.2.6 Visualization post &lt;- extract(fit) l_hat &lt;- apply(post$l, 2, mean) l_ci_lower &lt;- apply(post$l, 2, quantile, 0.025) l_ci_upper &lt;- apply(post$l, 2, quantile, 0.975) df &lt;- data.frame( time = 1:n, observed = y, level_mean = l_hat, level_lower = l_ci_lower, level_upper = l_ci_upper ) ggplot(df, aes(x = time)) + geom_line(aes(y = observed), linetype = &quot;dashed&quot;, color = &quot;black&quot;) + geom_line(aes(y = level_mean), color = &quot;red&quot;) + geom_ribbon(aes(ymin = level_lower, ymax = level_upper), fill = &quot;red&quot;, alpha = 0.2) + labs(title = &quot;Bayesian Seasonal Holt-Winters: Posterior Level Estimates&quot;, y = &quot;Value&quot;) + theme_minimal() 12.3 Conclusion Bayesian formulations of Exponential Smoothing and Holt-Winters models offer elegant alternatives to frequentist smoothing, with the advantage of posterior inference, natural handling of uncertainty, and full probabilistic forecasting. Using Stan via RStan, we can model level, trend, and seasonality in a transparent and interpretable way—crucial for applications in forecasting and planning. These methods can be extended further to include hierarchical structure, time-varying parameters, or non-Gaussian errors for more complex real-world tasks. "],["bayesian-ar-arma-and-arima-models-in-stan-and-rstan.html", "Chapter 13 Bayesian AR, ARMA, and ARIMA Models in Stan and RStan 13.1 Bayesian AR(1) Model 13.2 Bayesian ARMA(1,1) Model 13.3 Bayesian ARIMA(1,1,1) Model 13.4 Conclusion", " Chapter 13 Bayesian AR, ARMA, and ARIMA Models in Stan and RStan Bayesian methods for time series modeling have gained widespread appeal for their ability to provide full posterior inference, quantify uncertainty, and incorporate prior knowledge into the analysis of temporal data. In this post, we walk through the foundational Bayesian time series models: the autoregressive (AR), autoregressive moving average (ARMA), and autoregressive integrated moving average (ARIMA) models. Each builds upon the previous, offering increased flexibility for real-world applications. We implement each model using Stan, a powerful probabilistic programming language, and RStan, its R interface. Through these examples, we aim to offer a practical understanding of how Bayesian time series models are constructed, estimated, and interpreted. 13.1 Bayesian AR(1) Model We begin with the simplest dynamic model: the AR(1) process. This model assumes that the current value of the time series depends linearly on its previous value plus a noise term. Formally, \\[ y_t = \\alpha + \\phi y_{t-1} + \\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2) \\] To simulate such a process in R: set.seed(123) n &lt;- 100 phi &lt;- 0.7 sigma &lt;- 1 y &lt;- numeric(n) y[1] &lt;- rnorm(1, 0, sigma / sqrt(1 - phi^2)) for (t in 2:n) { y[t] &lt;- phi * y[t - 1] + rnorm(1, 0, sigma) } ts.plot(y, main = &quot;Simulated AR(1) Time Series&quot;) We fit this model in Stan using the following code: data { int&lt;lower=1&gt; N; vector[N] y; } parameters { real alpha; real&lt;lower=-1, upper=1&gt; phi; real&lt;lower=0&gt; sigma; } model { y[2:N] ~ normal(alpha + phi * y[1:N-1], sigma); } And we call the model in R as follows: library(rstan) data_list &lt;- list(N = length(y), y = y) fit &lt;- stan( file = &quot;ar1_model.stan&quot;, data = data_list, chains = 4, iter = 2000, warmup = 1000 ) print(fit) 13.2 Bayesian ARMA(1,1) Model While the AR model captures persistence in the series, it cannot account for short-term shocks that decay quickly. The ARMA(1,1) model addresses this by including a moving average component: \\[ y_t = \\alpha + \\phi y_{t-1} + \\epsilon_t + \\theta \\epsilon_{t-1}, \\quad \\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2) \\] To simulate an ARMA(1,1) process in R: set.seed(123) n &lt;- 200 phi &lt;- 0.6 theta &lt;- 0.5 sigma &lt;- 1 y &lt;- numeric(n) e &lt;- rnorm(n, 0, sigma) y[1] &lt;- e[1] for (t in 2:n) { y[t] &lt;- phi * y[t - 1] + e[t] + theta * e[t - 1] } ts.plot(y, main = &quot;Simulated ARMA(1,1) Time Series&quot;) The Stan model computes residuals explicitly in a transformed parameters block, as recursion over parameters is not allowed: data { int&lt;lower=2&gt; N; vector[N] y; } parameters { real alpha; real&lt;lower=-1, upper=1&gt; phi; real&lt;lower=-1, upper=1&gt; theta; real&lt;lower=0&gt; sigma; } transformed parameters { vector[N] mu; vector[N] eps; mu[1] = alpha; eps[1] = y[1] - mu[1]; for (t in 2:N) { mu[t] = alpha + phi * y[t - 1] + theta * eps[t - 1]; eps[t] = y[t] - mu[t]; } } model { eps[2:N] ~ normal(0, sigma); } Model fitting proceeds in R with: data_list &lt;- list(N = length(y), y = y) fit &lt;- stan( file = &quot;arma11.stan&quot;, data = data_list, chains = 4, iter = 2000, warmup = 1000 ) print(fit, pars = c(&quot;alpha&quot;, &quot;phi&quot;, &quot;theta&quot;, &quot;sigma&quot;)) 13.3 Bayesian ARIMA(1,1,1) Model Many time series exhibit non-stationary behavior, such as trends, that must be differenced away before modeling. The ARIMA(1,1,1) model applies a first-order difference to the series and models the differenced data as ARMA(1,1): \\[ \\Delta y_t = y_t - y_{t-1} = \\alpha + \\phi \\Delta y_{t-1} + \\epsilon_t + \\theta \\epsilon_{t-1} \\] We first simulate an ARIMA(1,1,1) process in R: set.seed(42) n &lt;- 200 phi &lt;- 0.6 theta &lt;- 0.5 sigma &lt;- 1 eps &lt;- rnorm(n, 0, sigma) dy &lt;- numeric(n) dy[1] &lt;- eps[1] for (t in 2:n) { dy[t] &lt;- phi * dy[t - 1] + eps[t] + theta * eps[t - 1] } y &lt;- cumsum(dy) ts.plot(y, main = &quot;Simulated ARIMA(1,1,1) Time Series&quot;) The Stan model computes the first difference internally and fits the resulting ARMA process: data { int&lt;lower=2&gt; N; vector[N] y; } transformed data { vector[N - 1] dy; for (t in 2:N) dy[t - 1] = y[t] - y[t - 1]; } parameters { real alpha; real&lt;lower=-1, upper=1&gt; phi; real&lt;lower=-1, upper=1&gt; theta; real&lt;lower=0&gt; sigma; } transformed parameters { vector[N - 1] mu; vector[N - 1] eps; mu[1] = alpha; eps[1] = dy[1] - mu[1]; for (t in 2:(N - 1)) { mu[t] = alpha + phi * dy[t - 1] + theta * eps[t - 1]; eps[t] = dy[t] - mu[t]; } } model { eps[2:(N - 1)] ~ normal(0, sigma); } We estimate the model with RStan using: data_list &lt;- list(N = length(y), y = y) fit &lt;- stan( file = &quot;arima11.stan&quot;, data = data_list, chains = 4, iter = 2000, warmup = 1000 ) print(fit, pars = c(&quot;alpha&quot;, &quot;phi&quot;, &quot;theta&quot;, &quot;sigma&quot;)) 13.4 Conclusion Bayesian time series modeling provides a flexible, interpretable framework for analyzing temporal data. Beginning with the autoregressive model, we can successively build toward more complex structures like ARMA and ARIMA. Each model expands our capacity to handle persistence, shocks, and non-stationarity in a principled probabilistic way. While Bayesian inference comes with computational cost, it also brings the benefits of full uncertainty quantification, model extensibility, and coherent predictions under uncertainty. Using Stan and RStan, we can translate classical time series models into a Bayesian framework and apply them to real-world data with transparency and rigor. Future work may involve extending these models to accommodate seasonality, hierarchical structure, or time-varying parameters. "],["bayesian-structural-time-series-models.html", "Chapter 14 14 Bayesian Structural Time Series Models 14.1 Introduction 14.2 Model Comparison and Selection 14.3 Conclusion", " Chapter 14 14 Bayesian Structural Time Series Models 14.1 Introduction Time series data pervade scientific and applied domains, from economics and epidemiology to environmental monitoring and finance. Extracting meaningful patterns from such data requires statistical models that can handle temporal dependencies, seasonality, trends, and structural breaks while providing interpretable decompositions. Bayesian Structural Time Series (BSTS) models offer a principled framework for addressing these challenges by decomposing time series into latent components while incorporating prior information and quantifying uncertainty. Initially popularized by Google for causal impact analysis, BSTS models extend classical structural time series approaches by embedding them within a fully Bayesian inference framework. However, many real-world time series exhibit periods of varying volatility—economic indicators may experience stable growth followed by high volatility due to macroeconomic shocks, while financial markets alternate between calm and turbulent periods. Standard BSTS models assume constant volatility in trend components, which can be overly restrictive and obscure critical temporal dynamics. In this comprehensive post, we explore both canonical BSTS models and their extension to time-varying trend volatility. We provide complete implementations in Stan, demonstrate fitting procedures in R using RStan, and illustrate applications across different scenarios. This unified treatment enables practitioners to understand the full spectrum of BSTS modeling capabilities and choose appropriate specifications for their specific time series challenges. 14.1.0.1 Model Specification: Core BSTS Framework The canonical BSTS model represents an observed time series as the sum of interpretable latent components: \\[y_t = \\mu_t + \\tau_t + \\gamma_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\] where: - \\(\\mu_t\\) is the local level component - \\(\\tau_t\\) represents the stochastic trend (slope or drift) - \\(\\gamma_t\\) captures seasonal effects - \\(\\varepsilon_t\\) is the observation error term The local level and trend components follow a local linear trend specification: \\[\\mu_t = \\mu_{t-1} + \\tau_{t-1} + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, \\sigma_\\mu^2)\\] \\[\\tau_t = \\tau_{t-1} + \\zeta_t, \\quad \\zeta_t \\sim \\mathcal{N}(0, \\sigma_\\tau^2)\\] This formulation allows the trend to evolve smoothly over time while maintaining flexibility in both level and slope dynamics. The seasonal component \\(\\gamma_t\\) is typically modeled using periodic structures that ensure the seasonal effects sum to zero over each complete cycle. 14.1.0.2 Extension to Time-Varying Trend Volatility In many applications, the assumption of constant trend volatility (\\(\\sigma_\\tau^2\\)) proves inadequate. Economic time series, for instance, may exhibit stable growth periods punctuated by high-volatility episodes during recessions or policy changes. To accommodate such dynamics, we extend the standard framework by allowing the volatility of the trend innovation to evolve over time. We introduce a stochastic volatility process for the trend innovation variance: \\[\\log \\sigma_{\\tau, t}^2 = h_t, \\quad h_t = h_{t-1} + \\nu_t, \\quad \\nu_t \\sim \\mathcal{N}(0, \\sigma_h^2)\\] \\[\\zeta_t \\sim \\mathcal{N}(0, \\exp(h_t))\\] This specification implements a log-random walk for the trend volatility, ensuring positive variance while allowing for gradual or abrupt changes in trend smoothness. The parameter \\(\\sigma_h^2\\) controls how quickly the trend volatility can change: larger values permit more rapid volatility shifts, while smaller values enforce smoother volatility evolution. 14.1.0.3 Data Simulation: Building Synthetic Time Series To demonstrate both modeling approaches, we begin by simulating realistic time series data that incorporates the key components of interest. Basic BSTS Simulation set.seed(42) n &lt;- 120 # number of time points m &lt;- 12 # seasonal period level &lt;- numeric(n) trend &lt;- numeric(n) season &lt;- numeric(n) y &lt;- numeric(n) # Initialize components level[1] &lt;- 10 trend[1] &lt;- 0.5 seasonal_pattern &lt;- sin(2 * pi * (1:m) / m) season[1:m] &lt;- seasonal_pattern # Generate time series for (t in 2:n) { level[t] &lt;- level[t-1] + trend[t-1] + rnorm(1, 0, 0.3) trend[t] &lt;- trend[t-1] + rnorm(1, 0, 0.05) season[t] &lt;- season[t %% m + 1] # repeat seasonal pattern y[t] &lt;- level[t] + season[t] + rnorm(1, 0, 1) } ts.plot(y, main = &quot;Simulated BSTS Time Series&quot;) Time-Varying Volatility Simulation For the extended model with stochastic volatility, we modify the simulation to include time-varying trend volatility: set.seed(123) n &lt;- 120 m &lt;- 12 level &lt;- numeric(n) trend &lt;- numeric(n) h &lt;- numeric(n) sigma_trend_t &lt;- numeric(n) season &lt;- numeric(n) y &lt;- numeric(n) # Initialize components level[1] &lt;- 10 trend[1] &lt;- 0.5 h[1] &lt;- log(0.05^2) # initial log-variance seasonal_pattern &lt;- sin(2 * pi * (1:m) / m) season[1:m] &lt;- seasonal_pattern # Generate time series with time-varying trend volatility for (t in 2:n) { # Update log-volatility h[t] &lt;- h[t - 1] + rnorm(1, 0, 0.1) sigma_trend_t[t] &lt;- sqrt(exp(h[t])) # Update components trend[t] &lt;- trend[t - 1] + rnorm(1, 0, sigma_trend_t[t]) level[t] &lt;- level[t - 1] + trend[t - 1] + rnorm(1, 0, 0.3) season[t] &lt;- season[t %% m + 1] y[t] &lt;- level[t] + season[t] + rnorm(1, 0, 1) } ts.plot(y, main = &quot;Simulated Time Series with Time-Varying Trend Volatility&quot;) This simulation generates periods of varying trend smoothness, providing a realistic testbed for the extended model. ## Stan Implementation 14.1.1 Stan Implementation: Basic BSTS Model We implement the core BSTS model in Stan, emphasizing clarity and computational efficiency: functions { vector seasonal_component(vector s, int N, int m) { vector[N] seasonal; for (t in 1:N) { if (t &gt; m) seasonal[t] = s[t - m]; else seasonal[t] = s[t]; } return seasonal; } } data { int&lt;lower=1&gt; N; int&lt;lower=1&gt; m; // seasonal period vector[N] y; } parameters { real&lt;lower=0&gt; sigma_obs; real&lt;lower=0&gt; sigma_level; real&lt;lower=0&gt; sigma_trend; real&lt;lower=0&gt; sigma_seasonal; vector[N] level; vector[N] trend; vector[N] seasonal; } model { // State evolution equations level[2:N] ~ normal(level[1:(N-1)] + trend[1:(N-1)], sigma_level); trend[2:N] ~ normal(trend[1:(N-1)], sigma_trend); seasonal[(m+1):N] ~ normal(seasonal[1:(N-m)], sigma_seasonal); // Observation equation y ~ normal(level + seasonal_component(seasonal, N, m), sigma_obs); // Priors sigma_obs ~ normal(0, 1); sigma_level ~ normal(0, 0.5); sigma_trend ~ normal(0, 0.1); sigma_seasonal ~ normal(0, 0.5); } 14.1.2 Stan Implementation: Time-Varying Trend Volatility The extended model incorporates stochastic volatility into the trend component: functions { vector seasonal_component(vector s, int N, int m) { vector[N] seasonal; for (t in 1:N) { if (t &gt; m) seasonal[t] = s[t - m]; else seasonal[t] = s[t]; } return seasonal; } } data { int&lt;lower=1&gt; N; int&lt;lower=1&gt; m; vector[N] y; } parameters { real&lt;lower=0&gt; sigma_obs; real&lt;lower=0&gt; sigma_level; real&lt;lower=0&gt; sigma_h; vector[N] level; vector[N] trend; vector[N] seasonal; vector[N] h; // log variance of trend innovation } model { // Priors sigma_obs ~ normal(0, 1); sigma_level ~ normal(0, 0.5); sigma_h ~ normal(0, 0.2); h[1] ~ normal(log(0.05^2), 0.1); // Seasonal component evolution seasonal[(m+1):N] ~ normal(seasonal[1:(N-m)], 0.5); // Latent state processes for (t in 2:N) { h[t] ~ normal(h[t-1], sigma_h); trend[t] ~ normal(trend[t-1], exp(0.5 * h[t])); level[t] ~ normal(level[t-1] + trend[t-1], sigma_level); } // Observation equation y ~ normal(level + seasonal_component(seasonal, N, m), sigma_obs); } The key innovation lies in the stochastic evolution of h[t], which governs the time-varying volatility of trend innovations through the exponential transformation exp(0.5 * h[t]). Model Fitting in R with RStan Both models can be fitted using RStan with similar workflows, differing primarily in their complexity and computational requirements. Basic BSTS Model Fitting library(rstan) options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) # Prepare data data_list_basic &lt;- list(N = length(y), m = m, y = y) # Fit basic model fit_basic &lt;- stan(model_code = stan_model_bsts_basic, data = data_list_basic, chains = 4, iter = 2000, warmup = 1000, seed = 42) print(fit_basic, pars = c(&quot;sigma_obs&quot;, &quot;sigma_level&quot;, &quot;sigma_trend&quot;, &quot;sigma_seasonal&quot;)) Time-Varying Volatility Model Fitting # Fit extended model fit_extended &lt;- stan(model_code = stan_model_bsts_extended, data = data_list_basic, chains = 4, iter = 2000, warmup = 1000, seed = 123) print(fit_extended, pars = c(&quot;sigma_obs&quot;, &quot;sigma_level&quot;, &quot;sigma_h&quot;)) The posterior for sigma_h provides insights into volatility dynamics: larger values indicate more variable trend volatility, while smaller values suggest smoother volatility evolution. Visualization and Interpretation Effective visualization of BSTS results requires displaying both point estimates and uncertainty quantification for the latent components. Basic Model Visualization # Extract posterior samples post_basic &lt;- extract(fit_basic) # Compute posterior summaries level_mean &lt;- apply(post_basic$level, 2, mean) level_lower &lt;- apply(post_basic$level, 2, quantile, probs = 0.025) level_upper &lt;- apply(post_basic$level, 2, quantile, probs = 0.975) trend_mean &lt;- apply(post_basic$trend, 2, mean) library(ggplot2) df_basic &lt;- data.frame( time = 1:n, observed = y, level = level_mean, level_lower = level_lower, level_upper = level_upper, trend = trend_mean ) # Plot level component with uncertainty ggplot(df_basic, aes(x = time)) + geom_line(aes(y = observed), color = &quot;black&quot;, linetype = &quot;dashed&quot;, alpha = 0.7) + geom_line(aes(y = level), color = &quot;blue&quot;, size = 1) + geom_ribbon(aes(ymin = level_lower, ymax = level_upper), fill = &quot;blue&quot;, alpha = 0.2) + labs(title = &quot;BSTS Level Component with 95% Credible Intervals&quot;, x = &quot;Time&quot;, y = &quot;Value&quot;) + theme_minimal() # Plot trend evolution ggplot(df_basic, aes(x = time)) + geom_line(aes(y = trend), color = &quot;red&quot;, size = 1) + labs(title = &quot;Estimated Trend Component&quot;, x = &quot;Time&quot;, y = &quot;Trend&quot;) + theme_minimal() Time-Varying Volatility Visualization # Extract posterior samples for extended model post_extended &lt;- extract(fit_extended) # Compute posterior summaries h_mean &lt;- apply(post_extended$h, 2, mean) h_lower &lt;- apply(post_extended$h, 2, quantile, probs = 0.025) h_upper &lt;- apply(post_extended$h, 2, quantile, probs = 0.975) trend_mean_ext &lt;- apply(post_extended$trend, 2, mean) df_extended &lt;- data.frame( time = 1:n, log_vol = h_mean, log_vol_lower = h_lower, log_vol_upper = h_upper, trend = trend_mean_ext ) # Plot time-varying log-volatility ggplot(df_extended, aes(x = time)) + geom_line(aes(y = log_vol), color = &quot;red&quot;, size = 1) + geom_ribbon(aes(ymin = log_vol_lower, ymax = log_vol_upper), fill = &quot;red&quot;, alpha = 0.2) + labs(title = &quot;Time-Varying Trend Log-Volatility with 95% Credible Intervals&quot;, y = &quot;log(σ²_τ)&quot;, x = &quot;Time&quot;) + theme_minimal() # Plot corresponding trend with varying smoothness ggplot(df_extended, aes(x = time)) + geom_line(aes(y = trend), color = &quot;blue&quot;, size = 1) + labs(title = &quot;Trend Component with Time-Varying Volatility&quot;, y = &quot;Trend&quot;, x = &quot;Time&quot;) + theme_minimal() These visualizations reveal how uncertainty in trend evolution changes over time, enabling analysts to identify periods of instability or structural transitions. 14.1.2.1 Applications and Extensions BSTS models find applications across diverse domains where interpretable time series decomposition is valuable. The basic framework handles missing data naturally and provides a foundation for causal impact analysis through counterfactual prediction. The time-varying volatility extension proves particularly valuable in financial econometrics, where volatility clustering and regime changes are common phenomena. In financial time series, periods of market calm alternate with high-volatility episodes during crises or major economic announcements. The time-varying trend volatility model can capture these dynamics while maintaining the interpretable decomposition that BSTS models provide. Economic indicators often exhibit structural breaks and changing volatility regimes due to policy interventions, technological changes, or external shocks. The flexible volatility specification enables more adaptive forecasting in such environments. Climate time series frequently display non-stationary variance patterns due to changing atmospheric or oceanic conditions. The stochastic volatility framework can accommodate such dynamics while preserving seasonal and trend interpretability. Both modeling frameworks can be extended to hierarchical structures where multiple related time series share common components, or to multivariate settings where cross-series dependencies are of interest. The Stan implementations provide a flexible foundation for such extensions. BSTS models naturally accommodate regression components with spike-and-slab priors for variable selection. This capability enables automatic identification of relevant predictors while maintaining the structural decomposition that makes BSTS models interpretable. The computational complexity of BSTS models scales with the number of time points and latent states. The basic model typically converges within standard MCMC runs, while the time-varying volatility extension requires longer chains due to the additional stochastic volatility component. Stan’s efficient gradient-based sampling (HMC/NUTS) handles the high-dimensional latent state spaces more effectively than traditional Gibbs sampling approaches. However, practitioners should monitor convergence diagnostics carefully, particularly for the volatility parameters in the extended model. For very long time series, computational efficiency can be improved through state space formulations that marginalize over latent states, though this sacrifices direct access to the latent component estimates that make BSTS models interpretable. 14.2 Model Comparison and Selection Comparing basic and extended BSTS models requires balancing goodness-of-fit against complexity. The Widely Applicable Information Criterion (WAIC) provides a practical tool for model comparison within the Bayesian framework: library(loo) # Compute WAIC for model comparison waic_basic &lt;- waic(extract_log_lik(fit_basic)) waic_extended &lt;- waic(extract_log_lik(fit_extended)) # Compare models loo_compare(waic_basic, waic_extended) Additionally, posterior predictive checks help assess whether the added complexity of time-varying volatility improves model adequacy for the specific application. 14.3 Conclusion Bayesian Structural Time Series models provide a powerful and interpretable framework for analyzing complex temporal data. The basic BSTS specification offers transparent decomposition into level, trend, and seasonal components while maintaining full uncertainty quantification through the Bayesian approach. The extension to time-varying trend volatility addresses limitations of constant volatility assumptions, enabling more adaptive modeling of time series with changing uncertainty regimes. The Stan implementations presented here provide complete, working examples that practitioners can adapt to their specific applications. The state-space framework naturally handles missing data and irregular observation patterns, while the Bayesian approach enables principled incorporation of prior information and coherent uncertainty propagation. As computational tools continue to improve and the demand for interpretable time series models grows, BSTS models are positioned to play an increasingly important role across scientific and applied domains. The flexibility demonstrated through the time-varying volatility extension illustrates how the basic framework can be adapted to address specific modeling challenges while preserving the interpretability that makes BSTS models valuable for practical time series analysis. The unified treatment presented here equips practitioners with both foundational understanding and practical tools for implementing BSTS models across a spectrum of applications, from basic seasonal decomposition to advanced volatility modeling in dynamic environments. "],["model-comparison-and-selection-in-bayesian-analysis.html", "Chapter 15 Model Comparison and Selection in Bayesian Analysis 15.1 Introduction 15.2 Posterior Predictive Checks 15.3 Information Criteria: WAIC and LOO-CV 15.4 Bayes Factors vs. Information Criteria 15.5 Summary of Bayesian Model Comparison Methods 15.6 Practical Example: Comparing Models in R", " Chapter 15 Model Comparison and Selection in Bayesian Analysis 15.1 Introduction Bayesian statistical modeling offers a flexible and principled approach to quantifying uncertainty and incorporating prior knowledge. As readers gain familiarity with building a variety of models—ranging from simple linear regressions to complex hierarchical and non-parametric models—a critical question emerges: how do we choose among competing models? Bayesian model comparison and selection are fundamental steps in the modeling process, ensuring that inferences are both robust and interpretable. Unlike frequentist paradigms, Bayesian methods provide a coherent framework for comparing models by evaluating the plausibility of the data given the model, known as model evidence. However, this task is often complicated by practical challenges in estimating model evidence and balancing predictive accuracy with model complexity. This article explores the key tools available for Bayesian model comparison, including posterior predictive checks, information criteria like WAIC and LOO-CV, and Bayes factors. Each method offers unique insights, and their strengths and limitations make them suitable for different analytical contexts. While selecting a single model is a common goal, Bayesian modeling also allows for model averaging and stacking, approaches that combine models to account for model uncertainty and improve predictive robustness. These alternatives are especially useful when no model clearly dominates or when combining perspectives from multiple models provides more comprehensive insights. 15.2 Posterior Predictive Checks Posterior predictive checks are a fundamental diagnostic tool used to assess the fit of a Bayesian model to the observed data. The idea is to simulate data from the posterior predictive distribution and compare it to the actual observed data. Discrepancies between the simulated and observed data can indicate model misfit or structural inadequacies. Formally, the posterior predictive distribution is given by: \\(p(\\tilde{y} \\mid y) = \\int p(\\tilde{y} \\mid \\theta) p(\\theta \\mid y) d\\theta\\) where \\(\\tilde{y}\\) represents future or replicated data, \\(y\\) is the observed data, and \\(\\theta\\) denotes the parameters. Visualization tools such as histograms, density plots, and test statistics (e.g., discrepancy measures) are commonly used to perform these checks. In practice, packages like bayesplot in R make it easy to implement these diagnostics. Although posterior predictive checks are excellent for identifying model fit issues, they do not provide a direct basis for comparing multiple models. Transition: While posterior predictive checks help diagnose model adequacy, they are less informative for ranking competing models. For this, we turn to information-theoretic approaches like WAIC and LOO-CV. 15.3 Information Criteria: WAIC and LOO-CV Two widely used Bayesian model comparison tools are the Watanabe-Akaike Information Criterion (WAIC) and Leave-One-Out Cross-Validation (LOO-CV). Both aim to estimate a model’s out-of-sample predictive performance. 15.3.1 WAIC (Watanabe-Akaike Information Criterion) WAIC is a fully Bayesian criterion that estimates out-of-sample predictive accuracy while penalizing for model complexity. It is computed as: \\(\\text{WAIC} = -2(\\text{lppd} - p_{\\text{WAIC}})\\) where lppd is the log pointwise predictive density and \\(p_{\\text{WAIC}}\\) is the effective number of parameters, which quantifies the flexibility of the posterior distribution in fitting the data. WAIC is asymptotically equivalent to Bayesian cross-validation and is computable directly from posterior samples. Unlike classical AIC or BIC, which penalize complexity by parameter count, WAIC adjusts based on how flexible the posterior is in fitting the data. This leads to more nuanced complexity penalties, especially in hierarchical models where the number of effective parameters can be non-integer and context-sensitive. 15.3.2 LOO-CV (Leave-One-Out Cross-Validation) LOO-CV involves fitting the model repeatedly, leaving out one observation at a time and evaluating predictive performance on the omitted data. To avoid the computational burden of refitting the model \\(n\\) times, Pareto-smoothed importance sampling (PSIS) offers an efficient approximation, implemented in the loo R package. The loo package also provides diagnostic tools, such as Pareto \\(k\\) values, to assess the reliability of importance sampling. Typically, values below 0.7 suggest stable estimates; higher values may indicate influential observations that require model reassessment. Additionally, LOO-CV provides a natural framework for identifying influential data points. Observations with high \\(k\\) values may point to model misspecification, data entry errors, or unaccounted-for heterogeneity. Thus, beyond model ranking, LOO-CV offers an avenue for deepening model diagnostics. Both WAIC and LOO-CV yield estimates of expected out-of-sample deviance. Lower values indicate better predictive accuracy. Transition: While WAIC and LOO-CV emphasize predictive performance, Bayes factors offer an alternative grounded in model plausibility and marginal likelihood. 15.4 Bayes Factors vs. Information Criteria Bayes factors provide a coherent Bayesian approach to model comparison by evaluating how well each model explains the observed data, integrating over all parameter values: \\(BF_{12} = \\frac{p(y \\mid M_1)}{p(y \\mid M_2)}\\) A Bayes factor greater than 1 indicates support for model \\(M_1\\) over \\(M_2\\), and vice versa. Interpretation guidelines (Kass &amp; Raftery, 1995) suggest: 1 to 3: Anecdotal evidence 3 to 10: Moderate evidence 10: Strong evidence Bayes factors incorporate prior model probabilities and are grounded in decision theory. However, they are often sensitive to the choice of priors and can be computationally intensive due to the need to estimate marginal likelihoods. Despite their theoretical appeal, Bayes factors suffer from two key limitations. First, the sensitivity to prior distributions — especially priors on nuisance parameters — can lead to unstable or unintuitive conclusions. Second, the marginal likelihood integrates over the entire parameter space, potentially over-penalizing complex models when priors are diffuse. This makes Bayes factors ill-suited to models with weak or uninformative priors unless priors are carefully calibrated. A useful computational shortcut in nested models is the Savage-Dickey density ratio, which allows Bayes factor computation via prior and posterior densities of a parameter of interest. In contrast, WAIC and LOO-CV focus on predictive accuracy and are generally more robust to prior specification, making them preferable in prediction-oriented tasks. 15.5 Summary of Bayesian Model Comparison Methods Method Goal Sensitive to Priors Penalizes Complexity Emphasis Tools (R Packages) Use Case Posterior Predictive Assess model fit No No Model adequacy bayesplot, rstanarm Diagnosing misfit or model structure WAIC Estimate predictive accuracy Mildly Yes Out-of-sample prediction loo, rstanarm Model ranking for prediction LOO-CV (PSIS) Estimate predictive accuracy Mildly Yes Out-of-sample prediction loo, rstanarm Robust prediction comparison Bayes Factors Marginal likelihood comparison Yes Implicit Model evidence bridgesampling, BayesFactor Hypothesis testing, model plausibility 15.6 Practical Example: Comparing Models in R To illustrate these concepts, consider the following example using a synthetic dataset. We compare two models: a simple linear regression and a polynomial regression. library(rstanarm) library(loo) library(bayesplot) # Simulate data set.seed(123) x &lt;- rnorm(100) y &lt;- 2 * x + x^2 + rnorm(100) data &lt;- data.frame(x = x, y = y) # Fit linear and quadratic models fit_linear &lt;- stan_glm(y ~ x, data = data, refresh = 0) fit_quad &lt;- stan_glm(y ~ x + I(x^2), data = data, refresh = 0) # Posterior predictive checks pp_check(fit_linear) pp_check(fit_quad) # WAIC comparison waic_linear &lt;- waic(fit_linear) waic_quad &lt;- waic(fit_quad) print(waic_linear) print(waic_quad) # LOO-CV comparison loo_linear &lt;- loo(fit_linear) loo_quad &lt;- loo(fit_quad) print(loo_compare(loo_linear, loo_quad)) # Bayes factor using bridgesampling library(rstanarm) library(bridgesampling) # Fit model with diagnostic_file argument fit_linear &lt;- stan_glm( y ~ x, data = data, refresh = 0, diagnostic_file = file.path(tempdir(), &quot;linear_diag.csv&quot;) ) fit_quad &lt;- stan_glm( y ~ x + I(x^2), data = data, refresh = 0, diagnostic_file = file.path(tempdir(), &quot;quad_diag.csv&quot;) ) # Run bridge sampling bridge_linear &lt;- bridge_sampler(fit_linear) bridge_quad &lt;- bridge_sampler(fit_quad) # Compute Bayes factor bf_result &lt;- bf(bridge_quad, bridge_linear) print(bf_result) In situations where neither model clearly dominates or when both offer complementary insights, Bayesian stacking or model averaging may be employed to combine predictive strengths rather than choosing a single model. The loo package also supports stacking weights, enabling ensemble predictive performance evaluation. No single method is universally superior; the choice depends on the modeling context, computational resources, and analytic goals. In some cases, model averaging or stacking may offer additional robustness when no single model clearly dominates. Posterior predictive checks serve as intuitive diagnostics for model adequacy, while information criteria like WAIC and LOO-CV offer principled approaches for evaluating predictive performance. Bayes factors provide an alternative rooted in model evidence, though they require careful prior specification and computational resources. By incorporating these tools into their workflow, practitioners can enhance the credibility, interpretability, and predictive utility of their Bayesian analyses. A reasonable workflow might begin with posterior predictive checks to assess absolute model fit, followed by WAIC or LOO-CV to evaluate predictive utility. Bayes factors can be used where prior calibration is strong and the goal is hypothesis testing. When model choice is ambiguous, averaging via stacking provides a principled compromise. Ultimately, careful model comparison is as much about understanding the modeling context as it is about applying formulas. "],["appendix.html", "Chapter 16 Appendix 16.1 Main Stan Distributions Cheatsheet 16.2 Main Stan Functions Cheatsheet 16.3 Why Non-Distribution Functions? 16.4 Stan Functions Cheatsheet 16.5 Example: Hierarchical Linear Regression 16.6 Tips for Using Stan Functions", " Chapter 16 Appendix 16.1 Main Stan Distributions Cheatsheet Statistical modeling in Stan is powered by a flexible and expressive probabilistic language grounded in log-density functions. While the modeling blocks (model, data, parameters, etc.) help structure a model, the core statistical logic is defined through distributions. This cheatsheet offers a practical summary of the most important distributions used in Stan, their syntax, required parameters, typical use cases, and examples of where they show up in statistical modeling. Distribution Function Parameters Use Case Model Type(s) Bernoulli `bernoulli_lpmf(y θ)` θ ∈ (0, 1) Binary outcome (0/1) Logistic regression, classification Binomial `binomial_lpmf(y n, θ)` n ∈ ℕ⁺, θ ∈ (0, 1) # of successes in n trials Logistic GLMs, grouped binomial models Categorical `categorical_lpmf(y θ)` θ: simplex vector (length K) Single draw from K categories Multinomial regression Multinomial `multinomial_lpmf(y θ)` y: int vector of counts, θ: simplex Category count data Count models with category splits Normal `normal_lpdf(y μ, σ)` μ ∈ ℝ, σ &gt; 0 Gaussian noise, residuals Linear regression, priors for real parameters Student’s t `student_t_lpdf(y ν, μ, σ)` ν &gt; 0, μ ∈ ℝ, σ &gt; 0 Heavy-tailed data, robust models Robust regression, hierarchical priors Cauchy `cauchy_lpdf(y μ, σ)` μ ∈ ℝ, σ &gt; 0 Weakly informative, heavy-tailed prior Priors on scale parameters (e.g., τ ~ cauchy(0, 2.5)) Exponential `exponential_lpdf(y λ)` λ &gt; 0 Time to event, memoryless processes Survival models, Poisson process modeling Gamma `gamma_lpdf(y α, β)` α &gt; 0, β &gt; 0 Positive skewed data Priors on rates or shape parameters Inverse Gamma `inv_gamma_lpdf(y α, β)` α &gt; 0, β &gt; 0 Prior for variances Priors on σ², τ², especially in hierarchies Lognormal `lognormal_lpdf(y μ, σ)` μ ∈ ℝ, σ &gt; 0 Positive, right-skewed data Income, durations, reliability Beta `beta_lpdf(y α, β)` α &gt; 0, β &gt; 0 Probabilities or proportions Priors on probabilities (θ ∈ (0, 1)) Dirichlet `dirichlet_lpdf(θ α)` θ: simplex, α &gt; 0 vector Probabilities summing to 1 Priors for category proportions, LDA Poisson `poisson_lpmf(y λ)` λ &gt; 0 Count data, rare event modeling GLMs for count data Negative Binomial `neg_binomial_2_lpmf(y μ, φ)` μ &gt; 0, φ &gt; 0 Overdispersed count data GLMs with extra-Poisson variation Ordered Logistic `ordered_logistic_lpmf(y η, c)` η ∈ ℝ, c: ordered cut-points Ordinal outcomes Ordinal regression Uniform `uniform_lpdf(y a, b)` a &lt; b Flat prior within range Non-informative priors Pareto `pareto_lpdf(y y_min, α)` y_min &gt; 0, α &gt; 0 Heavy-tail data, power-law phenomena Extremes, outlier modeling Von Mises `von_mises_lpdf(y μ, κ)` μ ∈ [0, 2π), κ ≥ 0 Circular data (angles, wind direction) Directional models Weibull `weibull_lpdf(y α, σ)` α, σ &gt; 0 Survival times, failure rates Survival models, reliability analysis LKJ Correlation `lkj_corr_cholesky_lpdf(L η)` η &gt; 0, L: Cholesky factor Prior for correlation matrices Hierarchical models with random slopes Wishart `wishart_lpdf(S ν, Σ)` ν &gt; dim-1, Σ: scale matrix Prior on covariance matrices Multivariate Gaussian models (rarely used) 16.2 Main Stan Functions Cheatsheet Stan is a robust platform for Bayesian statistical modeling, renowned for its Hamiltonian Monte Carlo (HMC) engine and flexible modeling language. While probability distributions like normal_lpdf or poisson_lpmf define priors and likelihoods, Stan’s non-distribution functions—spanning mathematical operations, matrix algebra, utility tools, and specialized solvers—are equally critical for building efficient and expressive models. These functions enable data transformations, efficient computations, and post-processing in the generated quantities block. This cheatsheet organizes Stan’s most commonly used non-distribution functions into categories, providing their purpose, example usage, and the model types where they’re most applicable. Whether you’re crafting linear regressions, hierarchical models, or dynamic systems, this guide will help you leverage Stan’s toolkit effectively. We’ll wrap up with an example model to bring these functions to life. 16.3 Why Non-Distribution Functions? Stan’s non-distribution functions serve several key purposes: - Transformations: Functions like log, exp, and inv_logit map parameters to constrained spaces or perform nonlinear calculations. - Matrix Operations: Functions like dot_product and cholesky_decompose enable efficient linear algebra for multivariate models. - Utilities: Functions like to_vector and mean simplify data manipulation and posterior summaries. - Specialized Tools: Solvers like ode_rk45 and integrate_1d tackle complex systems, such as differential equations or custom likelihoods. - Posterior Processing: Functions in the generated quantities block, like sum or sd, compute diagnostics or predictions. This cheatsheet focuses on these functions to help you streamline model specification and analysis. 16.4 Stan Functions Cheatsheet 16.4.1 1. Mathematical Functions These functions perform scalar operations, often used in transformed parameters or model blocks. Function Purpose Example Usage Model Type(s) abs(x) Absolute value real z = abs(x); General computations, robust stats exp(x) Exponential (e^x) lambda = exp(alpha); Rate models, transformations log(x) Natural logarithm real l = log(y); Log-likelihoods, transformations sqrt(x) Square root sigma = sqrt(variance); Variance computations, scaling lgamma(x) Log gamma function lp += lgamma(alpha); Mixture models, custom likelihoods log_sum_exp(x) Log-sum-exp for numerical stability lp = log_sum_exp(log_theta); Mixture models, marginal likelihoods 16.4.2 2. Transformation Functions These map parameters to constrained spaces, often in transformed parameters. Function Purpose Example Usage Model Type(s) inv_logit(x) Logistic sigmoid (ℝ → (0,1)) theta = inv_logit(alpha + beta*x); Logistic regression, probability models logit(p) Log-odds ((0,1) → ℝ) eta = logit(p); Logistic regression, probit models softmax(x) Normalize vector to simplex theta = softmax(alpha); Multinomial regression, LDA inv(x) Reciprocal (1/x) inv_sigma = inv(sigma); Variance transformations 16.4.3 3. Matrix and Vector Operations These enable efficient linear algebra, critical for multivariate and hierarchical models. Function Purpose Example Usage Model Type(s) dot_product(a, b) Inner product of two vectors real z = dot_product(a, b); Linear regression, similarity measures matrix_times_vector(A, v) Matrix-vector multiplication eta = matrix_times_vector(X, beta); Multivariate regression, GLMs cholesky_decompose(S) Cholesky factorization L = cholesky_decompose(Sigma); Hierarchical models, multivariate normals multiply_lower_tri_self_transpose(L) Covariance from Cholesky factor Sigma = multiply_lower_tri_self_transpose(L); Multivariate normals, hierarchical models diag_matrix(v) Diagonal matrix from vector M = diag_matrix(v); Covariance priors, scaling determinant(A) Matrix determinant det = determinant(Sigma); Model diagnostics, multivariate priors 16.4.4 4. Utility Functions These simplify data manipulation and posterior summaries, often in generated quantities. Function Purpose Example Usage Model Type(s) to_vector(x) Convert matrix/array to vector vec = to_vector(matrix); Posterior summaries, data reshaping to_array_1d(x) Convert to 1D array arr = to_array_1d(matrix); Data preprocessing, summaries sum(x) Sum of elements total = sum(y); Aggregations, diagnostics mean(x) Mean of elements avg = mean(y_rep); Posterior summaries, diagnostics sd(x) Standard deviation std = sd(y_rep); Posterior summaries, diagnostics int_step(x) Indicator (x ≥ 0 → 1, else 0) flag = int_step(x - 1); Conditional logic, model diagnostics 16.4.5 5. Specialized Solvers These handle advanced computations like differential equations or parallel processing. Function Purpose Example Usage Model Type(s) ode_rk45(fun, y0, t0, ts, ...) Solve ODEs (Runge-Kutta 45) y = ode_rk45(ode_sys, y0, t0, ts, params); Dynamic systems, pharmacokinetics integrate_1d(f, a, b, ...) Numerical integration val = integrate_1d(f, a, b, params); Custom likelihoods, marginalization map_rect(f, phi, ...) Parallel computation over data shards results = map_rect(f, phi, theta, data); Large-scale hierarchical models 16.5 Example: Hierarchical Linear Regression Here’s a Stan model for a hierarchical linear regression, using matrix_times_vector, to_vector, and mean to demonstrate practical function usage: data { int&lt;lower=0&gt; N; // Number of observations int&lt;lower=0&gt; J; // Number of groups array[N] int&lt;lower=1,upper=J&gt; group; // Group indicators matrix[N, 2] X; // Design matrix (intercept + predictor) vector[N] y; // Outcome } parameters { vector[2] beta; // Fixed effects vector[J] alpha; // Group-level intercepts real&lt;lower=0&gt; sigma; // Residual standard deviation real&lt;lower=0&gt; tau; // Standard deviation of group intercepts } model { beta ~ normal(0, 5); // Prior on fixed effects tau ~ cauchy(0, 2.5); // Prior on group SD alpha ~ normal(0, tau); // Group-level priors sigma ~ cauchy(0, 2.5); // Prior on residual SD vector[N] mu = matrix_times_vector(X, beta) + to_vector(alpha[group]); y ~ normal(mu, sigma); // Likelihood } generated quantities { vector[N] y_rep; // Posterior predictive real mean_y_rep; // Mean of predictions for (n in 1:N) { y_rep[n] = normal_rng(matrix_times_vector(X[n], beta) + alpha[group[n]], sigma); } mean_y_rep = mean(to_vector(y_rep)); // Summary statistic } This model: - Uses matrix_times_vector to compute the linear predictor efficiently. - Employs to_vector to align group-level intercepts with observations. - Computes mean_y_rep in generated quantities using mean and to_vector for posterior diagnostics. - Generates predictions with normal_rng for posterior predictive checks. 16.6 Tips for Using Stan Functions Efficiency: Prefer vectorized operations like matrix_times_vector over loops for speed. Numerical Stability: Use log_sum_exp for summing exponentials to avoid overflow. Posterior Analysis: Leverage mean, sd, and to_vector in generated quantities for summaries and diagnostics. Constraints: Ensure inputs meet requirements (e.g., x &gt; 0 for log, positive-definite matrices for cholesky_decompose). Advanced Modeling: Use ode_rk45 for dynamic systems or map_rect for parallelized large-scale models. Documentation: The Stan Reference Manual (e.g., version 2.33) and Stan’s GitHub examples provide detailed guidance. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
