[["index.html", "Stan Bookdown Chapter 1 Choosing the Right Bayesian Model", " Stan Bookdown Kamran Afzali 2025-06-20 Chapter 1 Choosing the Right Bayesian Model Introduction Bayesian modeling has become a central approach in modern data analysis, providing a coherent framework for incorporating prior knowledge and quantifying uncertainty. With the advent of powerful tools such as Stan and user-friendly interfaces like the R package brms, practitioners can now implement a wide array of Bayesian models with relative ease. However, the flexibility of the Bayesian framework also introduces a new challenge: selecting the most appropriate model for a given dataset and research question. The landscape of Bayesian models is vast, encompassing linear and generalized linear models, robust and regularized regressions, hierarchical models, and more sophisticated approaches such as Gaussian processes and mixture models. This guide aims to offer a structured approach to model selection within the Bayesian paradigm, focusing on practical considerations, data characteristics, and modeling objectives. Modeling Objectives and Data Characteristics The choice of a Bayesian model should begin with a clear understanding of the research objective. In broad terms, the aim of modeling can be categorized into two primary goals: inference and prediction. Inference focuses on understanding the relationships between variables, quantifying uncertainty in parameter estimates, and testing theoretical hypotheses. Prediction, on the other hand, emphasizes the accuracy of forecasting outcomes for new observations. While the two goals are not mutually exclusive, they can lead to different modeling choices, particularly in terms of model complexity and regularization. Another factor influencing model selection is the nature of the data. Key aspects include the type of response variable (continuous, binary, count, categorical), the presence of outliers or heavy-tailed distributions, the structure of the data (e.g., hierarchical or longitudinal), and the dimensionality of the predictor space. A careful examination of these characteristics provides essential guidance for selecting an appropriate Bayesian model. Prior Specification and Computational Considerations An essential feature of Bayesian modeling is the specification of prior distributions. Priors can be informative, weakly informative, or non-informative, depending on the amount of domain knowledge available. Informative priors are grounded in expert knowledge or historical data, while weakly informative priors help stabilize estimates without unduly influencing the posterior. Prior predictive checks can assess the implications of the priors before seeing the data, ensuring they encode plausible assumptions. Modelers should also perform sensitivity analyses to understand how different priors affect inferences. Computational feasibility is another practical concern. Some Bayesian models—especially nonparametric or high-dimensional ones—can be computationally intensive, requiring advanced MCMC algorithms or variational inference. Diagnostics such as the Gelman-Rubin R-hat statistic, effective sample size (ESS), and checks for divergent transitions should be used to ensure reliable inference (Gelman et al., 2013). Stan and brms provide tools to assess convergence and evaluate sampling efficiency. Bayesian Linear Regression Bayesian linear regression serves as the foundational model in the Bayesian framework. It assumes a linear relationship between predictors and a continuous response variable, with normally distributed residuals. This model is particularly useful for its simplicity and interpretability. When the assumptions of linearity and normality hold reasonably well, Bayesian linear regression provides reliable parameter estimates and predictive intervals. It also serves as a baseline model against which more complex models can be compared. In practice, Bayesian linear regression can be implemented in Stan with straightforward model code, specifying priors for the regression coefficients and residual variance. The flexibility of Bayesian inference allows for the incorporation of prior knowledge, which can be particularly valuable in small-sample contexts or when strong domain expertise is available. Robust Regression for Non-Normal Residuals Real-world data often deviate from the assumption of normally distributed residuals. Outliers or heavy-tailed distributions can exert undue influence on parameter estimates, leading to biased or unstable results. Bayesian robust regression addresses this issue by modeling the residuals using a t-distribution, which has heavier tails than the normal distribution. This approach reduces the influence of outliers, leading to more robust and reliable inferences. The implementation of robust regression in Stan involves specifying a likelihood based on the t-distribution and including an additional parameter for the degrees of freedom. This parameter controls the heaviness of the tails and can itself be estimated from the data. The robust regression model is particularly recommended when residual diagnostics from a standard linear model indicate non-normality or the presence of extreme observations. Regularized Regression for High-Dimensional Data When dealing with a large number of predictors or multicollinearity, regularization becomes essential to prevent overfitting and to enhance predictive performance. Bayesian regularized regression models incorporate shrinkage priors, such as the Laplace prior for Bayesian LASSO or the Gaussian prior for Bayesian ridge regression. These priors shrink the regression coefficients toward zero, effectively performing variable selection and regularization. In the Bayesian framework, regularization is naturally integrated through the prior distribution. For example, the Bayesian LASSO uses a double-exponential prior that induces sparsity by assigning higher probability mass near zero. These models are particularly useful in settings with more predictors than observations or when there is a need to identify the most influential variables. Models for Non-Normal Data In many applications, the response variable does not follow a normal distribution. Binary outcomes, count data, and categorical responses require specialized models. Bayesian generalized linear models (GLMs) extend the linear model framework to accommodate different types of response variables through appropriate link functions and likelihood distributions. For binary outcomes, the logistic regression model with a logit link is commonly used. For count data, Poisson and negative binomial models are appropriate, with the latter providing a flexible alternative in the presence of overdispersion. Multinomial and ordinal regression models are used for categorical outcomes, with the choice depending on whether the categories are ordered. These models are readily implemented in Stan and brms, allowing users to specify the appropriate family and link function. Model selection in this context should be guided by the distributional characteristics of the response variable and the research question at hand. Multilevel and Hierarchical Models Hierarchical data structures are common in social sciences, education, and biomedical research. In such settings, observations are nested within higher-level units, such as students within schools or patients within hospitals. Ignoring this structure can lead to biased inferences and underestimated uncertainty. Bayesian multilevel models explicitly account for the hierarchical structure by including group-level effects. These models allow for partial pooling of information across groups, balancing between complete pooling (ignoring group differences) and no pooling (treating each group separately). The brms package offers a user-friendly interface for fitting multilevel models, handling complex random effects structures with ease. The flexibility of Bayesian multilevel modeling also facilitates the inclusion of varying slopes, cross-level interactions, and non-linear effects. When the data structure suggests hierarchical dependencies, multilevel modeling should be the default approach. Nonlinear and Nonparametric Models In some applications, the relationship between predictors and the response variable is inherently nonlinear or unknown. Bayesian nonparametric models, such as Gaussian process regression, offer a flexible solution by modeling the function space directly. Gaussian processes define a prior over functions and use observed data to update this prior, resulting in a posterior distribution over functions. Gaussian process regression is particularly powerful when the form of the relationship is unknown or when modeling smooth, nonlinear trends is important. However, it comes at a higher computational cost and may not scale well with large datasets. Nevertheless, for problems involving spatial data, temporal trends, or complex functional relationships, Gaussian processes provide a valuable modeling tool. Mixture Models and Latent Structure Data arising from heterogeneous populations may be better modeled using mixture models. Bayesian Gaussian mixture models, for instance, assume that the data are generated from a mixture of several Gaussian distributions, each representing a subpopulation. These models can uncover latent structure in the data, such as clusters or subtypes. Mixture models introduce additional complexity due to the need to estimate both the component parameters and the mixing proportions. Bayesian inference provides a principled framework for dealing with this uncertainty, often using techniques such as latent variable augmentation and label switching adjustments. When there is reason to believe that the data comprise distinct subgroups with different underlying characteristics, mixture models offer an effective approach to modeling such heterogeneity. Comparative Summary Table Model Type Use Case Key Assumptions Priors Limitations Linear Regression Continuous outcome, low noise Linearity, normal errors Normal, Inverse-Gamma Poor with outliers Robust Regression Heavy-tailed residuals t-distributed residuals Prior on ν Increased complexity Regularized Regression High-dimensional predictors Sparsity Laplace, Gaussian Shrinkage may hide effects GLMs Binary/count/categorical outcomes Appropriate link function Varied Can overfit without strong priors Hierarchical Models Nested/grouped data Partial pooling Hierarchical priors Sensitive to group size Gaussian Processes Unknown nonlinear function Smoothness in kernel GP prior Poor scaling (O(n³)) Mixture Models Latent structure/clustering Finite components Dirichlet, etc. Label switching, identifiability Model Diagnostics and Comparison Choosing the right model also involves evaluating its performance and comparing it to alternative specifications. Bayesian model diagnostics include posterior predictive checks, which assess how well the model reproduces the observed data. Graphical comparisons between observed and replicated data can reveal model misfit or systematic discrepancies. Information criteria such as the Widely Applicable Information Criterion (WAIC) and Leave-One-Out Cross-Validation (LOO-CV) provide tools for model comparison, balancing fit and complexity. These criteria estimate the expected out-of-sample predictive performance and are particularly useful for selecting among nested or non-nested models (Vehtari, Gelman, &amp; Gabry, 2017). Bayes factors offer another method for model comparison, based on the ratio of marginal likelihoods. However, they are sensitive to prior specification and can be computationally intensive. In practice, WAIC and LOO-CV are often preferred for their robustness and ease of computation. A Decision Framework for Model Selection To aid practitioners in selecting the appropriate Bayesian model, a structured decision framework can be employed. This framework begins with identifying the type of response variable: continuous, binary, count, or categorical. Next, the data should be assessed for features such as outliers, overdispersion, hierarchical structure, and nonlinearity. Based on these characteristics, the modeler can then choose among linear models, robust regressions, generalized linear models, multilevel models, or nonparametric approaches. This decision process is iterative and should incorporate model diagnostics and domain knowledge. Starting with a simple model and progressively introducing complexity allows for a more transparent understanding of the data and the modeling assumptions. Each modeling choice should be justified in terms of its contribution to answering the research question and improving model fit. Conclusion Bayesian modeling offers unparalleled flexibility and rigor in statistical inference, but this power comes with the responsibility of thoughtful model selection. This guide has outlined the key considerations for choosing among the diverse array of Bayesian models available in tools like Stan and brms. By grounding model selection in the objectives of the analysis, the characteristics of the data, and robust diagnostic procedures, practitioners can make informed choices that enhance both the interpretability and predictive performance of their models. As with all statistical modeling, the process is iterative and benefits from a combination of statistical insight, computational tools, and substantive expertise. With this guide, researchers are better equipped to navigate the Bayesian modeling landscape and apply the appropriate models to their specific challenges. "],["bayesian-modeling-in-r-and-stan.html", "Chapter 2 Bayesian Modeling in R and Stan 2.1 What is STAN? 2.2 Model file 2.3 Fit the model 2.4 MCMC diagnostics 2.5 Parting thoughts 2.6 Conclusions 2.7 References", " Chapter 2 Bayesian Modeling in R and Stan The aim of this post is to provide a quick overview and introduction to fitting Bayesian models using STAN and R. For this, I strongly recommend installing Rstudio, an integrated development environment that allows a “user-friendly” interaction with R. 2.1 What is STAN? STAN is a tool for analysing Bayesian models using Markov Chain Monte Carlo (MCMC) methods. MCMC is a sampling method for estimating a probability distribution without knowing all of the features of the distribution. STAN is a probabilistic programming language and free software for specifying statistical models utilising Hamiltonian Monte Carlo methods (HMC), a type of MCMC algorithm. Stan works with the most widely used data-analysis languages including R and Python. In this quick overview, we’ll focus on the rstan package and demonstrate how to fit STAN models with it. For an example dataset, here we simulate our own data in R. We firsty create a continuous outcome variable y as a function of one predictor x and a disturbance term ϵ. I simulate a dataset with 100 observations. Create the error term, the predictor and the outcome using a linear form with an intercept β0, slope β1, etc. coefficients. set.seed(123) n &lt;- 1000 a &lt;- 40 #intercept b &lt;- c(-2, 3, 4, 1 , 0.25) #slopes sigma2 &lt;- 25 #residual variance (sd=5) x &lt;- matrix(rnorm(5000),1000,5) eps &lt;- rnorm(n, mean = 0, sd = sqrt(sigma2)) #residuals y &lt;- a +x%*%b+ eps #response variable data &lt;- data.frame(y, x) #dataset head(data)%&gt;%kableExtra::kable() y X1 X2 X3 X4 X5 33.51510 -0.5604756 -0.9957987 -0.5116037 -0.1503075 0.1965498 43.76098 -0.2301775 -1.0399550 0.2369379 -0.3277571 0.6501132 27.64712 1.5587083 -0.0179802 -0.5415892 -1.4481653 0.6710042 50.72614 0.0705084 -0.1321751 1.2192276 -0.6972846 -1.2841578 39.46286 0.1292877 -2.5493428 0.1741359 2.5984902 -2.0261096 39.42009 1.7150650 1.0405735 -0.6152683 -0.0374150 2.2053261 2.2 Model file STAN models are written in an imperative programming language, which means the order in which you write the elements in your model file matters, i.e. you must first define your variables (e.g. integers, vectors, matrices, etc.), then the constraints that define the range of values your variable can take (e.g. only positive values for standard deviations), and finally the relationship between the variables. A Stan model is defined by different blocks including: Data (required): The data block reads information from the outside world, such as data vectors, matrices, integers, and so on. We also need to define the lengths and dimensions of objects, which may appear unusual to those who are used to R or Python. The number of observations is first declared as an integer variable N: int N; (note the use of semicolon to denote the end of a line). The number of predictors in our model, which is K. The intercept is included in this count, so we end up with two predictors (2 columns in the model matrix). Transformed Data (optional): The converted data block enables data preprocessing, such as data transformation or rescaling. Parameters (required): The parameters block specifies the parameters that must be assigned to prior distributions. Transformed parameters (optional): Before computing the posterior, the changed parameters block provides for parameter processing, such as transformation or rescaling of the parameters. model_stan = &quot; data { // declare the input data / parameters } transformed data { // optional - for transforming/scaling input data } parameters { // define model parameters } transformed parameters { // optional - for deriving additional non-model parameters // note however, as they are part of the sampling chain // transformed parameters slow sampling down. } model { // specifying priors and likelihood as well as the linear predictor } generated quantities { // optional - derivatives (posteriors) of the samples } &quot; cat(model_stan) ## ## data { ## // declare the input data / parameters ## } ## transformed data { ## // optional - for transforming/scaling input data ## } ## parameters { ## // define model parameters ## } ## transformed parameters { ## // optional - for deriving additional non-model parameters ## // note however, as they are part of the sampling chain ## // transformed parameters slow sampling down. ## } ## model { ## // specifying priors and likelihood as well as the linear predictor ## } ## generated quantities { ## // optional - derivatives (posteriors) of the samples ## } ## For this introduction, I’ll use a very simple model in the STAN model that only involves the specification of three blocks. In the data block, I declare the variables y and x as reals (or vectors) with length equal to N and declare the sample size n sim as a positive integer number using the phrase int n sim. I define the coefficients for the linear regression alpha and beta (as real values) and the standard deviation parameter sigma in the parameters block (as a positive real number). Finally, in the model block, I give the regression coefficients and standard deviation parameters weakly informative priors, and I use a normal distribution indexed by the conditional mean mu and standard deviation sigma parameters to model the outcome data y. I give full recognition to McElreath’s outstanding Statistical Rethinking (2020) book for this section. stan_mod = &quot;data { int&lt;lower=0&gt; N; // number of observations int&lt;lower=0&gt; K; // number of predictors matrix[N, K] X; // predictor matrix vector[N] y; // outcome vector } parameters { real alpha; // intercept vector[K] beta; // coefficients for predictors real&lt;lower=0&gt; sigma; // error scale } model { y ~ normal(alpha + X * beta, sigma); // target density }&quot; writeLines(stan_mod, con = &quot;stan_mod.stan&quot;) cat(stan_mod) 2.3 Fit the model We’ll need to organise the information into a list for Stan. This list should contain everything we defined in the data block of our Stan code. Then it is possible to run the model with the stan function. library(tidyverse) predictors &lt;- data %&gt;% select(-y) stan_data &lt;- list( N = 1000, K = 5, X = predictors, y = data$y ) fit_rstan &lt;- rstan::stan( file = &quot;stan_mod.stan&quot;, data = stan_data ) fit_rstan ## Inference for Stan model: anon_model. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% ## alpha 40.19 0.00 0.15 39.89 40.08 40.19 40.29 40.48 ## beta[1] -2.07 0.00 0.16 -2.39 -2.18 -2.07 -1.97 -1.76 ## beta[2] 2.73 0.00 0.16 2.42 2.62 2.73 2.84 3.04 ## beta[3] 3.83 0.00 0.16 3.51 3.72 3.83 3.93 4.13 ## beta[4] 1.23 0.00 0.16 0.92 1.11 1.23 1.34 1.54 ## beta[5] 0.34 0.00 0.16 0.04 0.23 0.34 0.45 0.65 ## sigma 4.96 0.00 0.11 4.74 4.88 4.95 5.03 5.17 ## lp__ -2098.32 0.04 1.88 -2102.75 -2099.39 -2097.99 -2096.92 -2095.67 ## n_eff Rhat ## alpha 5280 1 ## beta[1] 5552 1 ## beta[2] 6251 1 ## beta[3] 5988 1 ## beta[4] 6172 1 ## beta[5] 6343 1 ## sigma 5687 1 ## lp__ 1916 1 ## ## Samples were drawn using NUTS(diag_e) at Wed May 4 13:23:35 2022. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 2.4 MCMC diagnostics For Bayesian analysis, it is required to investigate the properties of the MCMC chains and the sampler in general, in addition to the standard model diagnostic checks (such as residual plots). Remember that the goal of MCMC sampling is to reproduce the posterior distribution of the model likelihood and priors by formulating a probability distribution. This method reliable, only if the MCMC samples accurately reflect the posterior. For each parameter, traceplots show the MCMC sample values after each iteration along the chain. Bad chain mixing (any pattern) indicates that the MCMC sample chains may not have spanned all aspects of the posterior distribution and that further iterations are needed to ensure that the distribution is accurately represented. Each parameter’s autocorrelation graphic shows the degree of correlation between MCMC samples separated by different lags. The degree of correlation between each MCMC sample and itself, for example, is represented by a lag of (obviously this will be a correlation of ). The MCMC samples should be independent in order to obtain unbiased parameter estimations (uncorrelated). For each parameter, the potential scale reduction factor (Rhat) statistic offers a measure of sampling efficiency/effectiveness. All values should, in theory, be less than 1, if the sampler has values of or greater than 1, it is likely that it was not particularly efficient or effective. A misspecified model or extremely unclear priors that lead to misspecified parameter space might lead to this. We should have looked at the convergence diagnostics before looking at the summaries. For the effects model, we utilise the package mcmcplots to generate density and trace graphs. It’s crucial to evaluate if the chains have converged when using MCMC to fit a model. To visually inspect MCMC diagnostics, we propose the bayesplot software. The bayesplot package includes routines for displaying MCMC diagnostics and supports model objects from both rstan and rstanarm. We’ll show how to make a trace plot with the mcmc_trace() function and a plot of Rhat values with the mcmc rhat() function. By printing the model fit, we can examine the parameters in the console. For each parameter, we derive posterior means, standard errors, and quantiles. n eff and Rhat are two other terms we use. These are the results of Stan’s engine’s exploration of the parameter space. For the time being, it’s enough to know that when Rhat is 1, everything is well. fit_rstan %&gt;% mcmc_trace() fit_rstan %&gt;% rhat() %&gt;% mcmc_rhat() + yaxis_text() 2.5 Parting thoughts To finish this post, I’d like to point out that the rstanarm package makes it possible to fit STAN models without having to write them down and instead using standard R syntax, such as that found in a glm(). So, what’s the point of learning all this STAN jargon? It depends: if you’re merely fitting “classical” models to your data with no fanfare, just use rstanarm; it’ll save you time and the models in this package are unquestionably better parametrized (i.e. faster) than the one I presented here. Learning STAN, on the other hand, is a good approach to get into a very flexible and strong language that will continue to evolve if you believe you will need to fit your own models one day. rstanarm is a package that acts as a user interface for Stan on the front end, and enables R users to create Bayesian models without needing to learn how to code in Stan. Using the standard formula and data, you can fit a model in rstanarm. If you want to use rstan to fit a different model type, you’ll have to code it yourself. The prefix stan_ precedes the model fitting functions and is followed by the model type. stan glm() and stan glmer() are two examples. A complete list of rstanarm functions can be found on Cran’s package guide. 2.6 Conclusions This tutorial provided only a quick overview of how to fit simple linear regression models with the Bayesian software STAN and the rstan library and how to get a collection of useful summaries from the models. This is just a taste of the many models that STAN can fit; in future postings, we’ll look at generalised linear models, as well as non-normal models with various link functions and hierarchical models. The STAN model as provided here is quite adaptable and able to accommodate datasets of various sizes. Although this may appear to be a more difficult technique than just fitting a linear model in a frequentist framework, the real benefits of Bayesian methods become apparent as the analysis becomes more sophisticated (which is often the case in real applications), the flexibility of Bayesian modelling makes it very simple to account for increasingly complicated models. 2.7 References Stan Functions Reference Stan Users Guide Some things i’ve learned about stan Stan (+R) Workshop# The pool of tears "],["bayesian-regression-models-for-non-normal-data.html", "Chapter 3 Bayesian Regression Models for Non-Normal Data 3.1 Logistic Regression 3.2 Negative Binomial 3.3 Conclusion 3.4 References", " Chapter 3 Bayesian Regression Models for Non-Normal Data Our last post covered how to do bayesian regression for normally distributed data in R using STAN. In this post, we’ll take a look at how to fit a regression model adapted to non-normal model in STAN using two common distributions seen in empirical data, namely, binomial and negative-binomial. As mentioned before in Bayesian modelling we use a set of sampling methods known as Markov Chain Monte Carlo (MCMC), we define a statistical model and find probabilistic estimates for the parameters. 3.1 Logistic Regression The likelihood of a binary outcome, such as pass or fail, is estimated using logistic models (but this model can be extended to include more than two outcomes). This is accomplished by using the logit function to convert a standard regression. The main parameter that we focus on here describes odds of the outcome derived from probabilities and transformed using a logit function. The following is the code for a logistic regression model with one predictor and an intercept. library(tidyverse) library(kableExtra) library(arm) library(emdbook) library(rstan) library(rstanarm) set.seed(1234) x1 = rnorm(10000) z = 1 + 2*x1 pr = 1/(1+exp(-z)) y = rbinom(10000,1,pr) df = data.frame(y=y,x1=x1) head(df)%&gt;%kableExtra::kable() y x1 0 -1.2070657 0 0.2774292 1 1.0844412 0 -2.3456977 1 0.4291247 1 0.5060559 glm( y~x1,data=df,family=&quot;binomial&quot;)%&gt;%summary()%&gt;%pluck(coefficients)%&gt;%kableExtra::kable() Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.036922 0.0294863 35.16621 0 x1 1.979352 0.0417219 47.44162 0 model_stan = &quot; data { int&lt;lower=0&gt; N; vector[N] x; int&lt;lower=0,upper=1&gt; y[N]; } parameters { real alpha; real beta; } model { y ~ bernoulli_logit(alpha + beta * x); } &quot; writeLines(model_stan, con = &quot;model_stan.stan&quot;) cat(model_stan) ## ## data { ## int&lt;lower=0&gt; N; ## vector[N] x; ## int&lt;lower=0,upper=1&gt; y[N]; ## } ## parameters { ## real alpha; ## real beta; ## } ## model { ## y ~ bernoulli_logit(alpha + beta * x); ## } ## stan_data &lt;- list( N = 10000, x = df$x1, y = df$y ) fit_rstan &lt;- rstan::stan( file = &quot;model_stan.stan&quot;, data = stan_data ) fit_rstan ## Inference for Stan model: anon_model. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff ## alpha 1.04 0.00 0.03 0.98 1.02 1.04 1.06 1.10 1936 ## beta 1.98 0.00 0.04 1.90 1.95 1.98 2.01 2.06 1804 ## lp__ -4339.45 0.03 1.05 -4342.29 -4339.80 -4339.12 -4338.74 -4338.49 1479 ## Rhat ## alpha 1 ## beta 1 ## lp__ 1 ## ## Samples were drawn using NUTS(diag_e) at Tue Jun 21 14:03:59 2022. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). This also can be expanded to several predictors. Below you can find the code as well as some recommendations for making sense of priors. set.seed(1234) x1 = rnorm(10000) x2 = rnorm(10000) x3 = rnorm(10000) z = 1 + 2*x1 + 3*x2 - 1*x3 pr = 1/(1+exp(-z)) y = rbinom(10000,1,pr) df2 = data.frame(y=y,x1=x1,x2=x2,x3=x3) head(df2)%&gt;%kableExtra::kable() y x1 x2 x3 0 -1.2070657 -1.8168975 -1.6878627 1 0.2774292 0.6271668 -0.9552011 1 1.0844412 0.5180921 -0.6480572 0 -2.3456977 0.1409218 0.2610342 1 0.4291247 1.4572719 -1.2196940 1 0.5060559 -0.4935965 -1.5501888 glm( y~x1+x2+x3,data=df2,family=&quot;binomial&quot;)%&gt;%summary()%&gt;%pluck(coefficients)%&gt;%kableExtra::kable() Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.0108545 0.0367817 27.48253 0 x1 1.9471836 0.0494298 39.39289 0 x2 2.9598129 0.0641890 46.11088 0 x3 -0.9448316 0.0372918 -25.33619 0 model_stan2 =&quot; data { int&lt;lower=0&gt; N; // number of observations int&lt;lower=0&gt; K; // number of predictors matrix[N, K] X; // predictor matrix int&lt;lower=0,upper=1&gt; y[N]; // outcome vector } parameters { real alpha; // intercept vector[K] beta; // coefficients for predictors } model { y ~ bernoulli_logit(alpha + X * beta); } &quot; writeLines(model_stan2, con = &quot;model_stan2.stan&quot;) cat(model_stan2) ## ## ## ## data { ## int&lt;lower=0&gt; N; // number of observations ## int&lt;lower=0&gt; K; // number of predictors ## matrix[N, K] X; // predictor matrix ## int&lt;lower=0,upper=1&gt; y[N]; // outcome vector ## } ## parameters { ## real alpha; // intercept ## vector[K] beta; // coefficients for predictors ## } ## model { ## y ~ bernoulli_logit(alpha + X * beta); ## } predictors &lt;- df2[,2:4] stan_data2 &lt;- list( N = 10000, K = 3, X = predictors, y = df2$y ) fit_rstan2 &lt;- rstan::stan( file = &quot;model_stan2.stan&quot;, data = stan_data2 ) ## Trying to compile a simple C file fit_rstan2 ## Inference for Stan model: anon_model. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% ## alpha 1.01 0.00 0.04 0.94 0.99 1.01 1.04 1.08 ## beta[1] 1.95 0.00 0.05 1.85 1.92 1.95 1.98 2.05 ## beta[2] 2.96 0.00 0.06 2.84 2.92 2.96 3.00 3.09 ## beta[3] -0.95 0.00 0.04 -1.02 -0.97 -0.95 -0.92 -0.87 ## lp__ -3006.39 0.03 1.38 -3009.82 -3007.07 -3006.08 -3005.37 -3004.68 ## n_eff Rhat ## alpha 2845 1 ## beta[1] 2338 1 ## beta[2] 2221 1 ## beta[3] 2160 1 ## lp__ 1788 1 ## ## Samples were drawn using NUTS(diag_e) at Tue Jun 21 14:05:38 2022. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 3.2 Negative Binomial The Poisson distribution, which assumes that the variance is equal to the mean, is a popular choice for modelling count data. The data are said to be overdispersed when the variance exceeds the mean, the Negative Binomial distribution can be used in this case. Note that the negative binomial distribution is a probability distribution of discrete random variables. Let’s say we have a response variable y that has a negative binomial distribution and is influenced by a collection of k explanatory variables X. The negative binomial distribution to model this data has two parameters: The μ or the expected value that need to be positive so a log link function can be used to map the linear predictor (the explanatory variables times the regression parameters) and ϕ which is the overdispersion parameter, where a small value means a large deviation from a Poisson distribution, as ϕ gets larger the negative binomial looks more and more like a Poisson distribution. Let’s simulate some data and fit a STAN model to them: N&lt;-100000 df3 &lt;-data.frame(x1=runif(N,-2,2),x2=runif(N,-2,2)) #the model X&lt;-model.matrix(~x1*x2,df3) K&lt;-dim(X)[2] #number of regression params #the regression slopes betas&lt;-runif(K,-1,1) #the overdispersion for the simulated data phi&lt;-5 #simulate the response y_nb&lt;-rnbinom(N,size=phi,mu=exp(X%*%betas)) hist(y_nb) MASS::glm.nb(y_nb ~ X[,2:K])%&gt;%summary()%&gt;%pluck(coefficients)%&gt;%kableExtra::kable() Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.0788902 0.0039192 20.12938 0 X[, 2:K]x1 -0.0815968 0.0032703 -24.95109 0 X[, 2:K]x2 0.7941334 0.0031740 250.20341 0 X[, 2:K]x1:x2 0.4301282 0.0026264 163.77308 0 data { int N; //the number of observations int K; //the number of columns in the model matrix int y[N]; //the response matrix[N,K] X; //the model matrix } parameters { vector[K] beta; //the regression parameters real phi; //the overdispersion parameters } transformed parameters { vector[N] mu;//the linear predictor mu &lt;- exp(X*beta); //using the log link } model { beta[1] ~ cauchy(0,10); //prior for the intercept following Gelman 2008 for(i in 2:K) beta[i] ~ cauchy(0,2.5);//prior for the slopes following Gelman 2008 y ~ neg_binomial_2(mu,phi); } generated quantities { vector[N] y_rep; for(n in 1:N){ y_rep[n] &lt;- neg_binomial_2_rng(mu[n],phi); //posterior draws to get posterior predictive checks } } stan_mod = &quot;data { int N; //the number of observations int K; //the number of columns in the model matrix int y[N]; //the response matrix[N,K] X; //the model matrix } parameters { vector[K] beta; //the regression parameters real phi; //the overdispersion parameters } transformed parameters { vector[N] mu;//the linear predictor mu &lt;- exp(X*beta); //using the log link } model { beta[1] ~ cauchy(0,10); //prior for the intercept following Gelman 2008 for(i in 2:K) beta[i] ~ cauchy(0,2.5);//prior for the slopes following Gelman 2008 y ~ neg_binomial_2(mu,phi); } &quot; writeLines(stan_mod, con = &quot;stan_mod.stan&quot;) cat(stan_mod) ## data { ## int N; //the number of observations ## int K; //the number of columns in the model matrix ## int y[N]; //the response ## matrix[N,K] X; //the model matrix ## } ## parameters { ## vector[K] beta; //the regression parameters ## real phi; //the overdispersion parameters ## } ## transformed parameters { ## vector[N] mu;//the linear predictor ## mu &lt;- exp(X*beta); //using the log link ## } ## model { ## beta[1] ~ cauchy(0,10); //prior for the intercept following Gelman 2008 ## ## for(i in 2:K) ## beta[i] ~ cauchy(0,2.5);//prior for the slopes following Gelman 2008 ## ## y ~ neg_binomial_2(mu,phi); ## } stan_data3 &lt;- list( N = N, K = K, X = X, y = y_nb ) fit_rstan3 &lt;- rstan::stan( file = &quot;stan_mod.stan&quot;, data = stan_data3 ) fit_rstan3%&gt;%summary() ## $summary ## mean se_mean sd 2.5% 25% ## beta[1] 7.899069e-02 6.825217e-05 0.0039010484 7.162531e-02 7.635923e-02 ## beta[2] -8.157657e-02 5.689044e-05 0.0032236243 -8.786688e-02 -8.376046e-02 ## beta[3] 7.941039e-01 5.618629e-05 0.0031476117 7.880475e-01 7.919520e-01 ## beta[4] 4.301036e-01 4.552321e-05 0.0026265744 4.248578e-01 4.283603e-01 ## phi 4.908958e+00 2.039012e-03 0.0831883094 4.745765e+00 4.854542e+00 3.3 Conclusion This tutorial provided only a quick overview of how to fit logistic and negative binomial regression models with the Bayesian software STAN using the rstan library/API and to extract a collection of useful summaries from the models. Future postings will address the question of outliers and the use of robust linear models. 3.4 References Stan Functions Reference Stan Users Guide Some things i’ve learned about stan Stan (+R) Workshop "],["robust-t-regression.html", "Chapter 4 Robust t-regression 4.1 Motivation 4.2 Concepts and code 4.3 Conclusion 4.4 References", " Chapter 4 Robust t-regression 4.1 Motivation Simple linear regression is a widely used method for estimating the linear relation between two (or more) variables and for predicting the value of one variable (the response variable) based on the value of the other (the explanatory variable). The explanatory variable is typically represented on the x-axis and the response variable on the y-axis for visualising the results of linear regression. The regression coefficients can be distorted by outlying data points. Due to the normality assumption used by traditional regression techniques, noisy or outlier data can greatly affect their accuracy. Since the normal distribution must move to a new place in the parameter space to best accommodate the outliers in the data, this frequently leads to an underestimate of the relationship between the variables. In a frequentist framework, creating a linear regression model that is resistant to outliers necessitates the use of very complex statistical techniques; however, in a Bayesian framework, we can achieve robustness by simply using the t-distribution. in this context, Robust Regression refers to regression methods which are less sensitive to outliers. Bayesian robust regression uses distributions with wider tails than the normal. This means that outliers will have less of an affect on the models and the regression line would need to move less incorporate those observations since the error distribution will not consider them as unusual. Utilizing Student’s t density with an unidentified degrees of freedom parameter is a well-liked substitution for normal errors in regression investigations. The Student’s t distribution has heavier tails than the normal for low degrees of freedom, but it leans toward the normal as the degrees of freedom parameter rises. A check on the suitability of the normal is thus made possible by treating the degrees of freedom parameter as an unknown quantity that must be approximated. The degrees of freedom, or parameter ν, of this probability distribution determines how close to normal the distribution is: Low small values of produce a distribution with thicker tails (that is, a greater spread around the mean) than the normal distribution, but big values of (approximately &gt; 30) give a distribution that is quite similar to the normal distribution. As a result, we can allow the distribution of the regression line to be as normal or non-normal as the data imply while still capturing the underlying relationship between the variables by substituting the normal distribution above with a t-distribution and adding as an extra parameter to the model. 4.2 Concepts and code The standard approach to linear regression is defining the equation for a straight line that represents the relationship between the variables as accurately as possible. The equation for the line defines y (the response variable) as a linear function of x (the explanatory variable): 𝑦 = 𝛼 + 𝛽𝑥 + 𝜀 In this equation, ε represents the error in the linear relationship: if no noise were allowed, then the paired x- and y-values would need to be arranged in a perfect straight line (for example, as in y = 2x + 1). Because we assume that the relationship between x and y is truly linear, any variation observed around the regression line must be random noise, and therefore normally distributed. From a probabilistic standpoint, such relationship between the variables could be formalised as 𝑦 ~ 𝓝(𝛼 + 𝛽𝑥, 𝜎) That is, the response variable follows a normal distribution with mean equal to the regression line, and some standard deviation σ. Such a probability distribution of the regression line is illustrated in the figure below. The formulation of the robust simple linear regression Bayesian model is given below. We define a t likelihood for the response variable, y, and suitable vague priors on all the model parameters: normal for α and β, half-normal for σ and gamma for ν. 𝑦 ~ 𝓣(𝛼 + 𝛽𝑥, 𝜎, 𝜈) 𝛼, 𝛽 ~ 𝓝(0, 1000) 𝜎 ~ 𝓗𝓝(0, 1000) 𝜈 ~ 𝚪(2, 0.1) Below you can find R and Stan code for a simple Bayesian t-regression model with nu unknown. First let’s create data with and without ourliers library(readr) library(tidyverse) library(gridExtra) library(kableExtra) library(arm) library(emdbook) library(rstan) library(rstanarm) library(brms) s &lt;- matrix(c(1, .8, .8, 1), nrow = 2, ncol = 2) m &lt;- c(3, 3) set.seed(1234) data_n &lt;- MASS::mvrnorm(n = 100, mu = m, Sigma = s) %&gt;% as_tibble() %&gt;% rename(y = V1, x = V2) data_n &lt;- data_n %&gt;% arrange(x) head(data_n)%&gt;%kableExtra::kable() y x 0.9335732 0.6157783 1.0317982 0.8318674 1.9763140 0.9326985 2.3439117 1.1117327 1.4059413 1.1673553 2.0360302 1.3253002 data_o &lt;- data_n data_o[c(1:2), 1] &lt;- c(7.5, 8.5) head(data_o)%&gt;%kableExtra::kable() y x 7.500000 0.6157783 8.500000 0.8318674 1.976314 0.9326985 2.343912 1.1117327 1.405941 1.1673553 2.036030 1.3253002 ols_n &lt;- lm(data = data_n, y ~ 1 + x) ols_o &lt;- lm(data = data_o, y ~ 1 + x) p1 &lt;- ggplot(data = data_n, aes(x = x, y = y)) + stat_smooth(method = &quot;lm&quot;, color = &quot;grey92&quot;, fill = &quot;grey67&quot;, alpha = 1, fullrange = T) + geom_point(size = 1, alpha = 3/4) + scale_x_continuous(limits = c(0, 9)) + coord_cartesian(xlim = c(0, 9), ylim = c(0, 9)) + labs(title = &quot;No Outliers&quot;) + theme(panel.grid = element_blank()) # the data with two outliers p2 &lt;- ggplot(data = data_o, aes(x = x, y = y, color = y &gt; 7)) + stat_smooth(method = &quot;lm&quot;, color = &quot;grey92&quot;, fill = &quot;grey67&quot;, alpha = 1, fullrange = T) + geom_point(size = 1, alpha = 3/4) + scale_color_viridis_d(option = &quot;A&quot;, end = 4/7) + scale_x_continuous(limits = c(0, 9)) + coord_cartesian(xlim = c(0, 9), ylim = c(0, 9)) + labs(title = &quot;Two Outliers&quot;) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) grid.arrange(p1 ,p2) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; model_stan = &quot; data { int&lt;lower=1&gt; N; int&lt;lower=0&gt; M; int&lt;lower=0&gt; P; vector[N] x; vector[N] y; vector[M] x_cred; vector[P] x_pred; } parameters { real alpha; real beta; real&lt;lower=0&gt; sigma; real&lt;lower=1&gt; nu; } transformed parameters { vector[N] mu = alpha + beta * x; vector[M] mu_cred = alpha + beta * x_cred; vector[P] mu_pred = alpha + beta * x_pred; } model { y ~ student_t(nu, mu, sigma); alpha ~ normal(0, 1000); beta ~ normal(0, 1000); sigma ~ normal(0, 1000); nu ~ gamma(2, 0.1); } generated quantities { real y_pred[P]; for (p in 1:P) { y_pred[p] = student_t_rng(nu, mu_pred[p], sigma); } } &quot; writeLines(model_stan, con = &quot;model_stan.stan&quot;) cat(model_stan) ## ## data { ## int&lt;lower=1&gt; N; ## int&lt;lower=0&gt; M; ## int&lt;lower=0&gt; P; ## vector[N] x; ## vector[N] y; ## vector[M] x_cred; ## vector[P] x_pred; ## } ## ## parameters { ## real alpha; ## real beta; ## real&lt;lower=0&gt; sigma; ## real&lt;lower=1&gt; nu; ## } ## ## transformed parameters { ## vector[N] mu = alpha + beta * x; ## vector[M] mu_cred = alpha + beta * x_cred; ## vector[P] mu_pred = alpha + beta * x_pred; ## } ## ## model { ## y ~ student_t(nu, mu, sigma); ## alpha ~ normal(0, 1000); ## beta ~ normal(0, 1000); ## sigma ~ normal(0, 1000); ## nu ~ gamma(2, 0.1); ## } ## ## generated quantities { ## real y_pred[P]; ## for (p in 1:P) { ## y_pred[p] = student_t_rng(nu, mu_pred[p], sigma); ## } ## } ## stan_data &lt;- list(x=data_o$x, y=data_o$y, N=length(data_o$y), M=0, P=0, x_cred=numeric(0), x_pred=numeric(0)) fit_rstan &lt;- rstan::stan( file = &quot;model_stan.stan&quot;, data = stan_data ) ## Trying to compile a simple C file ## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c ## clang -mmacosx-version-min=10.13 -I&quot;/Library/Frameworks/R.framework/Resources/include&quot; -DNDEBUG -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/unsupported&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/BH/include&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/src/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppParallel/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/rstan/include&quot; -DEIGEN_NO_DEBUG -DBOOST_DISABLE_ASSERTS -DBOOST_PENDING_INTEGER_LOG2_HPP -DSTAN_THREADS -DUSE_STANC3 -DSTRICT_R_HEADERS -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION -DBOOST_NO_AUTO_PTR -include &#39;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp&#39; -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1 -I/usr/local/include -fPIC -Wall -g -O2 -c foo.c -o foo.o ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:88: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name &#39;namespace&#39; ## namespace Eigen { ## ^ ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected &#39;;&#39; after top level declarator ## namespace Eigen { ## ^ ## ; ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found ## #include &lt;complex&gt; ## ^~~~~~~~~ ## 3 errors generated. ## make: *** [foo.o] Error 1 ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 3.9e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.39 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.114 seconds (Warm-up) ## Chain 1: 0.122 seconds (Sampling) ## Chain 1: 0.236 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 1.3e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.126 seconds (Warm-up) ## Chain 2: 0.11 seconds (Sampling) ## Chain 2: 0.236 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 9e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.122 seconds (Warm-up) ## Chain 3: 0.111 seconds (Sampling) ## Chain 3: 0.233 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 1.1e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.134 seconds (Warm-up) ## Chain 4: 0.113 seconds (Sampling) ## Chain 4: 0.247 seconds (Total) ## Chain 4: trace &lt;- stan_trace(fit_rstan, pars=c(&quot;alpha&quot;, &quot;beta&quot;, &quot;sigma&quot;, &quot;nu&quot;)) trace + scale_color_brewer(type = &quot;div&quot;) + theme(legend.position = &quot;none&quot;) ## Scale for &#39;colour&#39; is already present. Adding another scale for &#39;colour&#39;, ## which will replace the existing scale. stan_dens(fit_rstan, pars=c(&quot;alpha&quot;, &quot;beta&quot;, &quot;sigma&quot;, &quot;nu&quot;), fill = &quot;skyblue&quot;) stan_plot(fit_rstan, pars=c(&quot;alpha&quot;, &quot;beta&quot;, &quot;sigma&quot;, &quot;nu&quot;), show_density = TRUE, fill_color = &quot;maroon&quot;) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) Bayesian regression models can be used to estimate highest posterior density or credible intervals (intervals of the distribution of the regression linefor), and prediction values, through predictive posterior distributions. More specifically, the prediction intervals are obtained by first drawing samples of the mean response at specific x-values of interest and then, for each of these samples, randomly selecting a y-value from a t-distribution with location mu pred. In contrast, the credible intervals are obtained by drawing MCMC samples of the mean response at regularly spaced points along the x-axis. The distributions of mu cred and y pred are represented, respectively, by the credible and prediction intervals. x.cred = seq(from=min(data_o$x), to=max(data_o$x), length.out=50) x.pred = c(0, 8) stan_data2 &lt;- list(x=data_o$x, y=data_o$y, N=length(data_o$y), x_cred=x.cred, x_pred=x.pred, M=length(x.cred), P=length(x.pred)) fit_rstan2 &lt;- rstan::stan( file = &quot;model_stan.stan&quot;, data = stan_data2 ) ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 2.6e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.157 seconds (Warm-up) ## Chain 1: 0.134 seconds (Sampling) ## Chain 1: 0.291 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 1e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.148 seconds (Warm-up) ## Chain 2: 0.101 seconds (Sampling) ## Chain 2: 0.249 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 1.4e-05 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.165 seconds (Warm-up) ## Chain 3: 0.132 seconds (Sampling) ## Chain 3: 0.297 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 1.2e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.151 seconds (Warm-up) ## Chain 4: 0.145 seconds (Sampling) ## Chain 4: 0.296 seconds (Total) ## Chain 4: For each value in x.cred, the mu cred parameter’s MCMC samples are contained in a separate column of mu.cred. In a similar manner, the columns of y.pred include the MCMC samples of the posterior expected response values (y pred values) for the x-values in x.pred. summary(extract(fit_rstan2, &quot;mu_cred&quot;)[[1]])%&gt;%kableExtra::kable() V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 V29 V30 V31 V32 V33 V34 V35 V36 V37 V38 V39 V40 V41 V42 V43 V44 V45 V46 V47 V48 V49 V50 Min. :0.4535 Min. :0.553 Min. :0.6525 Min. :0.752 Min. :0.8515 Min. :0.951 Min. :1.051 Min. :1.150 Min. :1.250 Min. :1.349 Min. :1.449 Min. :1.548 Min. :1.642 Min. :1.734 Min. :1.826 Min. :1.918 Min. :2.010 Min. :2.102 Min. :2.194 Min. :2.285 Min. :2.377 Min. :2.458 Min. :2.531 Min. :2.603 Min. :2.675 Min. :2.747 Min. :2.820 Min. :2.892 Min. :2.964 Min. :3.037 Min. :3.109 Min. :3.181 Min. :3.245 Min. :3.297 Min. :3.349 Min. :3.401 Min. :3.453 Min. :3.505 Min. :3.557 Min. :3.609 Min. :3.661 Min. :3.713 Min. :3.765 Min. :3.817 Min. :3.869 Min. :3.921 Min. :3.973 Min. :4.025 Min. :4.077 Min. :4.129 1st Qu.:0.9464 1st Qu.:1.029 1st Qu.:1.1113 1st Qu.:1.193 1st Qu.:1.2762 1st Qu.:1.359 1st Qu.:1.441 1st Qu.:1.524 1st Qu.:1.605 1st Qu.:1.688 1st Qu.:1.769 1st Qu.:1.851 1st Qu.:1.931 1st Qu.:2.012 1st Qu.:2.093 1st Qu.:2.174 1st Qu.:2.254 1st Qu.:2.335 1st Qu.:2.415 1st Qu.:2.495 1st Qu.:2.574 1st Qu.:2.654 1st Qu.:2.733 1st Qu.:2.810 1st Qu.:2.888 1st Qu.:2.966 1st Qu.:3.043 1st Qu.:3.119 1st Qu.:3.195 1st Qu.:3.270 1st Qu.:3.344 1st Qu.:3.419 1st Qu.:3.493 1st Qu.:3.569 1st Qu.:3.644 1st Qu.:3.717 1st Qu.:3.792 1st Qu.:3.867 1st Qu.:3.942 1st Qu.:4.015 1st Qu.:4.089 1st Qu.:4.163 1st Qu.:4.237 1st Qu.:4.311 1st Qu.:4.386 1st Qu.:4.460 1st Qu.:4.534 1st Qu.:4.608 1st Qu.:4.681 1st Qu.:4.753 Median :1.0605 Median :1.138 Median :1.2165 Median :1.295 Median :1.3724 Median :1.450 Median :1.528 Median :1.606 Median :1.684 Median :1.762 Median :1.840 Median :1.918 Median :1.996 Median :2.074 Median :2.152 Median :2.230 Median :2.308 Median :2.386 Median :2.463 Median :2.542 Median :2.619 Median :2.698 Median :2.775 Median :2.853 Median :2.931 Median :3.010 Median :3.087 Median :3.165 Median :3.243 Median :3.322 Median :3.400 Median :3.478 Median :3.557 Median :3.634 Median :3.713 Median :3.790 Median :3.868 Median :3.947 Median :4.024 Median :4.102 Median :4.180 Median :4.257 Median :4.335 Median :4.413 Median :4.491 Median :4.569 Median :4.647 Median :4.726 Median :4.803 Median :4.881 Mean :1.0605 Mean :1.138 Mean :1.2164 Mean :1.294 Mean :1.3723 Mean :1.450 Mean :1.528 Mean :1.606 Mean :1.684 Mean :1.762 Mean :1.840 Mean :1.918 Mean :1.996 Mean :2.074 Mean :2.152 Mean :2.230 Mean :2.308 Mean :2.386 Mean :2.464 Mean :2.542 Mean :2.619 Mean :2.697 Mean :2.775 Mean :2.853 Mean :2.931 Mean :3.009 Mean :3.087 Mean :3.165 Mean :3.243 Mean :3.321 Mean :3.399 Mean :3.477 Mean :3.555 Mean :3.633 Mean :3.711 Mean :3.789 Mean :3.867 Mean :3.945 Mean :4.023 Mean :4.101 Mean :4.178 Mean :4.256 Mean :4.334 Mean :4.412 Mean :4.490 Mean :4.568 Mean :4.646 Mean :4.724 Mean :4.802 Mean :4.880 3rd Qu.:1.1739 3rd Qu.:1.247 3rd Qu.:1.3213 3rd Qu.:1.394 3rd Qu.:1.4688 3rd Qu.:1.543 3rd Qu.:1.616 3rd Qu.:1.691 3rd Qu.:1.766 3rd Qu.:1.840 3rd Qu.:1.915 3rd Qu.:1.989 3rd Qu.:2.063 3rd Qu.:2.137 3rd Qu.:2.212 3rd Qu.:2.286 3rd Qu.:2.362 3rd Qu.:2.436 3rd Qu.:2.512 3rd Qu.:2.587 3rd Qu.:2.663 3rd Qu.:2.740 3rd Qu.:2.817 3rd Qu.:2.895 3rd Qu.:2.973 3rd Qu.:3.052 3rd Qu.:3.132 3rd Qu.:3.212 3rd Qu.:3.292 3rd Qu.:3.372 3rd Qu.:3.453 3rd Qu.:3.535 3rd Qu.:3.616 3rd Qu.:3.698 3rd Qu.:3.779 3rd Qu.:3.861 3rd Qu.:3.943 3rd Qu.:4.025 3rd Qu.:4.107 3rd Qu.:4.189 3rd Qu.:4.271 3rd Qu.:4.353 3rd Qu.:4.436 3rd Qu.:4.518 3rd Qu.:4.600 3rd Qu.:4.682 3rd Qu.:4.765 3rd Qu.:4.847 3rd Qu.:4.929 3rd Qu.:5.012 Max. :1.6368 Max. :1.697 Max. :1.7566 Max. :1.817 Max. :1.8764 Max. :1.936 Max. :1.996 Max. :2.056 Max. :2.116 Max. :2.176 Max. :2.236 Max. :2.296 Max. :2.356 Max. :2.416 Max. :2.475 Max. :2.535 Max. :2.595 Max. :2.655 Max. :2.729 Max. :2.811 Max. :2.893 Max. :2.974 Max. :3.056 Max. :3.138 Max. :3.220 Max. :3.301 Max. :3.383 Max. :3.465 Max. :3.547 Max. :3.628 Max. :3.710 Max. :3.792 Max. :3.874 Max. :3.955 Max. :4.037 Max. :4.123 Max. :4.222 Max. :4.321 Max. :4.420 Max. :4.520 Max. :4.619 Max. :4.718 Max. :4.818 Max. :4.917 Max. :5.016 Max. :5.116 Max. :5.215 Max. :5.314 Max. :5.414 Max. :5.513 summary(extract(fit_rstan2, &quot;y_pred&quot;)[[1]])%&gt;%kableExtra::kable() V1 V2 Min. :-36.0432 Min. :-7.818 1st Qu.: 0.1482 1st Qu.: 6.466 Median : 0.5704 Median : 6.954 Mean : 0.5951 Mean : 6.946 3rd Qu.: 1.0130 3rd Qu.: 7.431 Max. : 19.3150 Max. :23.107 These models can be also etimated through brms package’s API for Stan as follows M_gaussian &lt;- brm(data = data_o, family = gaussian, y ~ 1 + x, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(cauchy(0, 1), class = sigma)), seed = 1) ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 2.3e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.02 seconds (Warm-up) ## Chain 1: 0.019 seconds (Sampling) ## Chain 1: 0.039 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 5e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.019 seconds (Warm-up) ## Chain 2: 0.016 seconds (Sampling) ## Chain 2: 0.035 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 5e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.019 seconds (Warm-up) ## Chain 3: 0.016 seconds (Sampling) ## Chain 3: 0.035 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 4e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.019 seconds (Warm-up) ## Chain 4: 0.018 seconds (Sampling) ## Chain 4: 0.037 seconds (Total) ## Chain 4: summary(M_gaussian) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 1 + x ## Data: data_o (Number of observations: 100) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.56 0.34 0.88 2.22 1.00 3889 2871 ## x 0.50 0.11 0.28 0.72 1.00 3941 2995 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.13 0.08 0.98 1.30 1.00 3719 2831 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). M_student &lt;- brm(data = data_o, family = student, y ~ 1 + x, prior = c(prior(normal(0, 10), class = Intercept), prior(normal(0, 10), class = b), prior(gamma(4, 1), class = nu), prior(cauchy(0, 1), class = sigma)), seed = 1) ## Compiling Stan program... ## Trying to compile a simple C file ## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c ## clang -mmacosx-version-min=10.13 -I&quot;/Library/Frameworks/R.framework/Resources/include&quot; -DNDEBUG -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/unsupported&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/BH/include&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/src/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppParallel/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/rstan/include&quot; -DEIGEN_NO_DEBUG -DBOOST_DISABLE_ASSERTS -DBOOST_PENDING_INTEGER_LOG2_HPP -DSTAN_THREADS -DUSE_STANC3 -DSTRICT_R_HEADERS -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION -DBOOST_NO_AUTO_PTR -include &#39;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp&#39; -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1 -I/usr/local/include -fPIC -Wall -g -O2 -c foo.c -o foo.o ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:88: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name &#39;namespace&#39; ## namespace Eigen { ## ^ ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected &#39;;&#39; after top level declarator ## namespace Eigen { ## ^ ## ; ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found ## #include &lt;complex&gt; ## ^~~~~~~~~ ## 3 errors generated. ## make: *** [foo.o] Error 1 ## Start sampling ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.000456 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 4.56 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.07 seconds (Warm-up) ## Chain 1: 0.063 seconds (Sampling) ## Chain 1: 0.133 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 1.8e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.069 seconds (Warm-up) ## Chain 2: 0.066 seconds (Sampling) ## Chain 2: 0.135 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 1.3e-05 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.071 seconds (Warm-up) ## Chain 3: 0.063 seconds (Sampling) ## Chain 3: 0.134 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;anon_model&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 2.5e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.07 seconds (Warm-up) ## Chain 4: 0.065 seconds (Sampling) ## Chain 4: 0.135 seconds (Total) ## Chain 4: summary(M_student) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: y ~ 1 + x ## Data: data_o (Number of observations: 100) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.56 0.20 0.17 0.96 1.00 3835 2836 ## x 0.80 0.07 0.66 0.93 1.00 3836 3084 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.50 0.06 0.39 0.62 1.00 3183 2799 ## nu 2.80 0.70 1.71 4.49 1.00 3564 2917 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 4.3 Conclusion In this post, we have provided a simple Bayesian approach to robustly estimate both parameters β and σ of a simple linear regression where the estiamtes are robust to the variance of the error term. The specificity of this approach is to replace the traditional normal assumption on the dependant variable by a heavy-tailed t-distribution assumption. Robusness against outliers comes at a price of a loss of efficiency, especially when the observations are normally distributed. This is a low premium that comes with the robust alternatives that offers a large protection against over-fiting. 4.4 References Bayesian Robustness to Outliers in Linear Regression A New Bayesian Approach to Robustness Against Outliers in Linear Regression Robust Noise Models from stan manual "],["appendix.html", "Chapter 5 Appendix 5.1 Main Stan Distributions Cheatsheet 5.2 Main Stan Functions Cheatsheet 5.3 Why Non-Distribution Functions? 5.4 Stan Functions Cheatsheet 5.5 Example: Hierarchical Linear Regression 5.6 Tips for Using Stan Functions", " Chapter 5 Appendix 5.1 Main Stan Distributions Cheatsheet Statistical modeling in Stan is powered by a flexible and expressive probabilistic language grounded in log-density functions. While the modeling blocks (model, data, parameters, etc.) help structure a model, the core statistical logic is defined through distributions. This cheatsheet offers a practical summary of the most important distributions used in Stan, their syntax, required parameters, typical use cases, and examples of where they show up in statistical modeling. Distribution Function Parameters Use Case Model Type(s) Bernoulli `bernoulli_lpmf(y θ)` θ ∈ (0, 1) Binary outcome (0/1) Logistic regression, classification Binomial `binomial_lpmf(y n, θ)` n ∈ ℕ⁺, θ ∈ (0, 1) # of successes in n trials Logistic GLMs, grouped binomial models Categorical `categorical_lpmf(y θ)` θ: simplex vector (length K) Single draw from K categories Multinomial regression Multinomial `multinomial_lpmf(y θ)` y: int vector of counts, θ: simplex Category count data Count models with category splits Normal `normal_lpdf(y μ, σ)` μ ∈ ℝ, σ &gt; 0 Gaussian noise, residuals Linear regression, priors for real parameters Student’s t `student_t_lpdf(y ν, μ, σ)` ν &gt; 0, μ ∈ ℝ, σ &gt; 0 Heavy-tailed data, robust models Robust regression, hierarchical priors Cauchy `cauchy_lpdf(y μ, σ)` μ ∈ ℝ, σ &gt; 0 Weakly informative, heavy-tailed prior Priors on scale parameters (e.g., τ ~ cauchy(0, 2.5)) Exponential `exponential_lpdf(y λ)` λ &gt; 0 Time to event, memoryless processes Survival models, Poisson process modeling Gamma `gamma_lpdf(y α, β)` α &gt; 0, β &gt; 0 Positive skewed data Priors on rates or shape parameters Inverse Gamma `inv_gamma_lpdf(y α, β)` α &gt; 0, β &gt; 0 Prior for variances Priors on σ², τ², especially in hierarchies Lognormal `lognormal_lpdf(y μ, σ)` μ ∈ ℝ, σ &gt; 0 Positive, right-skewed data Income, durations, reliability Beta `beta_lpdf(y α, β)` α &gt; 0, β &gt; 0 Probabilities or proportions Priors on probabilities (θ ∈ (0, 1)) Dirichlet `dirichlet_lpdf(θ α)` θ: simplex, α &gt; 0 vector Probabilities summing to 1 Priors for category proportions, LDA Poisson `poisson_lpmf(y λ)` λ &gt; 0 Count data, rare event modeling GLMs for count data Negative Binomial `neg_binomial_2_lpmf(y μ, φ)` μ &gt; 0, φ &gt; 0 Overdispersed count data GLMs with extra-Poisson variation Ordered Logistic `ordered_logistic_lpmf(y η, c)` η ∈ ℝ, c: ordered cut-points Ordinal outcomes Ordinal regression Uniform `uniform_lpdf(y a, b)` a &lt; b Flat prior within range Non-informative priors Pareto `pareto_lpdf(y y_min, α)` y_min &gt; 0, α &gt; 0 Heavy-tail data, power-law phenomena Extremes, outlier modeling Von Mises `von_mises_lpdf(y μ, κ)` μ ∈ [0, 2π), κ ≥ 0 Circular data (angles, wind direction) Directional models Weibull `weibull_lpdf(y α, σ)` α, σ &gt; 0 Survival times, failure rates Survival models, reliability analysis LKJ Correlation `lkj_corr_cholesky_lpdf(L η)` η &gt; 0, L: Cholesky factor Prior for correlation matrices Hierarchical models with random slopes Wishart `wishart_lpdf(S ν, Σ)` ν &gt; dim-1, Σ: scale matrix Prior on covariance matrices Multivariate Gaussian models (rarely used) 5.2 Main Stan Functions Cheatsheet Stan is a robust platform for Bayesian statistical modeling, renowned for its Hamiltonian Monte Carlo (HMC) engine and flexible modeling language. While probability distributions like normal_lpdf or poisson_lpmf define priors and likelihoods, Stan’s non-distribution functions—spanning mathematical operations, matrix algebra, utility tools, and specialized solvers—are equally critical for building efficient and expressive models. These functions enable data transformations, efficient computations, and post-processing in the generated quantities block. This cheatsheet organizes Stan’s most commonly used non-distribution functions into categories, providing their purpose, example usage, and the model types where they’re most applicable. Whether you’re crafting linear regressions, hierarchical models, or dynamic systems, this guide will help you leverage Stan’s toolkit effectively. We’ll wrap up with an example model to bring these functions to life. 5.3 Why Non-Distribution Functions? Stan’s non-distribution functions serve several key purposes: - Transformations: Functions like log, exp, and inv_logit map parameters to constrained spaces or perform nonlinear calculations. - Matrix Operations: Functions like dot_product and cholesky_decompose enable efficient linear algebra for multivariate models. - Utilities: Functions like to_vector and mean simplify data manipulation and posterior summaries. - Specialized Tools: Solvers like ode_rk45 and integrate_1d tackle complex systems, such as differential equations or custom likelihoods. - Posterior Processing: Functions in the generated quantities block, like sum or sd, compute diagnostics or predictions. This cheatsheet focuses on these functions to help you streamline model specification and analysis. 5.4 Stan Functions Cheatsheet 5.4.1 1. Mathematical Functions These functions perform scalar operations, often used in transformed parameters or model blocks. Function Purpose Example Usage Model Type(s) abs(x) Absolute value real z = abs(x); General computations, robust stats exp(x) Exponential (e^x) lambda = exp(alpha); Rate models, transformations log(x) Natural logarithm real l = log(y); Log-likelihoods, transformations sqrt(x) Square root sigma = sqrt(variance); Variance computations, scaling lgamma(x) Log gamma function lp += lgamma(alpha); Mixture models, custom likelihoods log_sum_exp(x) Log-sum-exp for numerical stability lp = log_sum_exp(log_theta); Mixture models, marginal likelihoods 5.4.2 2. Transformation Functions These map parameters to constrained spaces, often in transformed parameters. Function Purpose Example Usage Model Type(s) inv_logit(x) Logistic sigmoid (ℝ → (0,1)) theta = inv_logit(alpha + beta*x); Logistic regression, probability models logit(p) Log-odds ((0,1) → ℝ) eta = logit(p); Logistic regression, probit models softmax(x) Normalize vector to simplex theta = softmax(alpha); Multinomial regression, LDA inv(x) Reciprocal (1/x) inv_sigma = inv(sigma); Variance transformations 5.4.3 3. Matrix and Vector Operations These enable efficient linear algebra, critical for multivariate and hierarchical models. Function Purpose Example Usage Model Type(s) dot_product(a, b) Inner product of two vectors real z = dot_product(a, b); Linear regression, similarity measures matrix_times_vector(A, v) Matrix-vector multiplication eta = matrix_times_vector(X, beta); Multivariate regression, GLMs cholesky_decompose(S) Cholesky factorization L = cholesky_decompose(Sigma); Hierarchical models, multivariate normals multiply_lower_tri_self_transpose(L) Covariance from Cholesky factor Sigma = multiply_lower_tri_self_transpose(L); Multivariate normals, hierarchical models diag_matrix(v) Diagonal matrix from vector M = diag_matrix(v); Covariance priors, scaling determinant(A) Matrix determinant det = determinant(Sigma); Model diagnostics, multivariate priors 5.4.4 4. Utility Functions These simplify data manipulation and posterior summaries, often in generated quantities. Function Purpose Example Usage Model Type(s) to_vector(x) Convert matrix/array to vector vec = to_vector(matrix); Posterior summaries, data reshaping to_array_1d(x) Convert to 1D array arr = to_array_1d(matrix); Data preprocessing, summaries sum(x) Sum of elements total = sum(y); Aggregations, diagnostics mean(x) Mean of elements avg = mean(y_rep); Posterior summaries, diagnostics sd(x) Standard deviation std = sd(y_rep); Posterior summaries, diagnostics int_step(x) Indicator (x ≥ 0 → 1, else 0) flag = int_step(x - 1); Conditional logic, model diagnostics 5.4.5 5. Specialized Solvers These handle advanced computations like differential equations or parallel processing. Function Purpose Example Usage Model Type(s) ode_rk45(fun, y0, t0, ts, ...) Solve ODEs (Runge-Kutta 45) y = ode_rk45(ode_sys, y0, t0, ts, params); Dynamic systems, pharmacokinetics integrate_1d(f, a, b, ...) Numerical integration val = integrate_1d(f, a, b, params); Custom likelihoods, marginalization map_rect(f, phi, ...) Parallel computation over data shards results = map_rect(f, phi, theta, data); Large-scale hierarchical models 5.5 Example: Hierarchical Linear Regression Here’s a Stan model for a hierarchical linear regression, using matrix_times_vector, to_vector, and mean to demonstrate practical function usage: data { int&lt;lower=0&gt; N; // Number of observations int&lt;lower=0&gt; J; // Number of groups array[N] int&lt;lower=1,upper=J&gt; group; // Group indicators matrix[N, 2] X; // Design matrix (intercept + predictor) vector[N] y; // Outcome } parameters { vector[2] beta; // Fixed effects vector[J] alpha; // Group-level intercepts real&lt;lower=0&gt; sigma; // Residual standard deviation real&lt;lower=0&gt; tau; // Standard deviation of group intercepts } model { beta ~ normal(0, 5); // Prior on fixed effects tau ~ cauchy(0, 2.5); // Prior on group SD alpha ~ normal(0, tau); // Group-level priors sigma ~ cauchy(0, 2.5); // Prior on residual SD vector[N] mu = matrix_times_vector(X, beta) + to_vector(alpha[group]); y ~ normal(mu, sigma); // Likelihood } generated quantities { vector[N] y_rep; // Posterior predictive real mean_y_rep; // Mean of predictions for (n in 1:N) { y_rep[n] = normal_rng(matrix_times_vector(X[n], beta) + alpha[group[n]], sigma); } mean_y_rep = mean(to_vector(y_rep)); // Summary statistic } This model: - Uses matrix_times_vector to compute the linear predictor efficiently. - Employs to_vector to align group-level intercepts with observations. - Computes mean_y_rep in generated quantities using mean and to_vector for posterior diagnostics. - Generates predictions with normal_rng for posterior predictive checks. 5.6 Tips for Using Stan Functions Efficiency: Prefer vectorized operations like matrix_times_vector over loops for speed. Numerical Stability: Use log_sum_exp for summing exponentials to avoid overflow. Posterior Analysis: Leverage mean, sd, and to_vector in generated quantities for summaries and diagnostics. Constraints: Ensure inputs meet requirements (e.g., x &gt; 0 for log, positive-definite matrices for cholesky_decompose). Advanced Modeling: Use ode_rk45 for dynamic systems or map_rect for parallelized large-scale models. Documentation: The Stan Reference Manual (e.g., version 2.33) and Stan’s GitHub examples provide detailed guidance. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
